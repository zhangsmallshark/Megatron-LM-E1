source-ing /home/czh5/genome/Megatron-LM-E1/ALCF/launch.sh
source-ing /home/czh5/genome/Megatron-LM-E1/ALCF/args.sh
------------------------
SCRIPT_DIR=/home/czh5/genome/Megatron-LM-E1/ALCF
SCRIPT_PATH=/home/czh5/genome/Megatron-LM-E1/ALCF/args.sh
------------------------
SOURCE=/home/czh5/genome/Megatron-LM-E1/ALCF/args.sh
DIR=/home/czh5/genome/Megatron-LM-E1/ALCF
------------------------
source-ing /home/czh5/genome/Megatron-LM-E1/ALCF/setup.sh
USING PYTHON: /usr/bin/python3
CFLAGS: 
LDFLAGS: 
Setting up MPI on Polaris from x3111c0s25b0n0
source-ing /home/czh5/genome/Megatron-LM-E1/ALCF/model.sh
--------------------------------
GLOBAL_BATCH=4
--------------------------------
OUTPUT TO: /home/czh5/genome/Megatron-LM-E1/outputs/GPT3_actCkpt_flashAttn_1.3B_z0_seqlen2048_mp4_pp1_nl20_hs2048_gb4_mb4
Setting up Polaris from x3111c0s25b0n0
Loading: 'module load conda 2023-01-10-unstable ; conda activate base'
Found venv at: /home/czh5/genome/Megatron-LM/venvs/polaris/2023-01-10/
USING PYTHON: /home/czh5/genome/Megatron-LM/venvs/polaris/2023-01-10/bin/python3
    Running on 1 hosts     with 4 GPUs each     for a total of 4 GPUs
Job started at: 2023-09-24-152718 on x3111c0s25b0n0
Job running in: /home/czh5/genome/Megatron-LM-E1/ALCF
Training GPT-3 with 1.3B parameters
Writing logs to: /home/czh5/genome/Megatron-LM-E1/outputs/GPT3_actCkpt_flashAttn_1.3B_z0_seqlen2048_mp4_pp1_nl20_hs2048_gb4_mb4
to view output: tail -f $(tail -1 logfiles)
i.e. tail -f /home/czh5/genome/Megatron-LM-E1/outputs/GPT3_actCkpt_flashAttn_1.3B_z0_seqlen2048_mp4_pp1_nl20_hs2048_gb4_mb4/logs/czh5-x3111c0s25b0n0-nhosts1-ngpu4-2023-09-24-152718.log
using: /home/czh5/genome/Megatron-LM/venvs/polaris/2023-01-10/bin/python3
Job started at: 2023-09-24-152718 on x3111c0s25b0n0
Job running in: /home/czh5/genome/Megatron-LM-E1/ALCF
Training GPT-3 with 1.3B parameters
Writing logs to: /home/czh5/genome/Megatron-LM-E1/outputs/GPT3_actCkpt_flashAttn_1.3B_z0_seqlen2048_mp4_pp1_nl20_hs2048_gb4_mb4
to view output: tail -f $(tail -1 logfiles)
i.e. tail -f /home/czh5/genome/Megatron-LM-E1/outputs/GPT3_actCkpt_flashAttn_1.3B_z0_seqlen2048_mp4_pp1_nl20_hs2048_gb4_mb4/logs/czh5-x3111c0s25b0n0-nhosts1-ngpu4-2023-09-24-152718.log
EXEC=/opt/cray/pe/pals/1.1.7/bin/mpiexec     --envall     --verbose     --hostfile /var/spool/pbs/aux/1111490.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov     -n 4     --ppn 4 /home/czh5/genome/Megatron-LM/venvs/polaris/2023-01-10/bin/python3 /home/czh5/genome/Megatron-LM-E1/pretrain_gpt.py     --use-flash-attn         --recompute-activations     --recompute-granularity full       --seed 2949   --pipeline-model-parallel-size 1   --tensor-model-parallel-size 4   --num-layers 20   --hidden-size 2048   --num-attention-heads 64   --micro-batch-size 4   --global-batch-size 4   --seq-length 2048   --max-position-embeddings 2048   --train-iters 100   --lr-decay-iters 320000   --num-workers 1   --data-path /lus/eagle/projects/MDClimSim/chengming/gpt_datasets/BookCorpusDataset_text_document   --vocab-file /lus/eagle/projects/MDClimSim/chengming/gpt_datasets/gpt2-vocab.json   --merge-file /lus/eagle/projects/MDClimSim/chengming/gpt_datasets/gpt2-merges.txt   --split 949,50,1   --distributed-backend nccl   --lr 0.00015   --lr-decay-style cosine   --min-lr 1.0e-5   --weight-decay 1e-2   --clip-grad 1.0   --lr-warmup-fraction .01   --log-interval 1   --save-interval 1000   --eval-interval 1000   --eval-iters 1   --timing-log-level 2   --tensorboard-dir /home/czh5/genome/Megatron-LM-E1/outputs/GPT3_actCkpt_flashAttn_1.3B_z0_seqlen2048_mp4_pp1_nl20_hs2048_gb4_mb4/tensorboard   --log-timers-to-tensorboard   --tensorboard-log-interval 1 --fp16
Writing logs to: /home/czh5/genome/Megatron-LM-E1/outputs/GPT3_actCkpt_flashAttn_1.3B_z0_seqlen2048_mp4_pp1_nl20_hs2048_gb4_mb4/logs/czh5-x3111c0s25b0n0-nhosts1-ngpu4-2023-09-24-152718.log
using world size: 4, data-parallel-size: 1, tensor-model-parallel size: 4, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.999
  adam_eps ........................................ 1e-08
  add_bias_linear ................................. True
  add_position_embedding .......................... True
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_layernorm_1p .............................. False
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  async_tensor_model_parallel_allreduce ........... True
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  barrier_with_L1_time ............................ True
  bert_binary_head ................................ True
  bert_embedder_type .............................. megatron
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  check_for_nan_in_loss_and_grad .................. True
  classes_fraction ................................ 1.0
  clip_grad ....................................... 1.0
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  data_cache_path ................................. None
  data_parallel_random_init ....................... False
  data_parallel_size .............................. 1
  data_path ....................................... ['/lus/eagle/projects/MDClimSim/chengming/gpt_datasets/BookCorpusDataset_text_document']
  data_per_class_fraction ......................... 1.0
  data_sharding ................................... True
  dataloader_type ................................. single
  decoder_num_layers .............................. None
  decoder_seq_length .............................. None
  dino_bottleneck_size ............................ 256
  dino_freeze_last_layer .......................... 1
  dino_head_hidden_size ........................... 2048
  dino_local_crops_number ......................... 10
  dino_local_img_size ............................. 96
  dino_norm_last_layer ............................ False
  dino_teacher_temp ............................... 0.07
  dino_warmup_teacher_temp ........................ 0.04
  dino_warmup_teacher_temp_epochs ................. 30
  distribute_saved_activations .................... False
  distributed_backend ............................. nccl
  distributed_timeout_minutes ..................... 10
  embedding_path .................................. None
  embedding_weights_in_fp32 ....................... False
  empty_unused_memory_level ....................... 0
  encoder_num_layers .............................. 20
  encoder_seq_length .............................. 2048
  end_weight_decay ................................ 0.01
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... 1
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  exit_on_missing_checkpoint ...................... False
  exit_signal_handler ............................. False
  ffn_hidden_size ................................. 8192
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  fp8 ............................................. None
  fp8_amax_compute_algo ........................... most_recent
  fp8_amax_history_len ............................ 1
  fp8_interval .................................... 1
  fp8_margin ...................................... 0
  fp8_wgrad ....................................... True
  global_batch_size ............................... 4
  gradient_accumulation_fusion .................... True
  group_query_attention ........................... False
  head_lr_mult .................................... 1.0
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 2048
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_h ........................................... 224
  img_w ........................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference_batch_times_seqlen_threshold .......... 512
  init_method_std ................................. 0.02
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  iter_per_epoch .................................. 1250
  kv_channels ..................................... 32
  lazy_mpu_init ................................... None
  load ............................................ None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... True
  log_validation_ppl_to_tensorboard ............... False
  log_world_size_to_tensorboard ................... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.00015
  lr_decay_iters .................................. 320000
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. 0.01
  lr_warmup_init .................................. 0.0
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 128
  mask_factor ..................................... 1.0
  mask_prob ....................................... 0.15
  mask_type ....................................... random
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 2048
  max_tokens_to_oom ............................... 12000
  merge_file ...................................... /lus/eagle/projects/MDClimSim/chengming/gpt_datasets/gpt2-merges.txt
  micro_batch_size ................................ 4
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mmap_warmup ..................................... False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_persist_layer_norm ........................... False
  no_save_optim ................................... None
  no_save_rng ..................................... None
  norm_epsilon .................................... 1e-05
  normalization ................................... LayerNorm
  num_attention_heads ............................. 64
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... None
  num_layers ...................................... 20
  num_layers_per_virtual_pipeline_stage ........... None
  num_query_groups ................................ 1
  num_workers ..................................... 1
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  output_bert_embeddings .......................... False
  overlap_grad_reduce ............................. False
  overlap_p2p_comm ................................ False
  override_opt_param_scheduler .................... False
  params_dtype .................................... torch.float16
  patch_dim ....................................... 16
  perform_initialization .......................... True
  pipeline_model_parallel_size .................... 1
  pipeline_model_parallel_split_rank .............. None
  position_embedding_type ......................... learned_absolute
  profile ......................................... False
  profile_ranks ................................... [0]
  profile_step_end ................................ 12
  profile_step_start .............................. 10
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  recompute_granularity ........................... selective
  recompute_method ................................ None
  recompute_num_layers ............................ None
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  retro_add_retriever ............................. False
  retro_cyclic_train_iters ........................ None
  retro_encoder_attention_dropout ................. 0.1
  retro_encoder_hidden_dropout .................... 0.1
  retro_encoder_layers ............................ 2
  retro_num_neighbors ............................. 2
  retro_num_retrieved_chunks ...................... 2
  retro_return_doc_ids ............................ False
  retro_workdir ................................... None
  rotary_percent .................................. 1.0
  rotary_seq_len_interpolation_factor ............. None
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... 1000
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 2949
  seq_length ...................................... 2048
  sequence_parallel ............................... False
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  skip_train ...................................... False
  split ........................................... 949,50,1
  squared_relu .................................... False
  standalone_embedding_stage ...................... False
  start_weight_decay .............................. 0.01
  swiglu .......................................... False
  swin_backbone_type .............................. tiny
  tensor_model_parallel_size ...................... 4
  tensorboard_dir ................................. /home/czh5/genome/Megatron-LM-E1/outputs/GPT3_actCkpt_flashAttn_1.3B_z0_seqlen2048_mp4_pp1_nl20_hs2048_gb4_mb4/tensorboard
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  test_data_path .................................. None
  timing_log_level ................................ 2
  timing_log_option ............................... minmax
  titles_data_path ................................ None
  tokenizer_model ................................. None
  tokenizer_type .................................. GPT2BPETokenizer
  train_data_path ................................. None
  train_iters ..................................... 100
  train_samples ................................... None
  transformer_impl ................................ local
  transformer_pipeline_model_parallel_size ........ 1
  untie_embeddings_and_output_weights ............. False
  use_checkpoint_args ............................. False
  use_checkpoint_opt_param_scheduler .............. False
  use_cpu_initialization .......................... None
  use_distributed_optimizer ....................... False
  use_flash_attn .................................. True
  use_one_sent_docs ............................... False
  use_ring_exchange_p2p ........................... False
  use_rotary_position_embeddings .................. False
  valid_data_path ................................. None
  variable_seq_lengths ............................ False
  virtual_pipeline_model_parallel_size ............ None
  vision_backbone_type ............................ vit
  vision_pretraining .............................. False
  vision_pretraining_type ......................... classify
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /lus/eagle/projects/MDClimSim/chengming/gpt_datasets/gpt2-vocab.json
  vocab_size ...................................... None
  weight_decay .................................... 0.01
  weight_decay_incr_style ......................... constant
  world_size ...................................... 4
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 1
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 431 dummy tokens (new size: 50688)
> initializing torch distributed ...
make: Entering directory '/home/czh5/genome/Megatron-LM-E1/megatron/data'
make: Nothing to be done for 'default'.
make: Leaving directory '/home/czh5/genome/Megatron-LM-E1/megatron/data'
> initialized tensor model parallel with size 4
> initialized pipeline model parallel with size 1
> setting random seeds to 2949 ...
> compiling dataset index builder ...
>>> done with dataset index builder. Compilation time: 0.034 seconds
> compiling and loading fused kernels ...
NCCL version 2.14.3+cuda11.8
>>> done with compiling and loading fused kernels. Compilation time: 8.986 seconds
time to initialize megatron (seconds): 16.232
[after megatron is initialized] datetime: 2023-09-24 15:27:39 
building GPT model ...
 > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 533938176
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 533938176
 > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 533938176
> buckets for gradient all-reduce:
    params for bucket 1
      module.language_model.encoder.layers.2.p_mlp.0.dense_h_to_4h.weight
      module.language_model.encoder.layers.18.p_mlp.1.dense_4h_to_h.weight
      module.language_model.encoder.layers.15.input_norm.weight
      module.language_model.encoder.layers.9.p_self_attention.0.query_key_value.bias
      module.language_model.encoder.layers.5.p_mlp.1.dense_h_to_4h.weight
      module.language_model.encoder.layers.19.p_self_attention.1.dense.weight
      module.language_model.encoder.layers.15.p_mlp.0.dense_h_to_4h.weight
      module.language_model.encoder.layers.9.p_mlp.0.dense_4h_to_h.weight
      module.language_model.encoder.layers.6.p_mlp.0.dense_h_to_4h.bias
      module.language_model.encoder.layers.0.p_self_attention.0.query_key_value.weight
      module.language_model.encoder.layers.3.p_self_attention.1.query_key_value.bias
      module.language_model.encoder.layers.14.input_norm.bias
      module.language_model.encoder.layers.4.p_self_attention.0.query_key_value.bias
      module.language_model.encoder.layers.16.p_self_attention.1.dense.bias
      module.language_model.encoder.layers.10.p_self_attention.0.dense.weight
      module.language_model.encoder.layers.7.p_self_attention.0.dense.bias
      module.language_model.encoder.layers.0.p_mlp.1.dense_h_to_4h.bias
      module.language_model.encoder.layers.17.input_norm.weight
      module.language_model.encoder.layers.10.p_mlp.1.dense_4h_to_h.bias
      module.language_model.encoder.layers.7.p_mlp.1.dense_4h_to_h.weight
      module.language_model.encoder.layers.1.post_attention_norm.weight
      module.language_model.encoder.layers.13.p_self_attention.1.dense.bias
      module.language_model.encoder.layers.4.p_mlp.0.dense_4h_to_h.weight
      module.language_model.encoder.layers.18.p_self_attention.0.dense.bias
      module.language_model.encoder.layers.14.post_attention_norm.weight
      module.language_model.encoder.layers.8.p_self_attention.1.dense.bias
      module.language_model.encoder.layers.5.p_self_attention.0.dense.weight
      module.language_model.encoder.layers.17.p_mlp.0.dense_h_to_4h.weight
      module.language_model.encoder.layers.11.post_attention_norm.bias
      module.language_model.encoder.layers.2.input_norm.bias
      module.language_model.encoder.layers.2.p_mlp.0.dense_4h_to_h.bias
      module.language_model.encoder.layers.18.p_mlp.1.dense_h_to_4h.weight
      module.language_model.encoder.layers.15.input_norm.bias
      module.language_model.encoder.layers.12.p_mlp.0.dense_4h_to_h.weight
      module.language_model.encoder.layers.9.input_norm.weight
      module.language_model.encoder.layers.5.p_mlp.1.dense_4h_to_h.bias
      module.language_model.encoder.layers.19.p_mlp.0.dense_h_to_4h.bias
      module.language_model.encoder.layers.15.p_mlp.0.dense_4h_to_h.bias
      module.language_model.encoder.layers.9.p_mlp.0.dense_h_to_4h.weight
      module.language_model.encoder.layers.6.post_attention_norm.bias
      module.language_model.encoder.layers.0.p_self_attention.0.dense.bias
      module.language_model.encoder.layers.13.p_mlp.1.dense_h_to_4h.bias
      module.language_model.encoder.layers.3.p_self_attention.1.dense.bias
      module.language_model.encoder.layers.12.p_mlp.0.dense_h_to_4h.weight
      module.language_model.encoder.layers.12.p_self_attention.1.query_key_value.weight
      module.language_model.encoder.layers.4.input_norm.weight
      module.language_model.encoder.layers.16.post_attention_norm.weight
      module.language_model.encoder.layers.10.p_self_attention.1.query_key_value.bias
      module.language_model.encoder.layers.7.p_self_attention.1.query_key_value.weight
      module.language_model.encoder.layers.0.p_mlp.1.dense_4h_to_h.weight
      module.language_model.encoder.layers.17.input_norm.bias
      module.language_model.encoder.layers.11.p_self_attention.0.query_key_value.bias
      module.language_model.encoder.layers.1.p_self_attention.1.dense.weight
      module.language_model.encoder.layers.12.p_self_attention.0.dense.bias
      module.language_model.encoder.layers.4.p_mlp.0.dense_h_to_4h.weight
      module.language_model.encoder.layers.18.p_self_attention.0.dense.weight
      module.language_model.encoder.layers.14.p_self_attention.1.dense.weight
      module.language_model.encoder.layers.8.post_attention_norm.weight
      module.language_model.encoder.layers.5.p_self_attention.1.query_key_value.bias
      module.language_model.encoder.layers.17.p_mlp.0.dense_4h_to_h.bias
      module.language_model.encoder.layers.11.p_mlp.0.dense_4h_to_h.weight
      module.language_model.encoder.layers.2.p_self_attention.0.query_key_value.weight
      module.language_model.encoder.layers.2.p_mlp.1.dense_h_to_4h.bias
      module.language_model.encoder.layers.18.p_mlp.1.dense_4h_to_h.bias
      module.language_model.encoder.layers.15.p_self_attention.0.query_key_value.weight
      module.language_model.encoder.layers.9.input_norm.bias
      module.language_model.encoder.layers.6.p_self_attention.0.query_key_value.bias
      module.language_model.encoder.layers.0.p_self_attention.0.query_key_value.bias
      module.language_model.encoder.layers.19.post_attention_norm.bias
      module.language_model.encoder.layers.15.p_mlp.1.dense_h_to_4h.bias
      module.language_model.encoder.layers.9.p_mlp.0.dense_4h_to_h.bias
      module.language_model.encoder.layers.6.p_mlp.0.dense_4h_to_h.weight
      module.language_model.encoder.layers.0.p_self_attention.1.query_key_value.weight
      module.language_model.encoder.layers.13.p_self_attention.0.query_key_value.bias
      module.language_model.encoder.layers.3.post_attention_norm.weight
      module.language_model.encoder.layers.14.p_self_attention.0.query_key_value.weight
      module.language_model.encoder.layers.12.p_mlp.1.dense_4h_to_h.bias
      module.language_model.encoder.layers.4.input_norm.bias
      module.language_model.encoder.layers.16.p_self_attention.1.dense.weight
      module.language_model.encoder.layers.12.p_mlp.1.dense_4h_to_h.weight
      module.language_model.encoder.layers.10.p_self_attention.1.dense.bias
      module.language_model.encoder.layers.7.p_self_attention.0.dense.weight
      module.language_model.encoder.layers.0.p_mlp.1.dense_h_to_4h.weight
      module.language_model.encoder.layers.17.p_self_attention.0.query_key_value.weight
      module.language_model.encoder.layers.11.input_norm.weight
      module.language_model.encoder.layers.1.p_mlp.0.dense_h_to_4h.bias
      module.language_model.encoder.layers.4.p_mlp.0.dense_4h_to_h.bias
      module.language_model.encoder.layers.18.p_self_attention.1.query_key_value.bias
      module.language_model.encoder.layers.15.p_mlp.1.dense_4h_to_h.weight
      module.language_model.encoder.layers.14.p_mlp.0.dense_h_to_4h.bias
      module.language_model.encoder.layers.8.p_self_attention.1.dense.weight
      module.language_model.encoder.layers.5.p_self_attention.1.dense.bias
      module.language_model.encoder.layers.17.p_mlp.1.dense_h_to_4h.bias
      module.language_model.encoder.layers.11.p_mlp.0.dense_h_to_4h.weight
      module.language_model.encoder.layers.2.p_self_attention.0.dense.bias
      module.language_model.encoder.layers.2.p_mlp.1.dense_4h_to_h.weight
      module.language_model.embedding.position_embeddings.weight
      module.language_model.encoder.layers.19.p_self_attention.0.query_key_value.bias
      module.language_model.encoder.layers.15.p_self_attention.0.dense.bias
      module.language_model.encoder.layers.9.p_self_attention.0.query_key_value.weight
      module.language_model.encoder.layers.6.input_norm.weight
      module.language_model.encoder.layers.19.p_mlp.0.dense_4h_to_h.weight
      module.language_model.encoder.layers.15.p_mlp.1.dense_h_to_4h.weight
      module.language_model.encoder.layers.12.p_mlp.1.dense_h_to_4h.bias
      module.language_model.encoder.layers.9.p_mlp.1.dense_h_to_4h.bias
      module.language_model.encoder.layers.6.p_mlp.0.dense_h_to_4h.weight
      module.language_model.encoder.layers.0.p_self_attention.0.dense.weight
      module.language_model.encoder.layers.13.p_mlp.1.dense_4h_to_h.weight
      module.language_model.encoder.layers.3.p_self_attention.1.dense.weight
      module.language_model.encoder.layers.4.p_self_attention.0.query_key_value.weight
      module.language_model.encoder.layers.16.p_mlp.0.dense_h_to_4h.bias
      module.language_model.encoder.layers.13.p_self_attention.0.dense.bias
      module.language_model.encoder.layers.10.post_attention_norm.weight
      module.language_model.encoder.layers.7.p_self_attention.1.query_key_value.bias
      module.language_model.encoder.layers.0.p_mlp.1.dense_4h_to_h.bias
      module.language_model.encoder.layers.17.p_self_attention.0.dense.bias
      module.language_model.encoder.layers.11.input_norm.bias
      module.language_model.encoder.layers.1.post_attention_norm.bias
      module.language_model.encoder.layers.13.post_attention_norm.weight
      module.language_model.encoder.layers.7.p_mlp.1.dense_h_to_4h.weight
      module.language_model.encoder.layers.4.p_mlp.1.dense_h_to_4h.bias
      module.language_model.encoder.layers.18.p_self_attention.1.dense.bias
      module.language_model.encoder.layers.14.post_attention_norm.bias
      module.language_model.encoder.layers.13.p_self_attention.0.query_key_value.weight
      module.language_model.encoder.layers.8.p_mlp.0.dense_h_to_4h.bias
      module.language_model.encoder.layers.5.post_attention_norm.weight
      module.language_model.encoder.layers.17.p_mlp.1.dense_4h_to_h.weight
      module.language_model.encoder.layers.12.p_self_attention.1.query_key_value.bias
      module.language_model.encoder.layers.11.p_mlp.0.dense_4h_to_h.bias
      module.language_model.encoder.layers.2.p_self_attention.1.query_key_value.weight
      module.language_model.encoder.layers.2.p_mlp.1.dense_h_to_4h.weight
      module.language_model.encoder.layers.19.input_norm.weight
      module.language_model.encoder.layers.15.p_self_attention.1.query_key_value.weight
      module.language_model.encoder.layers.9.p_self_attention.0.dense.bias
      module.language_model.encoder.layers.6.input_norm.bias
      module.language_model.encoder.layers.19.p_mlp.0.dense_h_to_4h.weight
      module.language_model.encoder.layers.15.p_mlp.1.dense_4h_to_h.bias
      module.language_model.encoder.layers.13.p_self_attention.1.query_key_value.weight
      module.language_model.encoder.layers.9.p_mlp.1.dense_4h_to_h.weight
      module.language_model.encoder.layers.6.p_mlp.0.dense_4h_to_h.bias
      module.language_model.encoder.layers.0.p_self_attention.1.query_key_value.bias
      module.language_model.encoder.layers.12.p_mlp.0.dense_h_to_4h.bias
      module.language_model.encoder.layers.3.p_mlp.0.dense_h_to_4h.bias
      module.language_model.encoder.layers.14.p_self_attention.0.dense.bias
      module.language_model.encoder.layers.4.p_self_attention.0.dense.bias
      module.language_model.encoder.layers.16.post_attention_norm.bias
      module.language_model.encoder.layers.10.p_self_attention.1.dense.weight
      module.language_model.encoder.layers.7.p_self_attention.1.dense.bias
      module.language_model.encoder.layers.1.p_self_attention.0.query_key_value.bias
      module.language_model.encoder.layers.17.p_self_attention.1.query_key_value.weight
      module.language_model.encoder.layers.11.p_self_attention.0.query_key_value.weight
      module.language_model.encoder.layers.1.p_mlp.0.dense_4h_to_h.weight
      module.language_model.encoder.layers.7.p_mlp.1.dense_4h_to_h.bias
      module.language_model.encoder.layers.4.p_mlp.1.dense_4h_to_h.weight
      module.language_model.encoder.layers.5.p_self_attention.1.dense.weight
      module.language_model.encoder.layers.18.post_attention_norm.weight
      module.language_model.encoder.layers.14.p_mlp.0.dense_4h_to_h.weight
      module.language_model.encoder.layers.8.post_attention_norm.bias
      module.language_model.encoder.layers.17.p_mlp.1.dense_h_to_4h.weight
      module.language_model.encoder.layers.11.p_mlp.1.dense_h_to_4h.bias
      module.language_model.encoder.layers.2.p_self_attention.0.dense.weight
      module.language_model.encoder.layers.13.p_mlp.0.dense_h_to_4h.bias
      module.language_model.encoder.layers.2.p_mlp.1.dense_4h_to_h.bias
      module.language_model.encoder.layers.19.input_norm.bias
      module.language_model.encoder.layers.15.p_self_attention.0.dense.weight
      module.language_model.encoder.layers.9.p_self_attention.1.query_key_value.weight
      module.language_model.encoder.layers.6.p_self_attention.0.query_key_value.weight
      module.language_model.encoder.layers.19.p_mlp.0.dense_4h_to_h.bias
      module.language_model.encoder.layers.16.p_self_attention.0.query_key_value.bias
      module.language_model.encoder.layers.9.p_mlp.1.dense_h_to_4h.weight
      module.language_model.encoder.layers.6.p_mlp.1.dense_h_to_4h.bias
      module.language_model.encoder.layers.0.p_self_attention.1.dense.bias
      module.language_model.encoder.layers.13.p_mlp.1.dense_h_to_4h.weight
      module.language_model.encoder.layers.3.post_attention_norm.bias
      module.language_model.encoder.layers.4.p_self_attention.1.query_key_value.weight
      module.language_model.encoder.layers.16.p_mlp.0.dense_4h_to_h.weight
      module.language_model.encoder.layers.10.p_mlp.0.dense_h_to_4h.bias
      module.language_model.encoder.layers.7.post_attention_norm.weight
      module.language_model.encoder.layers.1.input_norm.weight
      module.language_model.encoder.layers.17.p_self_attention.0.dense.weight
      module.language_model.encoder.layers.11.p_self_attention.0.dense.bias
      module.language_model.encoder.layers.1.p_mlp.0.dense_h_to_4h.weight
      module.language_model.encoder.layers.8.p_self_attention.0.query_key_value.bias
      module.language_model.encoder.layers.4.p_mlp.1.dense_h_to_4h.weight
      module.language_model.encoder.layers.5.p_mlp.0.dense_h_to_4h.bias
      module.language_model.encoder.layers.18.p_self_attention.1.dense.weight
      module.language_model.encoder.layers.14.p_mlp.0.dense_h_to_4h.weight
      module.language_model.encoder.layers.13.p_self_attention.0.dense.weight
      module.language_model.encoder.layers.8.p_mlp.0.dense_4h_to_h.weight
      module.language_model.encoder.layers.17.p_mlp.1.dense_4h_to_h.bias
      module.language_model.encoder.layers.11.p_mlp.1.dense_4h_to_h.weight
      module.language_model.encoder.layers.2.p_self_attention.1.query_key_value.bias
      module.language_model.encoder.layers.13.post_attention_norm.bias
      module.language_model.encoder.layers.3.p_self_attention.0.query_key_value.bias
      module.language_model.encoder.layers.19.p_self_attention.0.query_key_value.weight
      module.language_model.encoder.layers.15.p_self_attention.1.query_key_value.bias
      module.language_model.encoder.layers.9.p_self_attention.0.dense.weight
      module.language_model.encoder.layers.6.p_self_attention.0.dense.bias
      module.language_model.encoder.layers.19.p_mlp.1.dense_h_to_4h.bias
      module.language_model.encoder.layers.16.input_norm.weight
      module.language_model.encoder.layers.9.p_mlp.1.dense_4h_to_h.bias
      module.language_model.encoder.layers.6.p_mlp.1.dense_4h_to_h.weight
      module.language_model.encoder.layers.0.post_attention_norm.weight
      module.language_model.encoder.layers.3.p_mlp.0.dense_4h_to_h.weight
      module.language_model.encoder.layers.14.p_self_attention.1.query_key_value.weight
      module.language_model.encoder.layers.4.p_self_attention.0.dense.weight
      module.language_model.encoder.layers.16.p_mlp.0.dense_h_to_4h.weight
      module.language_model.encoder.layers.10.post_attention_norm.bias
      module.language_model.encoder.layers.7.p_self_attention.1.dense.weight
      module.language_model.encoder.layers.1.input_norm.bias
      module.language_model.encoder.layers.17.p_self_attention.1.query_key_value.bias
      module.language_model.encoder.layers.11.p_self_attention.1.query_key_value.weight
      module.language_model.encoder.layers.1.p_mlp.0.dense_4h_to_h.bias
      module.language_model.encoder.layers.8.input_norm.weight
      module.language_model.encoder.layers.4.p_mlp.1.dense_4h_to_h.bias
      module.language_model.encoder.layers.18.p_mlp.0.dense_h_to_4h.bias
      module.language_model.encoder.layers.14.p_mlp.0.dense_4h_to_h.bias
      module.language_model.encoder.layers.8.p_mlp.0.dense_h_to_4h.weight
      module.language_model.encoder.layers.5.post_attention_norm.bias
      module.language_model.encoder.layers.18.p_self_attention.0.query_key_value.bias
      module.language_model.encoder.layers.11.p_mlp.1.dense_h_to_4h.weight
      module.language_model.encoder.layers.2.p_self_attention.1.dense.bias
      module.language_model.encoder.layers.3.input_norm.weight
      module.language_model.encoder.layers.19.p_self_attention.0.dense.bias
      module.language_model.encoder.layers.15.p_self_attention.1.dense.bias
      module.language_model.encoder.layers.9.p_self_attention.1.query_key_value.bias
      module.language_model.encoder.layers.6.p_self_attention.1.query_key_value.weight
      module.language_model.encoder.layers.19.p_mlp.1.dense_4h_to_h.weight
      module.language_model.encoder.layers.16.input_norm.bias
      module.language_model.encoder.layers.12.p_self_attention.1.dense.bias
      module.language_model.encoder.layers.10.p_self_attention.0.query_key_value.bias
      module.language_model.encoder.layers.6.p_mlp.1.dense_h_to_4h.weight
      module.language_model.encoder.layers.0.p_self_attention.1.dense.weight
      module.language_model.encoder.layers.13.p_mlp.1.dense_4h_to_h.bias
      module.language_model.encoder.layers.13.input_norm.weight
      module.language_model.encoder.layers.3.p_mlp.0.dense_h_to_4h.weight
      module.language_model.encoder.layers.4.p_self_attention.1.query_key_value.bias
      module.language_model.encoder.layers.16.p_mlp.0.dense_4h_to_h.bias
      module.language_model.encoder.layers.10.p_mlp.0.dense_4h_to_h.weight
      module.language_model.encoder.layers.7.p_mlp.0.dense_h_to_4h.bias
      module.language_model.encoder.layers.1.p_self_attention.0.query_key_value.weight
      module.language_model.encoder.layers.17.p_self_attention.1.dense.bias
      module.language_model.encoder.layers.12.post_attention_norm.bias
      module.language_model.encoder.layers.11.p_self_attention.0.dense.weight
      module.language_model.encoder.layers.1.p_mlp.1.dense_h_to_4h.bias
      module.language_model.encoder.layers.8.input_norm.bias
      module.language_model.encoder.layers.5.p_self_attention.0.query_key_value.bias
      module.language_model.encoder.layers.18.post_attention_norm.bias
      module.language_model.encoder.layers.14.p_mlp.1.dense_h_to_4h.bias
      module.language_model.encoder.layers.8.p_mlp.0.dense_4h_to_h.bias
      module.language_model.encoder.layers.5.p_mlp.0.dense_4h_to_h.weight
      module.language_model.encoder.layers.18.input_norm.weight
      module.language_model.encoder.layers.11.p_mlp.1.dense_4h_to_h.bias
      module.language_model.encoder.layers.2.post_attention_norm.weight
      module.language_model.encoder.layers.13.p_mlp.0.dense_4h_to_h.weight
      module.language_model.encoder.layers.3.input_norm.bias
      module.language_model.encoder.layers.19.p_self_attention.1.query_key_value.weight
      module.language_model.encoder.layers.15.post_attention_norm.weight
      module.language_model.encoder.layers.9.p_self_attention.1.dense.bias
      module.language_model.encoder.layers.6.p_self_attention.0.dense.weight
      module.language_model.encoder.layers.19.p_mlp.1.dense_h_to_4h.weight
      module.language_model.encoder.layers.16.p_self_attention.0.query_key_value.weight
      module.language_model.encoder.layers.10.input_norm.weight
      module.language_model.encoder.layers.6.p_mlp.1.dense_4h_to_h.bias
      module.language_model.encoder.layers.0.p_mlp.0.dense_h_to_4h.bias
      module.language_model.encoder.layers.3.p_mlp.0.dense_4h_to_h.bias
      module.language_model.encoder.layers.14.p_self_attention.0.dense.weight
      module.language_model.encoder.layers.4.p_self_attention.1.dense.bias
      module.language_model.encoder.layers.16.p_mlp.1.dense_h_to_4h.bias
      module.language_model.encoder.layers.10.p_mlp.0.dense_h_to_4h.weight
      module.language_model.encoder.layers.7.post_attention_norm.bias
      module.language_model.encoder.layers.1.p_self_attention.0.dense.bias
      module.language_model.encoder.layers.17.post_attention_norm.weight
      module.language_model.encoder.layers.12.p_self_attention.0.dense.weight
      module.language_model.encoder.layers.11.p_self_attention.1.query_key_value.bias
      module.language_model.encoder.layers.1.p_mlp.1.dense_4h_to_h.weight
      module.language_model.encoder.layers.8.p_self_attention.0.query_key_value.weight
      module.language_model.encoder.layers.5.input_norm.weight
      module.language_model.encoder.layers.18.p_mlp.0.dense_4h_to_h.weight
      module.language_model.encoder.layers.14.p_mlp.1.dense_4h_to_h.weight
      module.language_model.encoder.layers.8.p_mlp.1.dense_h_to_4h.bias
      module.language_model.encoder.layers.5.p_mlp.0.dense_h_to_4h.weight
      module.language_model.encoder.layers.18.input_norm.bias
      module.language_model.encoder.layers.12.p_self_attention.0.query_key_value.bias
      module.language_model.encoder.layers.2.p_self_attention.1.dense.weight
      module.language_model.encoder.layers.3.p_self_attention.0.query_key_value.weight
      module.language_model.encoder.layers.19.p_self_attention.0.dense.weight
      module.language_model.encoder.layers.15.p_self_attention.1.dense.weight
      module.language_model.encoder.layers.12.p_mlp.1.dense_h_to_4h.weight
      module.language_model.encoder.layers.9.post_attention_norm.weight
      module.language_model.encoder.layers.6.p_self_attention.1.query_key_value.bias
      module.language_model.encoder.layers.0.input_norm.weight
      module.language_model.encoder.layers.16.p_self_attention.0.dense.bias
      module.language_model.encoder.layers.19.p_mlp.1.dense_4h_to_h.bias
      module.language_model.encoder.layers.12.p_self_attention.1.dense.weight
      module.language_model.encoder.layers.10.input_norm.bias
      module.language_model.encoder.layers.7.p_self_attention.0.query_key_value.bias
      module.language_model.encoder.layers.0.post_attention_norm.bias
      module.language_model.encoder.layers.14.p_self_attention.0.query_key_value.bias
      module.language_model.encoder.layers.3.p_mlp.1.dense_h_to_4h.bias
      module.language_model.encoder.layers.4.post_attention_norm.weight
      module.language_model.encoder.layers.16.p_mlp.1.dense_4h_to_h.weight
      module.language_model.encoder.layers.10.p_mlp.0.dense_4h_to_h.bias
      module.language_model.encoder.layers.7.p_mlp.0.dense_4h_to_h.weight
      module.language_model.encoder.layers.1.p_self_attention.1.query_key_value.weight
      module.language_model.encoder.layers.17.p_self_attention.1.dense.weight
      module.language_model.encoder.layers.11.p_self_attention.1.dense.bias
      module.language_model.encoder.layers.1.p_mlp.1.dense_h_to_4h.weight
      module.language_model.encoder.layers.5.input_norm.bias
      module.language_model.encoder.layers.13.p_self_attention.1.dense.weight
      module.language_model.encoder.layers.8.p_self_attention.0.dense.bias
      module.language_model.encoder.layers.18.p_mlp.0.dense_h_to_4h.weight
      module.language_model.encoder.layers.14.p_mlp.1.dense_h_to_4h.weight
      module.language_model.encoder.layers.8.p_mlp.1.dense_4h_to_h.weight
      module.language_model.encoder.layers.5.p_mlp.0.dense_4h_to_h.bias
      module.language_model.encoder.layers.18.p_self_attention.0.query_key_value.weight
      module.language_model.encoder.layers.12.input_norm.weight
      module.language_model.encoder.layers.2.p_mlp.0.dense_h_to_4h.bias
      module.language_model.encoder.layers.13.p_mlp.0.dense_h_to_4h.weight
      module.language_model.encoder.layers.3.p_self_attention.0.dense.bias
      module.language_model.encoder.layers.19.p_self_attention.1.query_key_value.bias
      module.language_model.encoder.layers.15.p_mlp.0.dense_h_to_4h.bias
      module.language_model.encoder.layers.9.p_self_attention.1.dense.weight
      module.language_model.encoder.layers.6.p_self_attention.1.dense.bias
      module.language_model.encoder.layers.0.input_norm.bias
      module.language_model.encoder.layers.16.p_self_attention.1.query_key_value.weight
      module.language_model.encoder.layers.10.p_self_attention.0.query_key_value.weight
      module.language_model.encoder.layers.7.input_norm.weight
      module.language_model.encoder.layers.0.p_mlp.0.dense_4h_to_h.weight
      module.language_model.encoder.layers.3.p_mlp.1.dense_4h_to_h.weight
      module.language_model.encoder.layers.13.p_self_attention.1.query_key_value.bias
      module.language_model.encoder.layers.4.p_self_attention.1.dense.weight
      module.language_model.encoder.layers.16.p_mlp.1.dense_h_to_4h.weight
      module.language_model.encoder.layers.10.p_mlp.1.dense_h_to_4h.bias
      module.language_model.encoder.layers.7.p_mlp.0.dense_h_to_4h.weight
      module.language_model.encoder.layers.1.p_self_attention.0.dense.weight
      module.language_model.encoder.layers.17.p_mlp.0.dense_h_to_4h.bias
      module.language_model.encoder.layers.12.post_attention_norm.weight
      module.language_model.encoder.layers.11.post_attention_norm.weight
      module.language_model.encoder.layers.1.p_mlp.1.dense_4h_to_h.bias
      module.language_model.encoder.layers.8.p_self_attention.1.query_key_value.weight
      module.language_model.encoder.layers.5.p_self_attention.0.query_key_value.weight
      module.language_model.encoder.layers.5.p_mlp.1.dense_h_to_4h.bias
      module.language_model.encoder.layers.18.p_mlp.0.dense_4h_to_h.bias
      module.language_model.encoder.layers.14.p_mlp.1.dense_4h_to_h.bias
      module.language_model.encoder.layers.8.p_mlp.1.dense_h_to_4h.weight
      module.language_model.encoder.layers.18.p_self_attention.1.query_key_value.weight
      module.language_model.encoder.layers.12.input_norm.bias
      module.language_model.encoder.layers.2.post_attention_norm.bias
      module.language_model.encoder.layers.3.p_self_attention.1.query_key_value.weight
      module.language_model.encoder.layers.19.p_self_attention.1.dense.bias
      module.language_model.encoder.layers.15.post_attention_norm.bias
      module.language_model.encoder.layers.9.p_mlp.0.dense_h_to_4h.bias
      module.language_model.encoder.layers.6.post_attention_norm.weight
      module.language_model.encoder.final_norm.weight
      module.language_model.encoder.layers.16.p_self_attention.0.dense.weight
      module.language_model.encoder.layers.10.p_self_attention.0.dense.bias
      module.language_model.encoder.layers.7.input_norm.bias
      module.language_model.encoder.layers.0.p_mlp.0.dense_h_to_4h.weight
      module.language_model.encoder.layers.14.input_norm.weight
      module.language_model.encoder.layers.3.p_mlp.1.dense_h_to_4h.weight
      module.language_model.encoder.layers.4.p_mlp.0.dense_h_to_4h.bias
      module.language_model.embedding.word_embeddings.weight
      module.language_model.encoder.layers.16.p_mlp.1.dense_4h_to_h.bias
      module.language_model.encoder.layers.10.p_mlp.1.dense_4h_to_h.weight
      module.language_model.encoder.layers.7.p_mlp.0.dense_4h_to_h.bias
      module.language_model.encoder.layers.1.p_self_attention.1.query_key_value.bias
      module.language_model.encoder.layers.17.post_attention_norm.bias
      module.language_model.encoder.layers.11.p_self_attention.1.dense.weight
      module.language_model.encoder.layers.2.p_self_attention.0.query_key_value.bias
      module.language_model.encoder.layers.5.p_self_attention.0.dense.bias
      module.language_model.encoder.layers.14.p_self_attention.1.query_key_value.bias
      module.language_model.encoder.layers.8.p_self_attention.0.dense.weight
      module.language_model.encoder.layers.18.p_mlp.1.dense_h_to_4h.bias
      module.language_model.encoder.layers.15.p_self_attention.0.query_key_value.bias
      module.language_model.encoder.layers.8.p_mlp.1.dense_4h_to_h.bias
      module.language_model.encoder.layers.5.p_mlp.1.dense_4h_to_h.weight
      module.language_model.encoder.layers.12.p_self_attention.0.query_key_value.weight
      module.language_model.encoder.layers.2.p_mlp.0.dense_4h_to_h.weight
      module.language_model.encoder.layers.13.p_mlp.0.dense_4h_to_h.bias
      module.language_model.encoder.layers.3.p_self_attention.0.dense.weight
      module.language_model.encoder.layers.19.post_attention_norm.weight
      module.language_model.encoder.layers.15.p_mlp.0.dense_4h_to_h.weight
      module.language_model.encoder.layers.9.post_attention_norm.bias
      module.language_model.encoder.layers.6.p_self_attention.1.dense.weight
      module.language_model.encoder.layers.7.p_self_attention.0.query_key_value.weight
      module.language_model.encoder.final_norm.bias
      module.language_model.encoder.layers.16.p_self_attention.1.query_key_value.bias
      module.language_model.encoder.layers.10.p_self_attention.1.query_key_value.weight
      module.language_model.encoder.layers.0.p_mlp.0.dense_4h_to_h.bias
      module.language_model.encoder.layers.12.p_mlp.0.dense_4h_to_h.bias
      module.language_model.encoder.layers.3.p_mlp.1.dense_4h_to_h.bias
      module.language_model.encoder.layers.4.post_attention_norm.bias
      module.language_model.encoder.layers.17.p_self_attention.0.query_key_value.bias
      module.language_model.encoder.layers.13.input_norm.bias
      module.language_model.encoder.layers.10.p_mlp.1.dense_h_to_4h.weight
      module.language_model.encoder.layers.7.p_mlp.1.dense_h_to_4h.bias
      module.language_model.encoder.layers.1.p_self_attention.1.dense.bias
      module.language_model.encoder.layers.17.p_mlp.0.dense_4h_to_h.weight
      module.language_model.encoder.layers.11.p_mlp.0.dense_h_to_4h.bias
      module.language_model.encoder.layers.2.input_norm.weight
      module.language_model.encoder.layers.14.p_self_attention.1.dense.bias
      module.language_model.encoder.layers.8.p_self_attention.1.query_key_value.bias
      module.language_model.encoder.layers.5.p_self_attention.1.query_key_value.weight
     total number of elements: 533938176
> learning rate decay style: cosine
> setting tensorboard ...
 > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 533938176
[after model, optimizer, and learning rate scheduler are built] datetime: 2023-09-24 15:27:39 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      400
    validation: 4
    test:       4
> building train, validation, and test datasets for GPT ...
Single data path provided for train, valid & test
 > building dataset index ...
    reading sequence lengths...
    reading sequence pointers...
    reading document indices...
    creating np buffer of mmap...
    creating memory view of np buffer...
 > finished creating indexed dataset in 0.001749 seconds
    number of documents: 17868
 > dataset split:
    train:
     document indices in [0, 16957) total of 16957 documents
    validation:
     document indices in [16957, 17850) total of 893 documents
    test:
     document indices in [17850, 17868) total of 18 documents
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.001758
    using:
     number of documents:       16957
     number of epochs:          1
     sequence length:           2048
     total number of samples:   747717
 > elasped time to build and save sample-idx mapping (seconds): 0.005289
 > building shuffle index with split [0, 747717) and [747717, 747717) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.017327
 > loading doc-idx mapping from /lus/eagle/projects/MDClimSim/chengming/gpt_datasets/index-cache/d7500a91b8dc61208a44d6b18f2dbc52_doc_idx.npy
 > loading sample-idx mapping from /lus/eagle/projects/MDClimSim/chengming/gpt_datasets/index-cache/d7500a91b8dc61208a44d6b18f2dbc52_sample_idx.npy
 > loading shuffle-idx mapping from /lus/eagle/projects/MDClimSim/chengming/gpt_datasets/index-cache/d7500a91b8dc61208a44d6b18f2dbc52_shuffle_idx.npy
    loaded indexed file in 0.003 seconds
    total number of samples: 747718
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.001616
    using:
     number of documents:       893
     number of epochs:          1
     sequence length:           2048
     total number of samples:   38903
 > elasped time to build and save sample-idx mapping (seconds): 0.001404
 > building shuffle index with split [0, 38903) and [38903, 38903) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.001971
 > loading doc-idx mapping from /lus/eagle/projects/MDClimSim/chengming/gpt_datasets/index-cache/1da50910c363463d2edfc652d50236d4_doc_idx.npy
 > loading sample-idx mapping from /lus/eagle/projects/MDClimSim/chengming/gpt_datasets/index-cache/1da50910c363463d2edfc652d50236d4_sample_idx.npy
 > loading shuffle-idx mapping from /lus/eagle/projects/MDClimSim/chengming/gpt_datasets/index-cache/1da50910c363463d2edfc652d50236d4_shuffle_idx.npy
    loaded indexed file in 0.002 seconds
    total number of samples: 38904
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.001152
    using:
     number of documents:       18
     number of epochs:          1
     sequence length:           2048
     total number of samples:   1094
 > elasped time to build and save sample-idx mapping (seconds): 0.001200
 > building shuffle index with split [0, 1094) and [1094, 1094) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.001337
 > loading doc-idx mapping from /lus/eagle/projects/MDClimSim/chengming/gpt_datasets/index-cache/03d60385d520dde9d68082ede7ef0511_doc_idx.npy
 > loading sample-idx mapping from /lus/eagle/projects/MDClimSim/chengming/gpt_datasets/index-cache/03d60385d520dde9d68082ede7ef0511_sample_idx.npy
 > loading shuffle-idx mapping from /lus/eagle/projects/MDClimSim/chengming/gpt_datasets/index-cache/03d60385d520dde9d68082ede7ef0511_shuffle_idx.npy
    loaded indexed file in 0.001 seconds
    total number of samples: 1095
    total number of epochs: 1
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2023-09-24 15:27:48 
done with setup ...
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (155.69, 238.43)
    train/valid/test-data-iterators-setup ..........: (7888.32, 8656.08)
training ...
[before the start of training step] datetime: 2023-09-24 15:27:48 
NCCL version 2.14.3+cuda11.8
NCCL version 2.14.3+cuda11.8
NCCL version 2.14.3+cuda11.8
 iteration        1/     100 | consumed samples:            4 | elapsed time per iteration (ms): 13378.4 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 4294967296.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
[Rank 2] (after 1 iterations) memory (MB) | allocated: 6111.94189453125 | max allocated: 10442.36328125 | reserved: 10740.0 | max reserved: 10740.0
[Rank 1] (after 1 iterations) memory (MB) | allocated: 6111.94189453125 | max allocated: 10442.36328125 | reserved: 10740.0 | max reserved: 10740.0
[Rank 0] (after 1 iterations) memory (MB) | allocated: 6111.94189453125 | max allocated: 10442.36328125 | reserved: 10740.0 | max reserved: 10740.0
[Rank 3] (after 1 iterations) memory (MB) | allocated: 6111.94189453125 | max allocated: 10442.36328125 | reserved: 10740.0 | max reserved: 10740.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (5815.88, 5816.12)
    forward-compute ................................: (939.06, 1191.79)
    backward-compute ...............................: (4623.60, 4876.04)
    batch-generator ................................: (50.16, 50.21)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-all-reduce ...............................: (1.58, 1.59)
    optimizer-copy-to-main-grad ....................: (4.86, 5.86)
    optimizer-unscale-and-check-inf ................: (7548.58, 7548.61)
    optimizer ......................................: (7554.62, 7554.65)
 iteration        2/     100 | consumed samples:            8 | elapsed time per iteration (ms): 272.9 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 2147483648.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (251.70, 251.97)
    forward-compute ................................: (102.09, 102.11)
    backward-compute ...............................: (148.97, 149.21)
    batch-generator ................................: (0.88, 0.96)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.63)
    optimizer-copy-to-main-grad ....................: (4.87, 5.71)
    optimizer-unscale-and-check-inf ................: (5.14, 5.14)
    optimizer ......................................: (11.07, 11.10)
 iteration        3/     100 | consumed samples:           12 | elapsed time per iteration (ms): 239.4 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 1073741824.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (220.54, 220.60)
    forward-compute ................................: (96.17, 96.18)
    backward-compute ...............................: (123.76, 123.79)
    batch-generator ................................: (0.80, 0.89)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.57, 1.58)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.71, 4.71)
    optimizer ......................................: (9.79, 9.79)
 iteration        4/     100 | consumed samples:           16 | elapsed time per iteration (ms): 233.6 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 536870912.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (214.84, 214.89)
    forward-compute ................................: (87.22, 87.23)
    backward-compute ...............................: (126.99, 127.01)
    batch-generator ................................: (0.76, 0.85)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.57, 1.58)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.65, 4.66)
    optimizer ......................................: (9.73, 9.73)
 iteration        5/     100 | consumed samples:           20 | elapsed time per iteration (ms): 235.2 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 268435456.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (216.51, 216.52)
    forward-compute ................................: (92.22, 92.23)
    backward-compute ...............................: (123.65, 123.67)
    batch-generator ................................: (0.79, 0.87)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.58)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.65, 4.66)
    optimizer ......................................: (9.73, 9.73)
 iteration        6/     100 | consumed samples:           24 | elapsed time per iteration (ms): 231.5 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 134217728.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (212.82, 212.86)
    forward-compute ................................: (88.14, 88.14)
    backward-compute ...............................: (124.07, 124.09)
    batch-generator ................................: (0.76, 0.84)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.57, 1.58)
    optimizer-copy-to-main-grad ....................: (4.86, 4.89)
    optimizer-unscale-and-check-inf ................: (4.64, 4.64)
    optimizer ......................................: (9.71, 9.72)
 iteration        7/     100 | consumed samples:           28 | elapsed time per iteration (ms): 231.1 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 67108864.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (212.46, 212.48)
    forward-compute ................................: (87.96, 87.97)
    backward-compute ...............................: (123.86, 123.87)
    batch-generator ................................: (0.78, 0.88)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.57, 1.58)
    optimizer-copy-to-main-grad ....................: (4.86, 4.89)
    optimizer-unscale-and-check-inf ................: (4.61, 4.61)
    optimizer ......................................: (9.68, 9.68)
 iteration        8/     100 | consumed samples:           32 | elapsed time per iteration (ms): 231.5 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 33554432.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (212.85, 212.87)
    forward-compute ................................: (88.36, 88.36)
    backward-compute ...............................: (123.86, 123.89)
    batch-generator ................................: (0.79, 0.88)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.62)
    optimizer-copy-to-main-grad ....................: (4.86, 4.89)
    optimizer-unscale-and-check-inf ................: (4.56, 4.57)
    optimizer ......................................: (9.65, 9.66)
 iteration        9/     100 | consumed samples:           36 | elapsed time per iteration (ms): 230.5 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 16777216.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (211.87, 211.91)
    forward-compute ................................: (87.60, 87.61)
    backward-compute ...............................: (123.65, 123.66)
    batch-generator ................................: (0.77, 0.85)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.57, 1.59)
    optimizer-copy-to-main-grad ....................: (4.86, 4.89)
    optimizer-unscale-and-check-inf ................: (4.55, 4.57)
    optimizer ......................................: (9.63, 9.65)
 iteration       10/     100 | consumed samples:           40 | elapsed time per iteration (ms): 230.9 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 8388608.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (212.44, 212.48)
    forward-compute ................................: (88.22, 88.23)
    backward-compute ...............................: (123.58, 123.61)
    batch-generator ................................: (0.78, 0.87)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.57)
    optimizer-copy-to-main-grad ....................: (4.86, 4.89)
    optimizer-unscale-and-check-inf ................: (4.43, 4.44)
    optimizer ......................................: (9.51, 9.52)
 iteration       11/     100 | consumed samples:           44 | elapsed time per iteration (ms): 230.9 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 4194304.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (212.49, 212.51)
    forward-compute ................................: (88.05, 88.05)
    backward-compute ...............................: (123.83, 123.84)
    batch-generator ................................: (0.77, 0.87)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.57, 1.58)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.39, 4.40)
    optimizer ......................................: (9.46, 9.48)
 iteration       12/     100 | consumed samples:           48 | elapsed time per iteration (ms): 230.8 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 2097152.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (212.24, 212.29)
    forward-compute ................................: (87.77, 87.78)
    backward-compute ...............................: (123.83, 123.86)
    batch-generator ................................: (0.79, 0.87)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.57, 1.58)
    optimizer-copy-to-main-grad ....................: (4.86, 4.89)
    optimizer-unscale-and-check-inf ................: (4.39, 4.39)
    optimizer ......................................: (9.47, 9.47)
 iteration       13/     100 | consumed samples:           52 | elapsed time per iteration (ms): 235.7 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 1048576.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (217.30, 217.34)
    forward-compute ................................: (93.01, 93.02)
    backward-compute ...............................: (123.67, 123.69)
    batch-generator ................................: (0.76, 0.84)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.57, 1.58)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.33, 4.34)
    optimizer ......................................: (9.40, 9.40)
 iteration       14/     100 | consumed samples:           56 | elapsed time per iteration (ms): 230.3 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 524288.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (211.98, 212.03)
    forward-compute ................................: (87.55, 87.55)
    backward-compute ...............................: (123.80, 123.81)
    batch-generator ................................: (0.75, 0.83)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.58)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.27, 4.27)
    optimizer ......................................: (9.34, 9.34)
 iteration       15/     100 | consumed samples:           60 | elapsed time per iteration (ms): 230.6 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 262144.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (212.27, 212.31)
    forward-compute ................................: (88.06, 88.07)
    backward-compute ...............................: (123.60, 123.62)
    batch-generator ................................: (0.78, 0.86)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.58)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.24, 4.25)
    optimizer ......................................: (9.32, 9.32)
 iteration       16/     100 | consumed samples:           64 | elapsed time per iteration (ms): 230.8 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 131072.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (212.22, 212.28)
    forward-compute ................................: (87.81, 87.82)
    backward-compute ...............................: (123.80, 123.82)
    batch-generator ................................: (0.79, 0.88)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.58)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.44, 4.45)
    optimizer ......................................: (9.52, 9.53)
 iteration       17/     100 | consumed samples:           68 | elapsed time per iteration (ms): 229.9 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 65536.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (211.65, 211.68)
    forward-compute ................................: (87.53, 87.53)
    backward-compute ...............................: (123.49, 123.51)
    batch-generator ................................: (0.78, 0.87)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.57)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.22, 4.22)
    optimizer ......................................: (9.29, 9.29)
 iteration       18/     100 | consumed samples:           72 | elapsed time per iteration (ms): 230.2 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 32768.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (211.96, 211.98)
    forward-compute ................................: (87.83, 87.84)
    backward-compute ...............................: (123.51, 123.51)
    batch-generator ................................: (0.77, 0.85)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.57, 1.57)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.19, 4.19)
    optimizer ......................................: (9.27, 9.27)
 iteration       19/     100 | consumed samples:           76 | elapsed time per iteration (ms): 229.9 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 16384.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (211.70, 211.73)
    forward-compute ................................: (87.46, 87.46)
    backward-compute ...............................: (123.61, 123.63)
    batch-generator ................................: (0.76, 0.84)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.57, 1.58)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.23, 4.24)
    optimizer ......................................: (9.31, 9.31)
 iteration       20/     100 | consumed samples:           80 | elapsed time per iteration (ms): 230.8 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 8192.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (212.41, 212.46)
    forward-compute ................................: (87.93, 87.95)
    backward-compute ...............................: (123.81, 123.83)
    batch-generator ................................: (0.77, 0.86)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.57, 1.57)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.19, 4.20)
    optimizer ......................................: (9.27, 9.28)
 iteration       21/     100 | consumed samples:           84 | elapsed time per iteration (ms): 229.7 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 4096.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (211.48, 211.54)
    forward-compute ................................: (87.25, 87.26)
    backward-compute ...............................: (123.61, 123.63)
    batch-generator ................................: (0.75, 0.83)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.62)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.15, 4.16)
    optimizer ......................................: (9.22, 9.23)
 iteration       22/     100 | consumed samples:           88 | elapsed time per iteration (ms): 230.1 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 2048.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (211.86, 211.93)
    forward-compute ................................: (87.78, 87.79)
    backward-compute ...............................: (123.46, 123.47)
    batch-generator ................................: (0.80, 0.88)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.57, 1.58)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.20, 4.21)
    optimizer ......................................: (9.28, 9.28)
 iteration       23/     100 | consumed samples:           92 | elapsed time per iteration (ms): 230.3 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 1024.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (211.99, 212.04)
    forward-compute ................................: (87.94, 87.95)
    backward-compute ...............................: (123.43, 123.44)
    batch-generator ................................: (0.79, 0.89)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.58)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.14, 4.15)
    optimizer ......................................: (9.22, 9.23)
 iteration       24/     100 | consumed samples:           96 | elapsed time per iteration (ms): 230.9 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 512.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (212.63, 212.70)
    forward-compute ................................: (88.25, 88.26)
    backward-compute ...............................: (123.77, 123.77)
    batch-generator ................................: (0.77, 0.85)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.57, 1.58)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.22, 4.23)
    optimizer ......................................: (9.30, 9.31)
 iteration       25/     100 | consumed samples:          100 | elapsed time per iteration (ms): 259.5 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 1.118969E+01 | loss scale: 512.0 | grad norm: 5909.363 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (211.79, 211.82)
    forward-compute ................................: (87.60, 87.61)
    backward-compute ...............................: (123.57, 123.58)
    batch-generator ................................: (0.80, 0.90)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.58)
    optimizer-copy-to-main-grad ....................: (4.87, 4.88)
    optimizer-unscale-and-check-inf ................: (4.15, 4.16)
    optimizer-clip-main-grad .......................: (6.50, 6.50)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (16.84, 17.01)
    optimizer-copy-main-to-model-params ............: (5.21, 5.26)
    optimizer ......................................: (38.21, 38.26)
 iteration       26/     100 | consumed samples:          104 | elapsed time per iteration (ms): 314.2 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 9.799859E+00 | loss scale: 512.0 | grad norm: 32.871 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (270.71, 270.78)
    forward-compute ................................: (146.19, 146.20)
    backward-compute ...............................: (123.86, 123.87)
    batch-generator ................................: (0.79, 0.92)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.58)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.25, 4.25)
    optimizer-clip-main-grad .......................: (6.40, 6.41)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.50, 12.56)
    optimizer-copy-main-to-model-params ............: (5.17, 5.21)
    optimizer ......................................: (33.70, 33.74)
 iteration       27/     100 | consumed samples:          108 | elapsed time per iteration (ms): 254.9 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 1.130387E+01 | loss scale: 512.0 | grad norm: 45.343 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (211.34, 211.38)
    forward-compute ................................: (87.08, 87.11)
    backward-compute ...............................: (123.58, 123.59)
    batch-generator ................................: (0.80, 0.87)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.58)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.26, 4.27)
    optimizer-clip-main-grad .......................: (6.39, 6.40)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.51, 12.56)
    optimizer-copy-main-to-model-params ............: (5.17, 5.21)
    optimizer ......................................: (33.71, 33.76)
 iteration       28/     100 | consumed samples:          112 | elapsed time per iteration (ms): 255.4 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 1.063572E+01 | loss scale: 512.0 | grad norm: 13.759 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (211.93, 212.03)
    forward-compute ................................: (87.80, 87.81)
    backward-compute ...............................: (123.47, 123.49)
    batch-generator ................................: (0.76, 0.85)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.58)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.31, 4.31)
    optimizer-clip-main-grad .......................: (6.38, 6.39)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.45, 12.56)
    optimizer-copy-main-to-model-params ............: (5.17, 5.22)
    optimizer ......................................: (33.74, 33.79)
 iteration       29/     100 | consumed samples:          116 | elapsed time per iteration (ms): 255.7 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 1.027892E+01 | loss scale: 512.0 | grad norm: 3.405 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (212.18, 212.23)
    forward-compute ................................: (87.74, 87.74)
    backward-compute ...............................: (123.79, 123.80)
    batch-generator ................................: (0.78, 0.86)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.57)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.20, 4.21)
    optimizer-clip-main-grad .......................: (6.38, 6.39)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.46, 12.64)
    optimizer-copy-main-to-model-params ............: (5.17, 5.23)
    optimizer ......................................: (33.72, 33.77)
 iteration       30/     100 | consumed samples:          120 | elapsed time per iteration (ms): 255.7 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 9.582676E+00 | loss scale: 512.0 | grad norm: 3.705 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (212.09, 212.12)
    forward-compute ................................: (87.71, 87.72)
    backward-compute ...............................: (123.72, 123.73)
    batch-generator ................................: (0.79, 0.87)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.57)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.21, 4.22)
    optimizer-clip-main-grad .......................: (6.38, 6.40)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.48, 12.64)
    optimizer-copy-main-to-model-params ............: (5.18, 5.34)
    optimizer ......................................: (33.74, 33.91)
 iteration       31/     100 | consumed samples:          124 | elapsed time per iteration (ms): 255.1 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 9.388111E+00 | loss scale: 512.0 | grad norm: 3.601 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (211.59, 211.64)
    forward-compute ................................: (87.05, 87.06)
    backward-compute ...............................: (123.89, 123.90)
    batch-generator ................................: (0.77, 0.86)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.57)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.23, 4.23)
    optimizer-clip-main-grad .......................: (6.44, 6.45)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.47, 12.54)
    optimizer-copy-main-to-model-params ............: (5.16, 5.22)
    optimizer ......................................: (33.69, 33.75)
 iteration       32/     100 | consumed samples:          128 | elapsed time per iteration (ms): 255.2 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 8.714436E+00 | loss scale: 512.0 | grad norm: 2.757 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (211.75, 211.79)
    forward-compute ................................: (87.56, 87.57)
    backward-compute ...............................: (123.52, 123.55)
    batch-generator ................................: (0.76, 0.85)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.57)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.21, 4.22)
    optimizer-clip-main-grad .......................: (6.42, 6.42)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.50, 12.56)
    optimizer-copy-main-to-model-params ............: (5.16, 5.22)
    optimizer ......................................: (33.67, 33.73)
 iteration       33/     100 | consumed samples:          132 | elapsed time per iteration (ms): 254.9 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 8.461533E+00 | loss scale: 512.0 | grad norm: 3.640 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (211.23, 211.30)
    forward-compute ................................: (87.11, 87.11)
    backward-compute ...............................: (123.45, 123.46)
    batch-generator ................................: (0.78, 0.87)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.58)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.23, 4.24)
    optimizer-clip-main-grad .......................: (6.37, 6.40)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.51, 12.63)
    optimizer-copy-main-to-model-params ............: (5.17, 5.22)
    optimizer ......................................: (33.75, 33.80)
 iteration       34/     100 | consumed samples:          136 | elapsed time per iteration (ms): 254.8 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 8.673872E+00 | loss scale: 512.0 | grad norm: 77.872 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (211.21, 211.25)
    forward-compute ................................: (87.04, 87.04)
    backward-compute ...............................: (123.51, 123.52)
    batch-generator ................................: (0.75, 0.83)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.56)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.21, 4.22)
    optimizer-clip-main-grad .......................: (6.39, 6.40)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.52, 12.63)
    optimizer-copy-main-to-model-params ............: (5.18, 5.21)
    optimizer ......................................: (33.76, 33.81)
 iteration       35/     100 | consumed samples:          140 | elapsed time per iteration (ms): 255.4 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 8.848783E+00 | loss scale: 512.0 | grad norm: 20.106 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (211.72, 211.78)
    forward-compute ................................: (87.40, 87.41)
    backward-compute ...............................: (123.61, 123.63)
    batch-generator ................................: (0.78, 0.91)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.57)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.26, 4.26)
    optimizer-clip-main-grad .......................: (6.35, 6.38)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.46, 12.69)
    optimizer-copy-main-to-model-params ............: (5.17, 5.22)
    optimizer ......................................: (33.81, 33.86)
 iteration       36/     100 | consumed samples:          144 | elapsed time per iteration (ms): 255.8 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 8.672762E+00 | loss scale: 512.0 | grad norm: 13.270 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (212.25, 212.31)
    forward-compute ................................: (87.76, 87.76)
    backward-compute ...............................: (123.83, 123.86)
    batch-generator ................................: (0.78, 0.87)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.61)
    optimizer-copy-to-main-grad ....................: (4.86, 4.89)
    optimizer-unscale-and-check-inf ................: (4.23, 4.23)
    optimizer-clip-main-grad .......................: (6.37, 6.38)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.48, 12.61)
    optimizer-copy-main-to-model-params ............: (5.17, 5.22)
    optimizer ......................................: (33.71, 33.75)
 iteration       37/     100 | consumed samples:          148 | elapsed time per iteration (ms): 255.2 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 8.040772E+00 | loss scale: 512.0 | grad norm: 6.073 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (211.67, 211.72)
    forward-compute ................................: (87.52, 87.53)
    backward-compute ...............................: (123.47, 123.48)
    batch-generator ................................: (0.78, 0.87)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.57)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.23, 4.23)
    optimizer-clip-main-grad .......................: (6.45, 6.47)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.50, 12.56)
    optimizer-copy-main-to-model-params ............: (5.16, 5.22)
    optimizer ......................................: (33.73, 33.79)
 iteration       38/     100 | consumed samples:          152 | elapsed time per iteration (ms): 255.4 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.479229E+00 | loss scale: 512.0 | grad norm: 2.962 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (211.66, 211.72)
    forward-compute ................................: (87.29, 87.29)
    backward-compute ...............................: (123.72, 123.75)
    batch-generator ................................: (0.80, 0.89)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.57)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.21, 4.22)
    optimizer-clip-main-grad .......................: (6.37, 6.39)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.46, 12.67)
    optimizer-copy-main-to-model-params ............: (5.17, 5.21)
    optimizer ......................................: (33.78, 33.81)
 iteration       39/     100 | consumed samples:          156 | elapsed time per iteration (ms): 255.9 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.448360E+00 | loss scale: 512.0 | grad norm: 2.614 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (212.29, 212.38)
    forward-compute ................................: (88.01, 88.01)
    backward-compute ...............................: (123.63, 123.66)
    batch-generator ................................: (0.76, 0.86)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.57)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.21, 4.22)
    optimizer-clip-main-grad .......................: (6.37, 6.39)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.51, 12.56)
    optimizer-copy-main-to-model-params ............: (5.17, 5.23)
    optimizer ......................................: (33.66, 33.71)
 iteration       40/     100 | consumed samples:          160 | elapsed time per iteration (ms): 255.8 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.423147E+00 | loss scale: 512.0 | grad norm: 2.478 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (212.40, 212.45)
    forward-compute ................................: (87.92, 87.93)
    backward-compute ...............................: (123.82, 123.84)
    batch-generator ................................: (0.75, 0.84)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.56)
    optimizer-copy-to-main-grad ....................: (4.86, 4.87)
    optimizer-unscale-and-check-inf ................: (4.21, 4.22)
    optimizer-clip-main-grad .......................: (6.36, 6.39)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.48, 12.57)
    optimizer-copy-main-to-model-params ............: (5.16, 5.21)
    optimizer ......................................: (33.65, 33.71)
 iteration       41/     100 | consumed samples:          164 | elapsed time per iteration (ms): 255.5 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.496157E+00 | loss scale: 512.0 | grad norm: 2.378 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (211.95, 212.00)
    forward-compute ................................: (87.45, 87.46)
    backward-compute ...............................: (123.84, 123.86)
    batch-generator ................................: (0.78, 0.87)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.57)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.23, 4.24)
    optimizer-clip-main-grad .......................: (6.38, 6.39)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.53, 12.57)
    optimizer-copy-main-to-model-params ............: (5.17, 5.21)
    optimizer ......................................: (33.68, 33.72)
 iteration       42/     100 | consumed samples:          168 | elapsed time per iteration (ms): 255.3 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.151468E+00 | loss scale: 512.0 | grad norm: 1.893 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (211.88, 211.92)
    forward-compute ................................: (87.60, 87.60)
    backward-compute ...............................: (123.62, 123.63)
    batch-generator ................................: (0.77, 0.86)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.57)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.21, 4.21)
    optimizer-clip-main-grad .......................: (6.38, 6.39)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.50, 12.60)
    optimizer-copy-main-to-model-params ............: (5.17, 5.21)
    optimizer ......................................: (33.69, 33.73)
 iteration       43/     100 | consumed samples:          172 | elapsed time per iteration (ms): 255.2 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.404797E+00 | loss scale: 512.0 | grad norm: 1.553 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (211.66, 211.73)
    forward-compute ................................: (87.32, 87.33)
    backward-compute ...............................: (123.68, 123.71)
    batch-generator ................................: (0.78, 0.87)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.57)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.21, 4.22)
    optimizer-clip-main-grad .......................: (6.37, 6.38)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.49, 12.57)
    optimizer-copy-main-to-model-params ............: (5.18, 5.22)
    optimizer ......................................: (33.73, 33.77)
 iteration       44/     100 | consumed samples:          176 | elapsed time per iteration (ms): 254.8 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.179595E+00 | loss scale: 512.0 | grad norm: 1.785 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (211.28, 211.34)
    forward-compute ................................: (86.79, 86.80)
    backward-compute ...............................: (123.81, 123.82)
    batch-generator ................................: (0.77, 0.85)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.58)
    optimizer-copy-to-main-grad ....................: (4.86, 4.89)
    optimizer-unscale-and-check-inf ................: (4.23, 4.23)
    optimizer-clip-main-grad .......................: (6.43, 6.45)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.51, 12.66)
    optimizer-copy-main-to-model-params ............: (5.17, 5.21)
    optimizer ......................................: (33.83, 33.87)
 iteration       45/     100 | consumed samples:          180 | elapsed time per iteration (ms): 254.6 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.762846E+00 | loss scale: 512.0 | grad norm: 2.541 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (211.08, 211.15)
    forward-compute ................................: (86.87, 86.88)
    backward-compute ...............................: (123.55, 123.58)
    batch-generator ................................: (0.78, 0.87)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.58)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.21, 4.22)
    optimizer-clip-main-grad .......................: (6.36, 6.39)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.47, 12.57)
    optimizer-copy-main-to-model-params ............: (5.16, 5.21)
    optimizer ......................................: (33.64, 33.69)
 iteration       46/     100 | consumed samples:          184 | elapsed time per iteration (ms): 255.6 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.326547E+00 | loss scale: 512.0 | grad norm: 3.103 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (212.18, 212.21)
    forward-compute ................................: (87.92, 87.93)
    backward-compute ...............................: (123.58, 123.60)
    batch-generator ................................: (0.75, 0.83)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.57)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.21, 4.22)
    optimizer-clip-main-grad .......................: (6.36, 6.39)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.45, 12.58)
    optimizer-copy-main-to-model-params ............: (5.16, 5.22)
    optimizer ......................................: (33.66, 33.72)
 iteration       47/     100 | consumed samples:          188 | elapsed time per iteration (ms): 255.6 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.379846E+00 | loss scale: 512.0 | grad norm: 3.210 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (212.08, 212.12)
    forward-compute ................................: (87.75, 87.76)
    backward-compute ...............................: (123.68, 123.70)
    batch-generator ................................: (0.78, 0.87)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.57)
    optimizer-copy-to-main-grad ....................: (4.87, 4.88)
    optimizer-unscale-and-check-inf ................: (4.21, 4.22)
    optimizer-clip-main-grad .......................: (6.37, 6.39)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.48, 12.63)
    optimizer-copy-main-to-model-params ............: (5.18, 5.22)
    optimizer ......................................: (33.73, 33.77)
 iteration       48/     100 | consumed samples:          192 | elapsed time per iteration (ms): 255.6 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 8.775054E+00 | loss scale: 512.0 | grad norm: 3.687 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (212.15, 212.21)
    forward-compute ................................: (87.72, 87.72)
    backward-compute ...............................: (123.77, 123.79)
    batch-generator ................................: (0.76, 0.85)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.57)
    optimizer-copy-to-main-grad ....................: (4.86, 4.89)
    optimizer-unscale-and-check-inf ................: (4.21, 4.22)
    optimizer-clip-main-grad .......................: (6.38, 6.39)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.48, 12.57)
    optimizer-copy-main-to-model-params ............: (5.17, 5.21)
    optimizer ......................................: (33.67, 33.72)
 iteration       49/     100 | consumed samples:          196 | elapsed time per iteration (ms): 255.6 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 8.118700E+00 | loss scale: 512.0 | grad norm: 4.818 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (212.21, 212.27)
    forward-compute ................................: (87.85, 87.85)
    backward-compute ...............................: (123.71, 123.73)
    batch-generator ................................: (0.78, 0.86)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.60)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.21, 4.22)
    optimizer-clip-main-grad .......................: (6.38, 6.39)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.53, 12.58)
    optimizer-copy-main-to-model-params ............: (5.16, 5.21)
    optimizer ......................................: (33.66, 33.71)
 iteration       50/     100 | consumed samples:          200 | elapsed time per iteration (ms): 255.6 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.130795E+00 | loss scale: 512.0 | grad norm: 1.657 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (212.01, 212.06)
    forward-compute ................................: (87.79, 87.80)
    backward-compute ...............................: (123.56, 123.57)
    batch-generator ................................: (0.77, 0.86)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.57)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.23, 4.24)
    optimizer-clip-main-grad .......................: (6.36, 6.40)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.48, 12.60)
    optimizer-copy-main-to-model-params ............: (5.22, 5.28)
    optimizer ......................................: (33.77, 33.83)
 iteration       51/     100 | consumed samples:          204 | elapsed time per iteration (ms): 255.6 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.803830E+00 | loss scale: 512.0 | grad norm: 1.975 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (212.07, 212.15)
    forward-compute ................................: (87.57, 87.58)
    backward-compute ...............................: (123.85, 123.88)
    batch-generator ................................: (0.77, 0.86)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.57)
    optimizer-copy-to-main-grad ....................: (4.86, 4.89)
    optimizer-unscale-and-check-inf ................: (4.22, 4.23)
    optimizer-clip-main-grad .......................: (6.37, 6.39)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.52, 12.58)
    optimizer-copy-main-to-model-params ............: (5.16, 5.21)
    optimizer ......................................: (33.67, 33.72)
 iteration       52/     100 | consumed samples:          208 | elapsed time per iteration (ms): 255.7 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.477561E+00 | loss scale: 512.0 | grad norm: 1.975 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (212.24, 212.32)
    forward-compute ................................: (87.79, 87.81)
    backward-compute ...............................: (123.79, 123.80)
    batch-generator ................................: (0.76, 0.85)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.56)
    optimizer-copy-to-main-grad ....................: (4.87, 4.88)
    optimizer-unscale-and-check-inf ................: (4.24, 4.24)
    optimizer-clip-main-grad .......................: (6.37, 6.38)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.53, 12.56)
    optimizer-copy-main-to-model-params ............: (5.17, 5.21)
    optimizer ......................................: (33.66, 33.71)
 iteration       53/     100 | consumed samples:          212 | elapsed time per iteration (ms): 255.1 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.379244E+00 | loss scale: 512.0 | grad norm: 1.500 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (211.59, 211.66)
    forward-compute ................................: (87.26, 87.26)
    backward-compute ...............................: (123.68, 123.69)
    batch-generator ................................: (0.78, 0.87)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.57)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.21, 4.22)
    optimizer-clip-main-grad .......................: (6.42, 6.44)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.50, 12.62)
    optimizer-copy-main-to-model-params ............: (5.17, 5.21)
    optimizer ......................................: (33.76, 33.80)
 iteration       54/     100 | consumed samples:          216 | elapsed time per iteration (ms): 255.5 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.638558E+00 | loss scale: 512.0 | grad norm: 1.431 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (212.09, 212.13)
    forward-compute ................................: (87.82, 87.83)
    backward-compute ...............................: (123.61, 123.63)
    batch-generator ................................: (0.79, 0.88)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.57)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.22, 4.22)
    optimizer-clip-main-grad .......................: (6.41, 6.43)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.53, 12.58)
    optimizer-copy-main-to-model-params ............: (5.17, 5.21)
    optimizer ......................................: (33.71, 33.75)
 iteration       55/     100 | consumed samples:          220 | elapsed time per iteration (ms): 255.4 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.298699E+00 | loss scale: 512.0 | grad norm: 1.864 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (211.92, 211.96)
    forward-compute ................................: (87.61, 87.62)
    backward-compute ...............................: (123.64, 123.65)
    batch-generator ................................: (0.75, 0.84)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.57)
    optimizer-copy-to-main-grad ....................: (4.86, 4.87)
    optimizer-unscale-and-check-inf ................: (4.23, 4.23)
    optimizer-clip-main-grad .......................: (6.38, 6.39)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.47, 12.63)
    optimizer-copy-main-to-model-params ............: (5.18, 5.21)
    optimizer ......................................: (33.74, 33.77)
 iteration       56/     100 | consumed samples:          224 | elapsed time per iteration (ms): 254.7 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.152692E+00 | loss scale: 512.0 | grad norm: 1.265 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (211.23, 211.28)
    forward-compute ................................: (86.91, 86.91)
    backward-compute ...............................: (123.65, 123.66)
    batch-generator ................................: (0.76, 0.84)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.57)
    optimizer-copy-to-main-grad ....................: (4.87, 4.89)
    optimizer-unscale-and-check-inf ................: (4.22, 4.22)
    optimizer-clip-main-grad .......................: (6.37, 6.40)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.51, 12.60)
    optimizer-copy-main-to-model-params ............: (5.16, 5.22)
    optimizer ......................................: (33.70, 33.77)
 iteration       57/     100 | consumed samples:          228 | elapsed time per iteration (ms): 255.9 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.237670E+00 | loss scale: 512.0 | grad norm: 1.419 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (212.15, 212.20)
    forward-compute ................................: (87.78, 87.79)
    backward-compute ...............................: (123.70, 123.71)
    batch-generator ................................: (0.79, 0.87)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.58)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.30, 4.31)
    optimizer-clip-main-grad .......................: (6.40, 6.41)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.51, 12.71)
    optimizer-copy-main-to-model-params ............: (5.18, 5.22)
    optimizer ......................................: (33.92, 33.97)
 iteration       58/     100 | consumed samples:          232 | elapsed time per iteration (ms): 257.1 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.347966E+00 | loss scale: 512.0 | grad norm: 2.082 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (211.40, 211.44)
    forward-compute ................................: (86.98, 86.98)
    backward-compute ...............................: (123.76, 123.78)
    batch-generator ................................: (0.80, 0.89)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.57)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.22, 4.23)
    optimizer-clip-main-grad .......................: (6.36, 6.38)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.50, 12.56)
    optimizer-copy-main-to-model-params ............: (5.16, 5.21)
    optimizer ......................................: (33.63, 33.68)
 iteration       59/     100 | consumed samples:          236 | elapsed time per iteration (ms): 255.4 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.545876E+00 | loss scale: 512.0 | grad norm: 1.683 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (211.82, 211.86)
    forward-compute ................................: (87.38, 87.39)
    backward-compute ...............................: (123.75, 123.77)
    batch-generator ................................: (0.80, 0.89)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.57)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.24, 4.24)
    optimizer-clip-main-grad .......................: (6.38, 6.39)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.49, 12.66)
    optimizer-copy-main-to-model-params ............: (5.18, 5.22)
    optimizer ......................................: (33.78, 33.83)
 iteration       60/     100 | consumed samples:          240 | elapsed time per iteration (ms): 255.2 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.266466E+00 | loss scale: 512.0 | grad norm: 1.212 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (211.70, 211.74)
    forward-compute ................................: (87.55, 87.64)
    backward-compute ...............................: (123.41, 123.50)
    batch-generator ................................: (0.78, 0.88)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.58)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.22, 4.23)
    optimizer-clip-main-grad .......................: (6.37, 6.40)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.51, 12.65)
    optimizer-copy-main-to-model-params ............: (5.16, 5.21)
    optimizer ......................................: (33.74, 33.79)
 iteration       61/     100 | consumed samples:          244 | elapsed time per iteration (ms): 254.9 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.355679E+00 | loss scale: 512.0 | grad norm: 1.182 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (211.54, 211.59)
    forward-compute ................................: (87.44, 87.45)
    backward-compute ...............................: (123.44, 123.47)
    batch-generator ................................: (0.79, 0.89)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.57)
    optimizer-copy-to-main-grad ....................: (4.86, 4.87)
    optimizer-unscale-and-check-inf ................: (4.22, 4.23)
    optimizer-clip-main-grad .......................: (6.36, 6.38)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.47, 12.55)
    optimizer-copy-main-to-model-params ............: (5.17, 5.21)
    optimizer ......................................: (33.63, 33.67)
 iteration       62/     100 | consumed samples:          248 | elapsed time per iteration (ms): 256.0 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.480498E+00 | loss scale: 512.0 | grad norm: 1.299 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (212.36, 212.41)
    forward-compute ................................: (87.12, 87.13)
    backward-compute ...............................: (124.56, 124.57)
    batch-generator ................................: (0.77, 0.86)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.57)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.25, 4.26)
    optimizer-clip-main-grad .......................: (6.37, 6.40)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.52, 12.64)
    optimizer-copy-main-to-model-params ............: (5.17, 5.21)
    optimizer ......................................: (33.78, 33.82)
 iteration       63/     100 | consumed samples:          252 | elapsed time per iteration (ms): 256.3 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 8.155821E+00 | loss scale: 512.0 | grad norm: 1.550 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (212.38, 212.85)
    forward-compute ................................: (88.07, 88.08)
    backward-compute ...............................: (123.64, 124.06)
    batch-generator ................................: (0.77, 0.85)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.58)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.24, 4.25)
    optimizer-clip-main-grad .......................: (6.37, 6.38)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.51, 12.57)
    optimizer-copy-main-to-model-params ............: (5.17, 5.21)
    optimizer ......................................: (33.70, 33.74)
 iteration       64/     100 | consumed samples:          256 | elapsed time per iteration (ms): 300.5 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.119409E+00 | loss scale: 512.0 | grad norm: 1.897 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (212.14, 212.17)
    forward-compute ................................: (87.90, 87.90)
    backward-compute ...............................: (123.57, 123.59)
    batch-generator ................................: (0.77, 0.86)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.58)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.28, 4.34)
    optimizer-clip-main-grad .......................: (6.37, 6.39)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.49, 12.59)
    optimizer-copy-main-to-model-params ............: (5.20, 49.81)
    optimizer ......................................: (33.82, 78.42)
 iteration       65/     100 | consumed samples:          260 | elapsed time per iteration (ms): 255.7 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.282529E+00 | loss scale: 512.0 | grad norm: 1.044 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (211.93, 211.96)
    forward-compute ................................: (87.73, 87.75)
    backward-compute ...............................: (123.53, 123.54)
    batch-generator ................................: (0.79, 0.90)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.58)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.27, 4.27)
    optimizer-clip-main-grad .......................: (6.37, 6.38)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.52, 12.54)
    optimizer-copy-main-to-model-params ............: (5.21, 5.30)
    optimizer ......................................: (33.73, 33.81)
 iteration       66/     100 | consumed samples:          264 | elapsed time per iteration (ms): 254.6 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.319403E+00 | loss scale: 512.0 | grad norm: 1.567 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (211.25, 211.30)
    forward-compute ................................: (86.97, 86.98)
    backward-compute ...............................: (123.63, 123.65)
    batch-generator ................................: (0.78, 0.87)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.58)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.25, 4.26)
    optimizer-clip-main-grad .......................: (6.43, 6.45)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.47, 12.52)
    optimizer-copy-main-to-model-params ............: (5.16, 5.22)
    optimizer ......................................: (33.70, 33.76)
 iteration       67/     100 | consumed samples:          268 | elapsed time per iteration (ms): 255.2 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.018765E+00 | loss scale: 512.0 | grad norm: 1.309 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (211.75, 211.81)
    forward-compute ................................: (87.36, 87.36)
    backward-compute ...............................: (123.69, 123.71)
    batch-generator ................................: (0.80, 0.89)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.58)
    optimizer-copy-to-main-grad ....................: (4.87, 4.88)
    optimizer-unscale-and-check-inf ................: (4.22, 4.23)
    optimizer-clip-main-grad .......................: (6.36, 6.38)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.48, 12.56)
    optimizer-copy-main-to-model-params ............: (5.17, 5.22)
    optimizer ......................................: (33.66, 33.70)
 iteration       68/     100 | consumed samples:          272 | elapsed time per iteration (ms): 255.6 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.690004E+00 | loss scale: 512.0 | grad norm: 1.776 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (212.12, 212.17)
    forward-compute ................................: (88.00, 88.00)
    backward-compute ...............................: (123.47, 123.48)
    batch-generator ................................: (0.77, 0.86)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.57)
    optimizer-copy-to-main-grad ....................: (4.86, 4.87)
    optimizer-unscale-and-check-inf ................: (4.21, 4.22)
    optimizer-clip-main-grad .......................: (6.37, 6.39)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.48, 12.63)
    optimizer-copy-main-to-model-params ............: (5.17, 5.21)
    optimizer ......................................: (33.71, 33.75)
 iteration       69/     100 | consumed samples:          276 | elapsed time per iteration (ms): 255.5 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.589587E+00 | loss scale: 512.0 | grad norm: 1.622 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (211.94, 212.03)
    forward-compute ................................: (87.64, 87.65)
    backward-compute ...............................: (123.66, 123.68)
    batch-generator ................................: (0.78, 0.86)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.58)
    optimizer-copy-to-main-grad ....................: (4.86, 4.89)
    optimizer-unscale-and-check-inf ................: (4.20, 4.21)
    optimizer-clip-main-grad .......................: (6.38, 6.39)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.48, 12.59)
    optimizer-copy-main-to-model-params ............: (5.17, 5.22)
    optimizer ......................................: (33.69, 33.74)
 iteration       70/     100 | consumed samples:          280 | elapsed time per iteration (ms): 254.9 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.132987E+00 | loss scale: 512.0 | grad norm: 1.186 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (211.32, 211.38)
    forward-compute ................................: (87.07, 87.08)
    backward-compute ...............................: (123.59, 123.61)
    batch-generator ................................: (0.76, 0.84)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.57)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.22, 4.23)
    optimizer-clip-main-grad .......................: (6.37, 6.38)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.48, 12.61)
    optimizer-copy-main-to-model-params ............: (5.18, 5.22)
    optimizer ......................................: (33.72, 33.84)
 iteration       71/     100 | consumed samples:          284 | elapsed time per iteration (ms): 256.0 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.562172E+00 | loss scale: 512.0 | grad norm: 1.340 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (212.58, 212.64)
    forward-compute ................................: (87.95, 87.96)
    backward-compute ...............................: (123.95, 123.97)
    batch-generator ................................: (0.75, 0.84)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.57, 1.58)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.26, 4.26)
    optimizer-clip-main-grad .......................: (6.37, 6.38)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.46, 12.59)
    optimizer-copy-main-to-model-params ............: (5.19, 5.21)
    optimizer ......................................: (33.74, 33.77)
 iteration       72/     100 | consumed samples:          288 | elapsed time per iteration (ms): 254.8 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.360419E+00 | loss scale: 512.0 | grad norm: 1.452 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (211.28, 211.33)
    forward-compute ................................: (87.16, 87.17)
    backward-compute ...............................: (123.46, 123.48)
    batch-generator ................................: (0.78, 0.86)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.57)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.23, 4.24)
    optimizer-clip-main-grad .......................: (6.44, 6.46)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.51, 12.60)
    optimizer-copy-main-to-model-params ............: (5.17, 5.21)
    optimizer ......................................: (33.76, 33.80)
 iteration       73/     100 | consumed samples:          292 | elapsed time per iteration (ms): 255.4 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.110188E+00 | loss scale: 512.0 | grad norm: 1.685 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (211.86, 211.93)
    forward-compute ................................: (87.26, 87.27)
    backward-compute ...............................: (123.96, 123.97)
    batch-generator ................................: (0.77, 0.85)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.57, 1.57)
    optimizer-copy-to-main-grad ....................: (4.87, 4.88)
    optimizer-unscale-and-check-inf ................: (4.25, 4.26)
    optimizer-clip-main-grad .......................: (6.43, 6.45)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.50, 12.57)
    optimizer-copy-main-to-model-params ............: (5.17, 5.21)
    optimizer ......................................: (33.77, 33.81)
 iteration       74/     100 | consumed samples:          296 | elapsed time per iteration (ms): 254.5 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.443500E+00 | loss scale: 512.0 | grad norm: 1.297 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (210.90, 210.94)
    forward-compute ................................: (86.72, 86.72)
    backward-compute ...............................: (123.52, 123.55)
    batch-generator ................................: (0.77, 0.85)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.58)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.22, 4.22)
    optimizer-clip-main-grad .......................: (6.47, 6.49)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.51, 12.58)
    optimizer-copy-main-to-model-params ............: (5.20, 5.24)
    optimizer ......................................: (33.80, 33.85)
 iteration       75/     100 | consumed samples:          300 | elapsed time per iteration (ms): 257.7 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.035624E+00 | loss scale: 512.0 | grad norm: 1.160 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (213.37, 213.42)
    forward-compute ................................: (88.54, 88.55)
    backward-compute ...............................: (124.12, 124.14)
    batch-generator ................................: (0.80, 0.90)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.57)
    optimizer-copy-to-main-grad ....................: (4.86, 4.89)
    optimizer-unscale-and-check-inf ................: (4.25, 4.26)
    optimizer-clip-main-grad .......................: (6.63, 6.64)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.55, 12.61)
    optimizer-copy-main-to-model-params ............: (5.19, 5.25)
    optimizer ......................................: (34.06, 34.12)
 iteration       76/     100 | consumed samples:          304 | elapsed time per iteration (ms): 259.2 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.256980E+00 | loss scale: 512.0 | grad norm: 1.127 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (215.45, 215.49)
    forward-compute ................................: (90.21, 90.23)
    backward-compute ...............................: (124.49, 124.52)
    batch-generator ................................: (0.79, 0.89)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.57)
    optimizer-copy-to-main-grad ....................: (4.87, 4.89)
    optimizer-unscale-and-check-inf ................: (4.25, 4.25)
    optimizer-clip-main-grad .......................: (6.43, 6.45)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.51, 12.60)
    optimizer-copy-main-to-model-params ............: (5.17, 5.22)
    optimizer ......................................: (33.80, 33.85)
 iteration       77/     100 | consumed samples:          308 | elapsed time per iteration (ms): 302.6 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.241488E+00 | loss scale: 512.0 | grad norm: 1.540 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (212.78, 212.87)
    forward-compute ................................: (88.33, 88.34)
    backward-compute ...............................: (123.79, 123.81)
    batch-generator ................................: (0.78, 0.89)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.57)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (49.91, 49.97)
    optimizer-clip-main-grad .......................: (6.64, 6.64)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.53, 12.76)
    optimizer-copy-main-to-model-params ............: (5.24, 5.28)
    optimizer ......................................: (79.96, 80.01)
 iteration       78/     100 | consumed samples:          312 | elapsed time per iteration (ms): 258.0 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.205489E+00 | loss scale: 512.0 | grad norm: 1.335 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (214.52, 214.58)
    forward-compute ................................: (90.10, 90.11)
    backward-compute ...............................: (123.76, 123.78)
    batch-generator ................................: (0.80, 0.89)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.57)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.21, 4.22)
    optimizer-clip-main-grad .......................: (6.46, 6.47)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.51, 12.59)
    optimizer-copy-main-to-model-params ............: (5.16, 5.21)
    optimizer ......................................: (33.74, 33.79)
 iteration       79/     100 | consumed samples:          316 | elapsed time per iteration (ms): 259.9 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.182086E+00 | loss scale: 512.0 | grad norm: 1.464 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (216.44, 216.50)
    forward-compute ................................: (92.19, 92.20)
    backward-compute ...............................: (123.58, 123.59)
    batch-generator ................................: (0.78, 0.86)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.58)
    optimizer-copy-to-main-grad ....................: (4.86, 4.87)
    optimizer-unscale-and-check-inf ................: (4.21, 4.22)
    optimizer-clip-main-grad .......................: (6.44, 6.47)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.50, 12.60)
    optimizer-copy-main-to-model-params ............: (5.18, 5.21)
    optimizer ......................................: (33.77, 33.79)
 iteration       80/     100 | consumed samples:          320 | elapsed time per iteration (ms): 255.7 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 6.941305E+00 | loss scale: 512.0 | grad norm: 1.622 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (212.26, 212.37)
    forward-compute ................................: (87.80, 87.81)
    backward-compute ...............................: (123.81, 123.83)
    batch-generator ................................: (0.79, 0.88)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.56)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.20, 4.21)
    optimizer-clip-main-grad .......................: (6.43, 6.44)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.48, 12.60)
    optimizer-copy-main-to-model-params ............: (5.16, 5.20)
    optimizer ......................................: (33.72, 33.77)
 iteration       81/     100 | consumed samples:          324 | elapsed time per iteration (ms): 255.7 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.982098E+00 | loss scale: 512.0 | grad norm: 1.623 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (212.13, 212.16)
    forward-compute ................................: (87.81, 87.82)
    backward-compute ...............................: (123.66, 123.68)
    batch-generator ................................: (0.80, 0.88)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.58)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.22, 4.23)
    optimizer-clip-main-grad .......................: (6.47, 6.49)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.49, 12.56)
    optimizer-copy-main-to-model-params ............: (5.16, 5.21)
    optimizer ......................................: (33.75, 33.80)
 iteration       82/     100 | consumed samples:          328 | elapsed time per iteration (ms): 255.6 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.139208E+00 | loss scale: 512.0 | grad norm: 1.132 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (212.06, 212.12)
    forward-compute ................................: (87.65, 87.65)
    backward-compute ...............................: (123.76, 123.78)
    batch-generator ................................: (0.76, 0.84)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.57)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.24, 4.25)
    optimizer-clip-main-grad .......................: (6.39, 6.40)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.51, 12.58)
    optimizer-copy-main-to-model-params ............: (5.16, 5.22)
    optimizer ......................................: (33.70, 33.76)
 iteration       83/     100 | consumed samples:          332 | elapsed time per iteration (ms): 255.6 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 6.914919E+00 | loss scale: 512.0 | grad norm: 1.378 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (212.20, 212.28)
    forward-compute ................................: (87.89, 87.89)
    backward-compute ...............................: (123.65, 123.67)
    batch-generator ................................: (0.77, 0.86)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.57)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.21, 4.21)
    optimizer-clip-main-grad .......................: (6.38, 6.40)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.47, 12.56)
    optimizer-copy-main-to-model-params ............: (5.17, 5.21)
    optimizer ......................................: (33.65, 33.70)
 iteration       84/     100 | consumed samples:          336 | elapsed time per iteration (ms): 255.4 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 6.913953E+00 | loss scale: 512.0 | grad norm: 1.139 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (211.88, 211.92)
    forward-compute ................................: (87.82, 87.82)
    backward-compute ...............................: (123.42, 123.43)
    batch-generator ................................: (0.77, 0.86)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.57)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.21, 4.22)
    optimizer-clip-main-grad .......................: (6.38, 6.39)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.46, 12.56)
    optimizer-copy-main-to-model-params ............: (5.17, 5.21)
    optimizer ......................................: (33.67, 33.71)
 iteration       85/     100 | consumed samples:          340 | elapsed time per iteration (ms): 255.3 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.121049E+00 | loss scale: 512.0 | grad norm: 1.182 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (211.75, 211.80)
    forward-compute ................................: (87.37, 87.38)
    backward-compute ...............................: (123.71, 123.73)
    batch-generator ................................: (0.76, 0.85)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.58)
    optimizer-copy-to-main-grad ....................: (4.87, 4.89)
    optimizer-unscale-and-check-inf ................: (4.22, 4.22)
    optimizer-clip-main-grad .......................: (6.38, 6.39)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.48, 12.62)
    optimizer-copy-main-to-model-params ............: (5.17, 5.21)
    optimizer ......................................: (33.72, 33.78)
 iteration       86/     100 | consumed samples:          344 | elapsed time per iteration (ms): 255.6 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.319918E+00 | loss scale: 512.0 | grad norm: 1.318 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (211.98, 212.05)
    forward-compute ................................: (87.63, 87.64)
    backward-compute ...............................: (123.66, 123.68)
    batch-generator ................................: (0.75, 0.84)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.57)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.23, 4.24)
    optimizer-clip-main-grad .......................: (6.36, 6.39)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.46, 12.55)
    optimizer-copy-main-to-model-params ............: (5.24, 5.30)
    optimizer ......................................: (33.73, 33.78)
 iteration       87/     100 | consumed samples:          348 | elapsed time per iteration (ms): 255.2 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.318967E+00 | loss scale: 512.0 | grad norm: 1.286 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (211.70, 211.76)
    forward-compute ................................: (87.50, 87.51)
    backward-compute ...............................: (123.55, 123.56)
    batch-generator ................................: (0.75, 0.84)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.58)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.21, 4.22)
    optimizer-clip-main-grad .......................: (6.37, 6.38)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.53, 12.60)
    optimizer-copy-main-to-model-params ............: (5.19, 5.21)
    optimizer ......................................: (33.69, 33.72)
 iteration       88/     100 | consumed samples:          352 | elapsed time per iteration (ms): 256.1 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.409718E+00 | loss scale: 512.0 | grad norm: 1.552 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (212.43, 212.47)
    forward-compute ................................: (88.36, 88.37)
    backward-compute ...............................: (123.41, 123.42)
    batch-generator ................................: (0.78, 0.88)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.57)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.29, 4.30)
    optimizer-clip-main-grad .......................: (6.37, 6.39)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.49, 12.64)
    optimizer-copy-main-to-model-params ............: (5.17, 5.22)
    optimizer ......................................: (33.82, 33.87)
 iteration       89/     100 | consumed samples:          356 | elapsed time per iteration (ms): 256.2 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.556677E+00 | loss scale: 512.0 | grad norm: 1.368 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (212.44, 212.47)
    forward-compute ................................: (88.14, 88.15)
    backward-compute ...............................: (123.61, 123.62)
    batch-generator ................................: (0.80, 0.89)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.58)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.26, 4.28)
    optimizer-clip-main-grad .......................: (6.37, 6.38)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.52, 12.63)
    optimizer-copy-main-to-model-params ............: (5.18, 5.29)
    optimizer ......................................: (33.90, 34.01)
 iteration       90/     100 | consumed samples:          360 | elapsed time per iteration (ms): 255.7 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.851620E+00 | loss scale: 512.0 | grad norm: 2.489 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (212.21, 212.27)
    forward-compute ................................: (88.08, 88.08)
    backward-compute ...............................: (123.45, 123.47)
    batch-generator ................................: (0.80, 0.89)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.57, 1.58)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.27, 4.27)
    optimizer-clip-main-grad .......................: (6.38, 6.39)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.52, 12.54)
    optimizer-copy-main-to-model-params ............: (5.17, 5.21)
    optimizer ......................................: (33.70, 33.74)
 iteration       91/     100 | consumed samples:          364 | elapsed time per iteration (ms): 255.9 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 6.828784E+00 | loss scale: 512.0 | grad norm: 1.514 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (212.37, 212.43)
    forward-compute ................................: (88.12, 88.13)
    backward-compute ...............................: (123.61, 123.62)
    batch-generator ................................: (0.77, 0.87)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.57)
    optimizer-copy-to-main-grad ....................: (4.86, 4.89)
    optimizer-unscale-and-check-inf ................: (4.25, 4.26)
    optimizer-clip-main-grad .......................: (6.41, 6.43)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.49, 12.61)
    optimizer-copy-main-to-model-params ............: (5.16, 5.21)
    optimizer ......................................: (33.77, 33.82)
 iteration       92/     100 | consumed samples:          368 | elapsed time per iteration (ms): 255.2 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 6.915037E+00 | loss scale: 512.0 | grad norm: 2.867 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (211.54, 211.64)
    forward-compute ................................: (87.30, 87.31)
    backward-compute ...............................: (123.58, 123.61)
    batch-generator ................................: (0.79, 0.88)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.57)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.20, 4.20)
    optimizer-clip-main-grad .......................: (6.43, 6.45)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.50, 12.67)
    optimizer-copy-main-to-model-params ............: (5.17, 5.21)
    optimizer ......................................: (33.81, 33.85)
 iteration       93/     100 | consumed samples:          372 | elapsed time per iteration (ms): 254.8 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.175044E+00 | loss scale: 512.0 | grad norm: 1.587 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (211.41, 211.44)
    forward-compute ................................: (87.35, 87.36)
    backward-compute ...............................: (123.41, 123.42)
    batch-generator ................................: (0.76, 0.84)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.57)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.21, 4.23)
    optimizer-clip-main-grad .......................: (6.36, 6.39)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.49, 12.54)
    optimizer-copy-main-to-model-params ............: (5.16, 5.21)
    optimizer ......................................: (33.62, 33.67)
 iteration       94/     100 | consumed samples:          376 | elapsed time per iteration (ms): 256.2 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 8.060285E+00 | loss scale: 512.0 | grad norm: 2.092 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (212.71, 212.76)
    forward-compute ................................: (87.17, 87.20)
    backward-compute ...............................: (124.85, 124.88)
    batch-generator ................................: (0.77, 0.86)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.62)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.22, 4.23)
    optimizer-clip-main-grad .......................: (6.37, 6.39)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.49, 12.60)
    optimizer-copy-main-to-model-params ............: (5.17, 5.21)
    optimizer ......................................: (33.70, 33.73)
 iteration       95/     100 | consumed samples:          380 | elapsed time per iteration (ms): 255.5 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.573568E+00 | loss scale: 512.0 | grad norm: 3.239 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (212.09, 212.14)
    forward-compute ................................: (87.91, 87.92)
    backward-compute ...............................: (123.51, 123.53)
    batch-generator ................................: (0.76, 0.85)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.57)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.22, 4.23)
    optimizer-clip-main-grad .......................: (6.37, 6.39)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.48, 12.55)
    optimizer-copy-main-to-model-params ............: (5.16, 5.22)
    optimizer ......................................: (33.64, 33.69)
 iteration       96/     100 | consumed samples:          384 | elapsed time per iteration (ms): 255.0 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.117379E+00 | loss scale: 512.0 | grad norm: 1.444 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (211.53, 211.57)
    forward-compute ................................: (87.08, 87.09)
    backward-compute ...............................: (123.79, 123.80)
    batch-generator ................................: (0.77, 0.86)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.57)
    optimizer-copy-to-main-grad ....................: (4.86, 4.89)
    optimizer-unscale-and-check-inf ................: (4.20, 4.21)
    optimizer-clip-main-grad .......................: (6.37, 6.39)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.48, 12.64)
    optimizer-copy-main-to-model-params ............: (5.17, 5.21)
    optimizer ......................................: (33.74, 33.78)
 iteration       97/     100 | consumed samples:          388 | elapsed time per iteration (ms): 255.2 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.181218E+00 | loss scale: 512.0 | grad norm: 1.586 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (211.77, 211.87)
    forward-compute ................................: (87.68, 87.69)
    backward-compute ...............................: (123.44, 123.46)
    batch-generator ................................: (0.76, 0.86)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.57, 1.57)
    optimizer-copy-to-main-grad ....................: (4.86, 4.89)
    optimizer-unscale-and-check-inf ................: (4.20, 4.22)
    optimizer-clip-main-grad .......................: (6.36, 6.37)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.49, 12.57)
    optimizer-copy-main-to-model-params ............: (5.16, 5.28)
    optimizer ......................................: (33.64, 33.76)
 iteration       98/     100 | consumed samples:          392 | elapsed time per iteration (ms): 255.6 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.223674E+00 | loss scale: 512.0 | grad norm: 1.839 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (212.10, 212.16)
    forward-compute ................................: (87.95, 87.96)
    backward-compute ...............................: (123.46, 123.48)
    batch-generator ................................: (0.79, 0.88)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.57)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.24, 4.26)
    optimizer-clip-main-grad .......................: (6.37, 6.38)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.49, 12.55)
    optimizer-copy-main-to-model-params ............: (5.16, 5.21)
    optimizer ......................................: (33.66, 33.71)
 iteration       99/     100 | consumed samples:          396 | elapsed time per iteration (ms): 255.7 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 6.860013E+00 | loss scale: 512.0 | grad norm: 1.457 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (212.21, 212.28)
    forward-compute ................................: (88.03, 88.06)
    backward-compute ...............................: (123.51, 123.54)
    batch-generator ................................: (0.77, 0.86)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.58)
    optimizer-copy-to-main-grad ....................: (4.86, 4.89)
    optimizer-unscale-and-check-inf ................: (4.23, 4.23)
    optimizer-clip-main-grad .......................: (6.42, 6.43)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.52, 12.63)
    optimizer-copy-main-to-model-params ............: (5.16, 5.21)
    optimizer ......................................: (33.76, 33.81)
 iteration      100/     100 | consumed samples:          400 | elapsed time per iteration (ms): 254.9 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.071443E+00 | loss scale: 512.0 | grad norm: 1.480 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (211.43, 211.49)
    forward-compute ................................: (87.15, 87.15)
    backward-compute ...............................: (123.62, 123.64)
    batch-generator ................................: (0.77, 0.87)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (1.56, 1.58)
    optimizer-copy-to-main-grad ....................: (4.86, 4.88)
    optimizer-unscale-and-check-inf ................: (4.21, 4.22)
    optimizer-clip-main-grad .......................: (6.42, 6.43)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (12.53, 12.58)
    optimizer-copy-main-to-model-params ............: (5.16, 5.22)
    optimizer ......................................: (33.71, 33.77)
[after training is done] datetime: 2023-09-24 15:28:26 
Evaluating on 4 samples
Evaluating iter 1/1
-----------------------------------------------------------------------------------------------------------------
 validation loss at iteration 100 on validation set | lm loss value: 7.022842E+00 | lm loss PPL: 1.121971E+03 | 
-----------------------------------------------------------------------------------------------------------------
Evaluating on 4 samples
Evaluating iter 1/1
-----------------------------------------------------------------------------------------------------------
 validation loss at iteration 100 on test set | lm loss value: 6.806300E+00 | lm loss PPL: 9.035217E+02 | 
-----------------------------------------------------------------------------------------------------------
