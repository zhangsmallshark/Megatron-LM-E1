source-ing /home/czh5/genome/Megatron-LM-E1/ALCF/launch.sh
source-ing /home/czh5/genome/Megatron-LM-E1/ALCF/args.sh
------------------------
SCRIPT_DIR=/home/czh5/genome/Megatron-LM-E1/ALCF
SCRIPT_PATH=/home/czh5/genome/Megatron-LM-E1/ALCF/args.sh
------------------------
SOURCE=/home/czh5/genome/Megatron-LM-E1/ALCF/args.sh
DIR=/home/czh5/genome/Megatron-LM-E1/ALCF
------------------------
source-ing /home/czh5/genome/Megatron-LM-E1/ALCF/setup.sh
USING PYTHON: /usr/bin/python3
CFLAGS: 
LDFLAGS: 
Setting up MPI on Polaris from x3111c0s25b0n0
source-ing /home/czh5/genome/Megatron-LM-E1/ALCF/model.sh
--------------------------------
GLOBAL_BATCH=4
--------------------------------
OUTPUT TO: /home/czh5/genome/Megatron-LM-E1/outputs/GPT3_actCkpt_flashAttn_1.3B_z0_seqlen2048_mp4_pp1_nl20_hs2048_gb4_mb4
Setting up Polaris from x3111c0s25b0n0
Loading: 'module load conda 2023-01-10-unstable ; conda activate base'
Found venv at: /home/czh5/genome/Megatron-LM/venvs/polaris/2023-01-10/
USING PYTHON: /home/czh5/genome/Megatron-LM/venvs/polaris/2023-01-10/bin/python3
    Running on 1 hosts     with 4 GPUs each     for a total of 4 GPUs
Job started at: 2023-09-24-151735 on x3111c0s25b0n0
Job running in: /home/czh5/genome/Megatron-LM-E1/ALCF
Training GPT-3 with 1.3B parameters
Writing logs to: /home/czh5/genome/Megatron-LM-E1/outputs/GPT3_actCkpt_flashAttn_1.3B_z0_seqlen2048_mp4_pp1_nl20_hs2048_gb4_mb4
to view output: tail -f $(tail -1 logfiles)
i.e. tail -f /home/czh5/genome/Megatron-LM-E1/outputs/GPT3_actCkpt_flashAttn_1.3B_z0_seqlen2048_mp4_pp1_nl20_hs2048_gb4_mb4/logs/czh5-x3111c0s25b0n0-nhosts1-ngpu4-2023-09-24-151735.log
using: /home/czh5/genome/Megatron-LM/venvs/polaris/2023-01-10/bin/python3
Job started at: 2023-09-24-151735 on x3111c0s25b0n0
Job running in: /home/czh5/genome/Megatron-LM-E1/ALCF
Training GPT-3 with 1.3B parameters
Writing logs to: /home/czh5/genome/Megatron-LM-E1/outputs/GPT3_actCkpt_flashAttn_1.3B_z0_seqlen2048_mp4_pp1_nl20_hs2048_gb4_mb4
to view output: tail -f $(tail -1 logfiles)
i.e. tail -f /home/czh5/genome/Megatron-LM-E1/outputs/GPT3_actCkpt_flashAttn_1.3B_z0_seqlen2048_mp4_pp1_nl20_hs2048_gb4_mb4/logs/czh5-x3111c0s25b0n0-nhosts1-ngpu4-2023-09-24-151735.log
EXEC=/opt/cray/pe/pals/1.1.7/bin/mpiexec     --envall     --verbose     --hostfile /var/spool/pbs/aux/1111490.polaris-pbs-01.hsn.cm.polaris.alcf.anl.gov     -n 4     --ppn 4 /home/czh5/genome/Megatron-LM/venvs/polaris/2023-01-10/bin/python3 /home/czh5/genome/Megatron-LM-E1/pretrain_gpt.py     --use-flash-attn         --recompute-activations     --recompute-granularity full       --seed 2786   --pipeline-model-parallel-size 1   --tensor-model-parallel-size 4   --num-layers 20   --hidden-size 2048   --num-attention-heads 64   --micro-batch-size 4   --global-batch-size 4   --seq-length 2048   --max-position-embeddings 2048   --train-iters 100   --lr-decay-iters 320000   --num-workers 1   --data-path /lus/eagle/projects/MDClimSim/chengming/gpt_datasets/BookCorpusDataset_text_document   --vocab-file /lus/eagle/projects/MDClimSim/chengming/gpt_datasets/gpt2-vocab.json   --merge-file /lus/eagle/projects/MDClimSim/chengming/gpt_datasets/gpt2-merges.txt   --split 949,50,1   --distributed-backend nccl   --lr 0.00015   --lr-decay-style cosine   --min-lr 1.0e-5   --weight-decay 1e-2   --clip-grad 1.0   --lr-warmup-fraction .01   --log-interval 1   --save-interval 1000   --eval-interval 1000   --eval-iters 1   --timing-log-level 2   --tensorboard-dir /home/czh5/genome/Megatron-LM-E1/outputs/GPT3_actCkpt_flashAttn_1.3B_z0_seqlen2048_mp4_pp1_nl20_hs2048_gb4_mb4/tensorboard   --log-timers-to-tensorboard   --tensorboard-log-interval 1 --fp16
Writing logs to: /home/czh5/genome/Megatron-LM-E1/outputs/GPT3_actCkpt_flashAttn_1.3B_z0_seqlen2048_mp4_pp1_nl20_hs2048_gb4_mb4/logs/czh5-x3111c0s25b0n0-nhosts1-ngpu4-2023-09-24-151735.log
using world size: 4, data-parallel-size: 1, tensor-model-parallel size: 4, pipeline-model-parallel size: 1 
using torch.float16 for parameters ...
------------------------ arguments ------------------------
  accumulate_allreduce_grads_in_fp32 .............. False
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.999
  adam_eps ........................................ 1e-08
  add_bias_linear ................................. True
  add_position_embedding .......................... True
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  apply_layernorm_1p .............................. False
  apply_query_key_layer_scaling ................... True
  apply_residual_connection_post_layernorm ........ False
  async_tensor_model_parallel_allreduce ........... True
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  barrier_with_L1_time ............................ True
  bert_binary_head ................................ True
  bert_embedder_type .............................. megatron
  bert_load ....................................... None
  bf16 ............................................ False
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  check_for_nan_in_loss_and_grad .................. True
  classes_fraction ................................ 1.0
  clip_grad ....................................... 1.0
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  data_cache_path ................................. None
  data_parallel_random_init ....................... False
  data_parallel_size .............................. 1
  data_path ....................................... ['/lus/eagle/projects/MDClimSim/chengming/gpt_datasets/BookCorpusDataset_text_document']
  data_per_class_fraction ......................... 1.0
  data_sharding ................................... True
  dataloader_type ................................. single
  decoder_num_layers .............................. None
  decoder_seq_length .............................. None
  dino_bottleneck_size ............................ 256
  dino_freeze_last_layer .......................... 1
  dino_head_hidden_size ........................... 2048
  dino_local_crops_number ......................... 10
  dino_local_img_size ............................. 96
  dino_norm_last_layer ............................ False
  dino_teacher_temp ............................... 0.07
  dino_warmup_teacher_temp ........................ 0.04
  dino_warmup_teacher_temp_epochs ................. 30
  distribute_saved_activations .................... False
  distributed_backend ............................. nccl
  distributed_timeout_minutes ..................... 10
  embedding_path .................................. None
  embedding_weights_in_fp32 ....................... False
  empty_unused_memory_level ....................... 0
  encoder_num_layers .............................. 20
  encoder_seq_length .............................. 2048
  end_weight_decay ................................ 0.01
  eod_mask_loss ................................... False
  eval_interval ................................... 1000
  eval_iters ...................................... 1
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  exit_on_missing_checkpoint ...................... False
  exit_signal_handler ............................. False
  ffn_hidden_size ................................. 8192
  finetune ........................................ False
  fp16 ............................................ True
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  fp8 ............................................. None
  fp8_amax_compute_algo ........................... most_recent
  fp8_amax_history_len ............................ 1
  fp8_interval .................................... 1
  fp8_margin ...................................... 0
  fp8_wgrad ....................................... True
  global_batch_size ............................... 4
  gradient_accumulation_fusion .................... True
  group_query_attention ........................... False
  head_lr_mult .................................... 1.0
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 2048
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_h ........................................... 224
  img_w ........................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference_batch_times_seqlen_threshold .......... 512
  init_method_std ................................. 0.02
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  iter_per_epoch .................................. 1250
  kv_channels ..................................... 32
  lazy_mpu_init ................................... None
  load ............................................ None
  local_rank ...................................... None
  log_batch_size_to_tensorboard ................... False
  log_interval .................................... 1
  log_learning_rate_to_tensorboard ................ True
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... False
  log_params_norm ................................. False
  log_timers_to_tensorboard ....................... True
  log_validation_ppl_to_tensorboard ............... False
  log_world_size_to_tensorboard ................... False
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.00015
  lr_decay_iters .................................. 320000
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. 0.01
  lr_warmup_init .................................. 0.0
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  make_vocab_size_divisible_by .................... 128
  mask_factor ..................................... 1.0
  mask_prob ....................................... 0.15
  mask_type ....................................... random
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 2048
  max_tokens_to_oom ............................... 12000
  merge_file ...................................... /lus/eagle/projects/MDClimSim/chengming/gpt_datasets/gpt2-merges.txt
  micro_batch_size ................................ 4
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1e-05
  mmap_warmup ..................................... False
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_persist_layer_norm ........................... False
  no_save_optim ................................... None
  no_save_rng ..................................... None
  norm_epsilon .................................... 1e-05
  normalization ................................... LayerNorm
  num_attention_heads ............................. 64
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_experts ..................................... None
  num_layers ...................................... 20
  num_layers_per_virtual_pipeline_stage ........... None
  num_query_groups ................................ 1
  num_workers ..................................... 1
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  output_bert_embeddings .......................... False
  overlap_grad_reduce ............................. False
  overlap_p2p_comm ................................ False
  override_opt_param_scheduler .................... False
  params_dtype .................................... torch.float16
  patch_dim ....................................... 16
  perform_initialization .......................... True
  pipeline_model_parallel_size .................... 1
  pipeline_model_parallel_split_rank .............. None
  position_embedding_type ......................... learned_absolute
  profile ......................................... False
  profile_ranks ................................... [0]
  profile_step_end ................................ 12
  profile_step_start .............................. 10
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  recompute_granularity ........................... selective
  recompute_method ................................ None
  recompute_num_layers ............................ None
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  retro_add_retriever ............................. False
  retro_cyclic_train_iters ........................ None
  retro_encoder_attention_dropout ................. 0.1
  retro_encoder_hidden_dropout .................... 0.1
  retro_encoder_layers ............................ 2
  retro_num_neighbors ............................. 2
  retro_num_retrieved_chunks ...................... 2
  retro_return_doc_ids ............................ False
  retro_workdir ................................... None
  rotary_percent .................................. 1.0
  rotary_seq_len_interpolation_factor ............. None
  sample_rate ..................................... 1.0
  save ............................................ None
  save_interval ................................... 1000
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 2786
  seq_length ...................................... 2048
  sequence_parallel ............................... False
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  skip_train ...................................... False
  split ........................................... 949,50,1
  squared_relu .................................... False
  standalone_embedding_stage ...................... False
  start_weight_decay .............................. 0.01
  swiglu .......................................... False
  swin_backbone_type .............................. tiny
  tensor_model_parallel_size ...................... 4
  tensorboard_dir ................................. /home/czh5/genome/Megatron-LM-E1/outputs/GPT3_actCkpt_flashAttn_1.3B_z0_seqlen2048_mp4_pp1_nl20_hs2048_gb4_mb4/tensorboard
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  test_data_path .................................. None
  timing_log_level ................................ 2
  timing_log_option ............................... minmax
  titles_data_path ................................ None
  tokenizer_model ................................. None
  tokenizer_type .................................. GPT2BPETokenizer
  train_data_path ................................. None
  train_iters ..................................... 100
  train_samples ................................... None
  transformer_impl ................................ local
  transformer_pipeline_model_parallel_size ........ 1
  untie_embeddings_and_output_weights ............. False
  use_checkpoint_args ............................. False
  use_checkpoint_opt_param_scheduler .............. False
  use_cpu_initialization .......................... None
  use_distributed_optimizer ....................... False
  use_flash_attn .................................. True
  use_one_sent_docs ............................... False
  use_ring_exchange_p2p ........................... False
  use_rotary_position_embeddings .................. False
  valid_data_path ................................. None
  variable_seq_lengths ............................ False
  virtual_pipeline_model_parallel_size ............ None
  vision_backbone_type ............................ vit
  vision_pretraining .............................. False
  vision_pretraining_type ......................... classify
  vocab_extra_ids ................................. 0
  vocab_file ...................................... /lus/eagle/projects/MDClimSim/chengming/gpt_datasets/gpt2-vocab.json
  vocab_size ...................................... None
  weight_decay .................................... 0.01
  weight_decay_incr_style ......................... constant
  world_size ...................................... 4
-------------------- end of arguments ---------------------
setting number of micro-batches to constant 1
> building GPT2BPETokenizer tokenizer ...
 > padded vocab (size: 50257) with 431 dummy tokens (new size: 50688)
> initializing torch distributed ...
make: Entering directory '/home/czh5/genome/Megatron-LM-E1/megatron/data'
make: Nothing to be done for 'default'.
make: Leaving directory '/home/czh5/genome/Megatron-LM-E1/megatron/data'
> initialized tensor model parallel with size 4
> initialized pipeline model parallel with size 1
> setting random seeds to 2786 ...
> compiling dataset index builder ...
>>> done with dataset index builder. Compilation time: 0.033 seconds
> compiling and loading fused kernels ...
NCCL version 2.14.3+cuda11.8
>>> done with compiling and loading fused kernels. Compilation time: 8.715 seconds
time to initialize megatron (seconds): 15.875
[after megatron is initialized] datetime: 2023-09-24 15:17:55 
building GPT model ...
 > number of parameters on (tensor, pipeline) model parallel rank (2, 0): 282126336
> setting tensorboard ...
 > number of parameters on (tensor, pipeline) model parallel rank (3, 0): 282126336
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 282126336
 > number of parameters on (tensor, pipeline) model parallel rank (1, 0): 282126336
> buckets for gradient all-reduce:
    params for bucket 1
      module.language_model.encoder.layers.17.p_mlp.0.dense_h_to_4h.bias
      module.language_model.encoder.layers.14.p_mlp.0.dense_4h_to_h.bias
      module.language_model.encoder.layers.12.post_attention_norm.weight
      module.language_model.encoder.layers.9.p_mlp.0.dense_4h_to_h.weight
      module.language_model.encoder.layers.7.input_norm.weight
      module.language_model.encoder.layers.4.post_attention_norm.weight
      module.language_model.encoder.layers.1.post_attention_norm.bias
      module.language_model.encoder.layers.18.post_attention_norm.bias
      module.language_model.encoder.layers.16.p_self_attention.0.query_key_value.bias
      module.language_model.encoder.layers.13.p_self_attention.0.dense.bias
      module.language_model.encoder.layers.10.p_mlp.0.dense_h_to_4h.weight
      module.language_model.encoder.layers.8.input_norm.bias
      module.language_model.encoder.layers.5.p_self_attention.0.dense.weight
      module.language_model.encoder.layers.2.p_mlp.0.dense_h_to_4h.weight
      module.language_model.encoder.layers.19.p_mlp.0.dense_4h_to_h.weight
      module.language_model.encoder.layers.17.input_norm.weight
      module.language_model.encoder.layers.14.post_attention_norm.weight
      module.language_model.encoder.layers.11.p_mlp.0.dense_4h_to_h.bias
      module.language_model.encoder.layers.9.p_self_attention.0.query_key_value.weight
      module.language_model.encoder.layers.6.p_mlp.0.dense_h_to_4h.bias
      module.language_model.encoder.layers.3.p_mlp.0.dense_4h_to_h.bias
      module.language_model.encoder.layers.1.input_norm.bias
      module.language_model.encoder.layers.18.input_norm.bias
      module.language_model.encoder.layers.15.p_self_attention.0.dense.weight
      module.language_model.encoder.layers.12.p_mlp.0.dense_h_to_4h.weight
      module.language_model.encoder.layers.10.p_self_attention.0.dense.bias
      module.language_model.encoder.layers.7.post_attention_norm.bias
      module.language_model.encoder.layers.5.p_self_attention.0.query_key_value.bias
      module.language_model.encoder.layers.2.p_self_attention.0.query_key_value.weight
      module.language_model.encoder.layers.19.p_self_attention.0.query_key_value.weight
      module.language_model.encoder.layers.16.p_mlp.0.dense_h_to_4h.bias
      module.language_model.encoder.layers.13.p_mlp.0.dense_4h_to_h.bias
      module.language_model.encoder.layers.11.post_attention_norm.weight
      module.language_model.encoder.layers.8.p_mlp.0.dense_4h_to_h.weight
      module.language_model.encoder.layers.6.input_norm.weight
      module.language_model.encoder.layers.3.post_attention_norm.weight
      module.language_model.encoder.layers.0.post_attention_norm.bias
      module.language_model.encoder.layers.17.post_attention_norm.bias
      module.language_model.encoder.layers.15.p_self_attention.0.query_key_value.bias
      module.language_model.encoder.layers.12.p_self_attention.0.dense.weight
      module.language_model.encoder.layers.9.p_mlp.0.dense_h_to_4h.weight
      module.language_model.encoder.layers.7.input_norm.bias
      module.language_model.encoder.layers.4.p_self_attention.0.dense.weight
      module.language_model.encoder.layers.1.p_mlp.0.dense_4h_to_h.weight
      module.language_model.encoder.layers.18.p_mlp.0.dense_4h_to_h.weight
      module.language_model.encoder.layers.16.input_norm.weight
      module.language_model.encoder.layers.13.post_attention_norm.weight
      module.language_model.encoder.layers.10.p_mlp.0.dense_4h_to_h.bias
      module.language_model.encoder.layers.8.p_self_attention.0.query_key_value.weight
      module.language_model.encoder.layers.5.p_mlp.0.dense_h_to_4h.bias
      module.language_model.encoder.layers.2.p_mlp.0.dense_4h_to_h.bias
      module.language_model.encoder.layers.19.p_mlp.0.dense_h_to_4h.weight
      module.language_model.encoder.layers.17.input_norm.bias
      module.language_model.encoder.layers.14.p_self_attention.0.dense.weight
      module.language_model.encoder.layers.12.p_self_attention.0.query_key_value.bias
      module.language_model.encoder.layers.9.p_self_attention.0.dense.bias
      module.language_model.encoder.layers.6.post_attention_norm.bias
      module.language_model.encoder.layers.4.p_self_attention.0.query_key_value.bias
      module.language_model.encoder.layers.1.p_self_attention.0.query_key_value.weight
      module.language_model.encoder.layers.18.p_self_attention.0.query_key_value.weight
      module.language_model.encoder.layers.15.p_mlp.0.dense_h_to_4h.bias
      module.language_model.encoder.layers.12.p_mlp.0.dense_4h_to_h.bias
      module.language_model.encoder.layers.10.post_attention_norm.weight
      module.language_model.encoder.layers.7.p_mlp.0.dense_4h_to_h.weight
      module.language_model.encoder.layers.5.input_norm.weight
      module.language_model.encoder.layers.2.p_self_attention.0.dense.bias
      module.language_model.encoder.layers.19.p_self_attention.0.dense.bias
      module.language_model.encoder.layers.16.post_attention_norm.bias
      module.language_model.encoder.layers.14.p_self_attention.0.query_key_value.bias
      module.language_model.encoder.layers.11.p_self_attention.0.dense.weight
      module.language_model.encoder.layers.8.p_mlp.0.dense_h_to_4h.weight
      module.language_model.encoder.layers.6.input_norm.bias
      module.language_model.encoder.layers.3.p_self_attention.0.dense.weight
      module.language_model.encoder.layers.0.p_mlp.0.dense_4h_to_h.weight
      module.language_model.embedding.position_embeddings.weight
      module.language_model.encoder.layers.17.p_mlp.0.dense_4h_to_h.weight
      module.language_model.encoder.layers.15.input_norm.weight
      module.language_model.encoder.layers.12.p_mlp.0.dense_h_to_4h.bias
      module.language_model.encoder.layers.9.p_mlp.0.dense_4h_to_h.bias
      module.language_model.encoder.layers.7.p_self_attention.0.query_key_value.weight
      module.language_model.encoder.layers.4.p_mlp.0.dense_h_to_4h.bias
      module.language_model.encoder.layers.1.p_mlp.0.dense_h_to_4h.weight
      module.language_model.encoder.layers.18.p_mlp.0.dense_h_to_4h.weight
      module.language_model.encoder.layers.16.input_norm.bias
      module.language_model.encoder.layers.13.p_self_attention.0.dense.weight
      module.language_model.encoder.layers.11.p_self_attention.0.query_key_value.bias
      module.language_model.encoder.layers.8.p_self_attention.0.dense.bias
      module.language_model.encoder.layers.5.post_attention_norm.bias
      module.language_model.encoder.layers.3.p_self_attention.0.query_key_value.bias
      module.language_model.encoder.layers.0.post_attention_norm.weight
      module.language_model.encoder.layers.19.p_mlp.0.dense_4h_to_h.bias
      module.language_model.encoder.layers.17.p_self_attention.0.query_key_value.weight
      module.language_model.encoder.layers.14.p_mlp.0.dense_h_to_4h.bias
      module.language_model.encoder.layers.12.input_norm.weight
      module.language_model.encoder.layers.9.post_attention_norm.weight
      module.language_model.encoder.layers.6.p_mlp.0.dense_4h_to_h.weight
      module.language_model.encoder.layers.4.input_norm.weight
      module.language_model.encoder.layers.1.p_self_attention.0.dense.bias
      module.language_model.encoder.layers.0.p_self_attention.0.query_key_value.bias
      module.language_model.encoder.layers.18.p_self_attention.0.dense.bias
      module.language_model.encoder.layers.15.post_attention_norm.bias
      module.language_model.encoder.layers.13.p_self_attention.0.query_key_value.bias
      module.language_model.encoder.layers.10.p_self_attention.0.dense.weight
      module.language_model.encoder.layers.7.p_mlp.0.dense_h_to_4h.weight
      module.language_model.encoder.layers.5.input_norm.bias
      module.language_model.encoder.layers.2.post_attention_norm.bias
      module.language_model.encoder.layers.19.post_attention_norm.weight
      module.language_model.encoder.layers.16.p_mlp.0.dense_4h_to_h.weight
      module.language_model.encoder.layers.14.input_norm.weight
      module.language_model.encoder.layers.11.p_mlp.0.dense_h_to_4h.bias
      module.language_model.encoder.layers.8.p_mlp.0.dense_4h_to_h.bias
      module.language_model.encoder.layers.6.p_self_attention.0.query_key_value.weight
      module.language_model.encoder.layers.3.p_mlp.0.dense_h_to_4h.bias
      module.language_model.encoder.layers.0.p_mlp.0.dense_h_to_4h.weight
      module.language_model.encoder.layers.17.p_mlp.0.dense_h_to_4h.weight
      module.language_model.encoder.layers.15.input_norm.bias
      module.language_model.encoder.layers.12.post_attention_norm.bias
      module.language_model.encoder.layers.10.p_self_attention.0.query_key_value.bias
      module.language_model.encoder.layers.7.p_self_attention.0.dense.bias
      module.language_model.encoder.layers.4.post_attention_norm.bias
      module.language_model.encoder.layers.1.p_mlp.0.dense_4h_to_h.bias
      module.language_model.encoder.layers.18.p_mlp.0.dense_4h_to_h.bias
      module.language_model.encoder.layers.16.p_self_attention.0.query_key_value.weight
      module.language_model.encoder.layers.13.p_mlp.0.dense_h_to_4h.bias
      module.language_model.encoder.layers.11.input_norm.weight
      module.language_model.encoder.layers.8.post_attention_norm.weight
      module.language_model.encoder.layers.5.p_mlp.0.dense_4h_to_h.weight
      module.language_model.encoder.layers.3.input_norm.weight
      module.language_model.encoder.layers.0.p_self_attention.0.query_key_value.weight
      module.language_model.encoder.layers.17.p_self_attention.0.dense.bias
      module.language_model.encoder.layers.14.post_attention_norm.bias
      module.language_model.encoder.layers.12.input_norm.bias
      module.language_model.encoder.layers.9.p_self_attention.0.dense.weight
      module.language_model.encoder.layers.6.p_mlp.0.dense_h_to_4h.weight
      module.language_model.encoder.layers.4.input_norm.bias
      module.language_model.encoder.layers.1.post_attention_norm.weight
      module.language_model.embedding.word_embeddings.weight
      module.language_model.encoder.layers.18.post_attention_norm.weight
      module.language_model.encoder.layers.15.p_mlp.0.dense_4h_to_h.weight
      module.language_model.encoder.layers.13.input_norm.weight
      module.language_model.encoder.layers.10.p_mlp.0.dense_h_to_4h.bias
      module.language_model.encoder.layers.7.p_mlp.0.dense_4h_to_h.bias
      module.language_model.encoder.layers.5.p_self_attention.0.query_key_value.weight
      module.language_model.encoder.layers.2.p_self_attention.0.dense.weight
      module.language_model.encoder.layers.19.p_self_attention.0.dense.weight
      module.language_model.encoder.layers.16.p_mlp.0.dense_h_to_4h.weight
      module.language_model.encoder.layers.14.input_norm.bias
      module.language_model.encoder.layers.11.post_attention_norm.bias
      module.language_model.encoder.layers.9.p_self_attention.0.query_key_value.bias
      module.language_model.encoder.layers.6.p_self_attention.0.dense.bias
      module.language_model.encoder.layers.3.post_attention_norm.bias
      module.language_model.encoder.layers.0.p_mlp.0.dense_4h_to_h.bias
      module.language_model.encoder.layers.17.p_mlp.0.dense_4h_to_h.bias
      module.language_model.encoder.layers.15.p_self_attention.0.query_key_value.weight
      module.language_model.encoder.layers.12.p_mlp.0.dense_4h_to_h.weight
      module.language_model.encoder.layers.10.input_norm.weight
      module.language_model.encoder.layers.7.post_attention_norm.weight
      module.language_model.encoder.layers.4.p_mlp.0.dense_4h_to_h.weight
      module.language_model.encoder.layers.2.p_self_attention.0.query_key_value.bias
      module.language_model.encoder.layers.19.p_self_attention.0.query_key_value.bias
      module.language_model.encoder.layers.16.p_self_attention.0.dense.bias
      module.language_model.encoder.layers.13.post_attention_norm.bias
      module.language_model.encoder.layers.11.input_norm.bias
      module.language_model.encoder.layers.8.p_self_attention.0.dense.weight
      module.language_model.encoder.layers.5.p_mlp.0.dense_h_to_4h.weight
      module.language_model.encoder.layers.3.input_norm.bias
      module.language_model.encoder.layers.0.p_self_attention.0.dense.bias
      module.language_model.encoder.final_norm.weight
      module.language_model.encoder.layers.17.post_attention_norm.weight
      module.language_model.encoder.layers.14.p_mlp.0.dense_4h_to_h.weight
      module.language_model.encoder.layers.12.p_self_attention.0.query_key_value.weight
      module.language_model.encoder.layers.9.p_mlp.0.dense_h_to_4h.bias
      module.language_model.encoder.layers.6.p_mlp.0.dense_4h_to_h.bias
      module.language_model.encoder.layers.4.p_self_attention.0.query_key_value.weight
      module.language_model.encoder.layers.1.p_self_attention.0.dense.weight
      module.language_model.encoder.layers.0.input_norm.weight
      module.language_model.encoder.layers.18.p_self_attention.0.dense.weight
      module.language_model.encoder.layers.15.p_mlp.0.dense_h_to_4h.weight
      module.language_model.encoder.layers.13.input_norm.bias
      module.language_model.encoder.layers.10.post_attention_norm.bias
      module.language_model.encoder.layers.8.p_self_attention.0.query_key_value.bias
      module.language_model.encoder.layers.5.p_self_attention.0.dense.bias
      module.language_model.encoder.layers.2.p_mlp.0.dense_h_to_4h.bias
      module.language_model.encoder.layers.0.input_norm.bias
      module.language_model.encoder.layers.19.p_mlp.0.dense_h_to_4h.bias
      module.language_model.encoder.layers.16.p_mlp.0.dense_4h_to_h.bias
      module.language_model.encoder.layers.14.p_self_attention.0.query_key_value.weight
      module.language_model.encoder.layers.11.p_mlp.0.dense_4h_to_h.weight
      module.language_model.encoder.layers.9.input_norm.weight
      module.language_model.encoder.layers.6.post_attention_norm.weight
      module.language_model.encoder.layers.3.p_mlp.0.dense_4h_to_h.weight
      module.language_model.encoder.layers.2.post_attention_norm.weight
      module.language_model.encoder.layers.1.p_self_attention.0.query_key_value.bias
      module.language_model.encoder.layers.18.p_self_attention.0.query_key_value.bias
      module.language_model.encoder.layers.15.p_self_attention.0.dense.bias
      module.language_model.encoder.layers.10.input_norm.bias
      module.language_model.encoder.layers.7.p_self_attention.0.dense.weight
      module.language_model.encoder.layers.4.p_mlp.0.dense_h_to_4h.weight
      module.language_model.encoder.layers.2.input_norm.weight
      module.language_model.encoder.layers.19.input_norm.weight
      module.language_model.encoder.layers.16.post_attention_norm.weight
      module.language_model.encoder.layers.13.p_mlp.0.dense_4h_to_h.weight
      module.language_model.encoder.layers.11.p_self_attention.0.query_key_value.weight
      module.language_model.encoder.layers.8.p_mlp.0.dense_h_to_4h.bias
      module.language_model.encoder.layers.5.p_mlp.0.dense_4h_to_h.bias
      module.language_model.encoder.layers.3.p_self_attention.0.query_key_value.weight
      module.language_model.encoder.layers.0.p_self_attention.0.dense.weight
      module.language_model.encoder.final_norm.bias
      module.language_model.encoder.layers.17.p_self_attention.0.dense.weight
      module.language_model.encoder.layers.14.p_mlp.0.dense_h_to_4h.weight
      module.language_model.encoder.layers.12.p_self_attention.0.dense.bias
      module.language_model.encoder.layers.9.post_attention_norm.bias
      module.language_model.encoder.layers.7.p_self_attention.0.query_key_value.bias
      module.language_model.encoder.layers.4.p_self_attention.0.dense.bias
      module.language_model.encoder.layers.1.p_mlp.0.dense_h_to_4h.bias
      module.language_model.encoder.layers.18.p_mlp.0.dense_h_to_4h.bias
      module.language_model.encoder.layers.15.p_mlp.0.dense_4h_to_h.bias
      module.language_model.encoder.layers.13.p_self_attention.0.query_key_value.weight
      module.language_model.encoder.layers.10.p_mlp.0.dense_4h_to_h.weight
      module.language_model.encoder.layers.8.input_norm.weight
      module.language_model.encoder.layers.5.post_attention_norm.weight
      module.language_model.encoder.layers.2.p_mlp.0.dense_4h_to_h.weight
      module.language_model.encoder.layers.19.post_attention_norm.bias
      module.language_model.encoder.layers.17.p_self_attention.0.query_key_value.bias
      module.language_model.encoder.layers.14.p_self_attention.0.dense.bias
      module.language_model.encoder.layers.11.p_mlp.0.dense_h_to_4h.weight
      module.language_model.encoder.layers.9.input_norm.bias
      module.language_model.encoder.layers.6.p_self_attention.0.dense.weight
      module.language_model.encoder.layers.3.p_mlp.0.dense_h_to_4h.weight
      module.language_model.encoder.layers.1.input_norm.weight
      module.language_model.encoder.layers.18.input_norm.weight
      module.language_model.encoder.layers.15.post_attention_norm.weight
      module.language_model.encoder.layers.10.p_self_attention.0.query_key_value.weight
      module.language_model.encoder.layers.7.p_mlp.0.dense_h_to_4h.bias
      module.language_model.encoder.layers.4.p_mlp.0.dense_4h_to_h.bias
      module.language_model.encoder.layers.2.input_norm.bias
      module.language_model.encoder.layers.19.input_norm.bias
      module.language_model.encoder.layers.16.p_self_attention.0.dense.weight
      module.language_model.encoder.layers.13.p_mlp.0.dense_h_to_4h.weight
      module.language_model.encoder.layers.11.p_self_attention.0.dense.bias
      module.language_model.encoder.layers.8.post_attention_norm.bias
      module.language_model.encoder.layers.6.p_self_attention.0.query_key_value.bias
      module.language_model.encoder.layers.3.p_self_attention.0.dense.bias
      module.language_model.encoder.layers.0.p_mlp.0.dense_h_to_4h.bias
     total number of elements: 282126336
> learning rate decay style: cosine
[after model, optimizer, and learning rate scheduler are built] datetime: 2023-09-24 15:17:55 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      400
    validation: 4
    test:       4
> building train, validation, and test datasets for GPT ...
Single data path provided for train, valid & test
 > building dataset index ...
    reading sequence lengths...
    reading sequence pointers...
    reading document indices...
    creating np buffer of mmap...
    creating memory view of np buffer...
 > finished creating indexed dataset in 0.001730 seconds
    number of documents: 17868
 > dataset split:
    train:
     document indices in [0, 16957) total of 16957 documents
    validation:
     document indices in [16957, 17850) total of 893 documents
    test:
     document indices in [17850, 17868) total of 18 documents
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.002226
    using:
     number of documents:       16957
     number of epochs:          1
     sequence length:           2048
     total number of samples:   747717
 > elasped time to build and save sample-idx mapping (seconds): 0.006007
 > building shuffle index with split [0, 747717) and [747717, 747717) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.018010
 > loading doc-idx mapping from /lus/eagle/projects/MDClimSim/chengming/gpt_datasets/index-cache/d8e914d840336a97449bea42ad3a9f35_doc_idx.npy
 > loading sample-idx mapping from /lus/eagle/projects/MDClimSim/chengming/gpt_datasets/index-cache/d8e914d840336a97449bea42ad3a9f35_sample_idx.npy
 > loading shuffle-idx mapping from /lus/eagle/projects/MDClimSim/chengming/gpt_datasets/index-cache/d8e914d840336a97449bea42ad3a9f35_shuffle_idx.npy
    loaded indexed file in 0.003 seconds
    total number of samples: 747718
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.001791
    using:
     number of documents:       893
     number of epochs:          1
     sequence length:           2048
     total number of samples:   38903
 > elasped time to build and save sample-idx mapping (seconds): 0.001631
 > building shuffle index with split [0, 38903) and [38903, 38903) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.002361
 > loading doc-idx mapping from /lus/eagle/projects/MDClimSim/chengming/gpt_datasets/index-cache/377216053031274a7e898265c63b274f_doc_idx.npy
 > loading sample-idx mapping from /lus/eagle/projects/MDClimSim/chengming/gpt_datasets/index-cache/377216053031274a7e898265c63b274f_sample_idx.npy
 > loading shuffle-idx mapping from /lus/eagle/projects/MDClimSim/chengming/gpt_datasets/index-cache/377216053031274a7e898265c63b274f_shuffle_idx.npy
    loaded indexed file in 0.002 seconds
    total number of samples: 38904
    total number of epochs: 1
 > WARNING: could not find index map files, building the indices on rank 0 ...
 > only one epoch required, setting separate_last_epoch to False
 > elasped time to build and save doc-idx mapping (seconds): 0.001682
    using:
     number of documents:       18
     number of epochs:          1
     sequence length:           2048
     total number of samples:   1094
 > elasped time to build and save sample-idx mapping (seconds): 0.001435
 > building shuffle index with split [0, 1094) and [1094, 1094) ...
 > elasped time to build and save shuffle-idx mapping (seconds): 0.001355
 > loading doc-idx mapping from /lus/eagle/projects/MDClimSim/chengming/gpt_datasets/index-cache/ae4b311a1234137e5af77b1dfd385ac4_doc_idx.npy
 > loading sample-idx mapping from /lus/eagle/projects/MDClimSim/chengming/gpt_datasets/index-cache/ae4b311a1234137e5af77b1dfd385ac4_sample_idx.npy
 > loading shuffle-idx mapping from /lus/eagle/projects/MDClimSim/chengming/gpt_datasets/index-cache/ae4b311a1234137e5af77b1dfd385ac4_shuffle_idx.npy
    loaded indexed file in 0.001 seconds
    total number of samples: 1095
    total number of epochs: 1
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2023-09-24 15:18:04 
done with setup ...
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (107.84, 200.96)
    train/valid/test-data-iterators-setup ..........: (8160.31, 8915.21)
training ...
[before the start of training step] datetime: 2023-09-24 15:18:04 
NCCL version 2.14.3+cuda11.8
NCCL version 2.14.3+cuda11.8
NCCL version 2.14.3+cuda11.8
 iteration        1/     100 | consumed samples:            4 | elapsed time per iteration (ms): 13809.7 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 4294967296.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
[Rank 2] (after 1 iterations) memory (MB) | allocated: 3230.18408203125 | max allocated: 8521.15234375 | reserved: 8738.0 | max reserved: 8738.0
[Rank 0] (after 1 iterations) memory (MB) | allocated: 3230.18408203125 | max allocated: 8521.15234375 | reserved: 8738.0 | max reserved: 8738.0
[Rank 1] (after 1 iterations) memory (MB) | allocated: 3230.18408203125 | max allocated: 8521.15234375 | reserved: 8738.0 | max reserved: 8738.0
[Rank 3] (after 1 iterations) memory (MB) | allocated: 3230.18408203125 | max allocated: 8521.15234375 | reserved: 8738.0 | max reserved: 8738.0
(min, max) time across ranks (ms):
    forward-backward ...............................: (5993.86, 5993.97)
    forward-compute ................................: (875.41, 1107.69)
    backward-compute ...............................: (4885.71, 5117.94)
    batch-generator ................................: (50.20, 50.46)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-all-reduce ...............................: (0.85, 0.88)
    optimizer-copy-to-main-grad ....................: (2.72, 3.02)
    optimizer-unscale-and-check-inf ................: (7806.45, 7806.48)
    optimizer ......................................: (7809.64, 7809.68)
 iteration        2/     100 | consumed samples:            8 | elapsed time per iteration (ms): 215.3 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 2147483648.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (200.39, 200.66)
    forward-compute ................................: (80.25, 80.25)
    backward-compute ...............................: (119.79, 120.05)
    batch-generator ................................: (0.83, 0.90)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.72, 3.30)
    optimizer-unscale-and-check-inf ................: (2.80, 2.81)
    optimizer ......................................: (6.32, 6.35)
 iteration        3/     100 | consumed samples:           12 | elapsed time per iteration (ms): 189.0 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 1073741824.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (176.36, 176.39)
    forward-compute ................................: (72.27, 72.27)
    backward-compute ...............................: (103.74, 103.76)
    batch-generator ................................: (0.69, 0.76)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.83, 0.86)
    optimizer-copy-to-main-grad ....................: (2.71, 2.75)
    optimizer-unscale-and-check-inf ................: (2.54, 2.55)
    optimizer ......................................: (5.47, 5.48)
 iteration        4/     100 | consumed samples:           16 | elapsed time per iteration (ms): 184.9 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 536870912.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (172.35, 172.37)
    forward-compute ................................: (68.34, 68.35)
    backward-compute ...............................: (103.67, 103.67)
    batch-generator ................................: (0.71, 0.80)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.71, 2.75)
    optimizer-unscale-and-check-inf ................: (2.53, 2.53)
    optimizer ......................................: (5.46, 5.47)
 iteration        5/     100 | consumed samples:           20 | elapsed time per iteration (ms): 181.1 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 268435456.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.26, 168.65)
    forward-compute ................................: (68.34, 68.71)
    backward-compute ...............................: (99.57, 99.60)
    batch-generator ................................: (0.40, 0.74)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.72, 2.75)
    optimizer-unscale-and-check-inf ................: (2.52, 2.52)
    optimizer ......................................: (5.45, 5.46)
 iteration        6/     100 | consumed samples:           24 | elapsed time per iteration (ms): 180.9 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 134217728.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.12, 168.16)
    forward-compute ................................: (68.16, 68.16)
    backward-compute ...............................: (99.61, 99.64)
    batch-generator ................................: (0.70, 0.77)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.72, 2.75)
    optimizer-unscale-and-check-inf ................: (2.50, 2.51)
    optimizer ......................................: (5.44, 5.44)
 iteration        7/     100 | consumed samples:           28 | elapsed time per iteration (ms): 181.6 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 67108864.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.90, 168.94)
    forward-compute ................................: (68.95, 68.96)
    backward-compute ...............................: (99.60, 99.64)
    batch-generator ................................: (0.70, 0.77)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.71, 2.75)
    optimizer-unscale-and-check-inf ................: (2.49, 2.50)
    optimizer ......................................: (5.43, 5.43)
 iteration        8/     100 | consumed samples:           32 | elapsed time per iteration (ms): 184.6 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 33554432.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (172.10, 172.15)
    forward-compute ................................: (72.01, 72.03)
    backward-compute ...............................: (99.74, 99.77)
    batch-generator ................................: (0.69, 0.76)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.50, 2.50)
    optimizer ......................................: (5.44, 5.44)
 iteration        9/     100 | consumed samples:           36 | elapsed time per iteration (ms): 181.5 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 16777216.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.98, 169.01)
    forward-compute ................................: (68.95, 68.96)
    backward-compute ...............................: (99.68, 99.71)
    batch-generator ................................: (0.69, 0.77)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.83, 0.84)
    optimizer-copy-to-main-grad ....................: (2.72, 2.75)
    optimizer-unscale-and-check-inf ................: (2.47, 2.47)
    optimizer ......................................: (5.40, 5.41)
 iteration       10/     100 | consumed samples:           40 | elapsed time per iteration (ms): 180.9 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 8388608.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.15, 168.18)
    forward-compute ................................: (68.22, 68.23)
    backward-compute ...............................: (99.57, 99.59)
    batch-generator ................................: (0.71, 0.79)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.44, 2.44)
    optimizer ......................................: (5.38, 5.38)
 iteration       11/     100 | consumed samples:           44 | elapsed time per iteration (ms): 181.0 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 4194304.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.27, 168.30)
    forward-compute ................................: (68.18, 68.19)
    backward-compute ...............................: (99.74, 99.75)
    batch-generator ................................: (0.67, 0.75)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.71, 2.75)
    optimizer-unscale-and-check-inf ................: (2.38, 2.39)
    optimizer ......................................: (5.31, 5.32)
 iteration       12/     100 | consumed samples:           48 | elapsed time per iteration (ms): 185.4 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 2097152.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (173.02, 173.05)
    forward-compute ................................: (68.51, 68.53)
    backward-compute ...............................: (104.13, 104.15)
    batch-generator ................................: (0.70, 0.78)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.85, 0.85)
    optimizer-copy-to-main-grad ....................: (2.72, 2.75)
    optimizer-unscale-and-check-inf ................: (2.38, 2.39)
    optimizer ......................................: (5.32, 5.34)
 iteration       13/     100 | consumed samples:           52 | elapsed time per iteration (ms): 182.3 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 1048576.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (169.23, 169.26)
    forward-compute ................................: (68.90, 68.91)
    backward-compute ...............................: (99.94, 99.97)
    batch-generator ................................: (0.71, 0.79)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.84)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.36, 2.37)
    optimizer ......................................: (5.30, 5.32)
 iteration       14/     100 | consumed samples:           56 | elapsed time per iteration (ms): 180.6 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 524288.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.17, 168.20)
    forward-compute ................................: (68.27, 68.29)
    backward-compute ...............................: (99.53, 99.57)
    batch-generator ................................: (0.74, 0.86)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.30, 2.31)
    optimizer ......................................: (5.24, 5.25)
 iteration       15/     100 | consumed samples:           60 | elapsed time per iteration (ms): 181.1 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 262144.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.75, 168.78)
    forward-compute ................................: (68.70, 68.71)
    backward-compute ...............................: (99.69, 99.71)
    batch-generator ................................: (0.73, 0.82)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.30, 2.30)
    optimizer ......................................: (5.24, 5.24)
 iteration       16/     100 | consumed samples:           64 | elapsed time per iteration (ms): 180.9 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 131072.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.30, 168.33)
    forward-compute ................................: (68.21, 68.24)
    backward-compute ...............................: (99.72, 99.76)
    batch-generator ................................: (0.69, 0.77)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.72, 2.75)
    optimizer-unscale-and-check-inf ................: (2.28, 2.29)
    optimizer ......................................: (5.22, 5.22)
 iteration       17/     100 | consumed samples:           68 | elapsed time per iteration (ms): 180.6 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 65536.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.07, 168.10)
    forward-compute ................................: (68.04, 68.05)
    backward-compute ...............................: (99.68, 99.70)
    batch-generator ................................: (0.70, 0.78)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.71, 2.75)
    optimizer-unscale-and-check-inf ................: (2.28, 2.28)
    optimizer ......................................: (5.22, 5.22)
 iteration       18/     100 | consumed samples:           72 | elapsed time per iteration (ms): 180.7 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 32768.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.54, 168.56)
    forward-compute ................................: (68.38, 68.39)
    backward-compute ...............................: (99.81, 99.83)
    batch-generator ................................: (0.67, 0.74)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.84)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.26, 2.27)
    optimizer ......................................: (5.20, 5.21)
 iteration       19/     100 | consumed samples:           76 | elapsed time per iteration (ms): 180.4 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 16384.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.20, 168.24)
    forward-compute ................................: (68.08, 68.09)
    backward-compute ...............................: (99.77, 99.79)
    batch-generator ................................: (0.70, 0.78)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.84)
    optimizer-copy-to-main-grad ....................: (2.72, 2.75)
    optimizer-unscale-and-check-inf ................: (2.25, 2.26)
    optimizer ......................................: (5.19, 5.20)
 iteration       20/     100 | consumed samples:           80 | elapsed time per iteration (ms): 181.7 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 8192.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (169.52, 169.56)
    forward-compute ................................: (69.51, 69.52)
    backward-compute ...............................: (99.67, 99.69)
    batch-generator ................................: (0.72, 0.81)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.24, 2.25)
    optimizer ......................................: (5.18, 5.19)
 iteration       21/     100 | consumed samples:           84 | elapsed time per iteration (ms): 180.8 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 4096.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.25, 168.30)
    forward-compute ................................: (68.12, 68.12)
    backward-compute ...............................: (99.78, 99.80)
    batch-generator ................................: (0.69, 0.76)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.24, 2.24)
    optimizer ......................................: (5.18, 5.19)
 iteration       22/     100 | consumed samples:           88 | elapsed time per iteration (ms): 180.3 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 2048.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.04, 168.08)
    forward-compute ................................: (68.14, 68.15)
    backward-compute ...............................: (99.55, 99.57)
    batch-generator ................................: (0.67, 0.74)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.86)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.23, 2.24)
    optimizer ......................................: (5.17, 5.18)
 iteration       23/     100 | consumed samples:           92 | elapsed time per iteration (ms): 181.1 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 1024.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.92, 168.93)
    forward-compute ................................: (68.05, 68.06)
    backward-compute ...............................: (100.51, 100.53)
    batch-generator ................................: (0.67, 0.75)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.84)
    optimizer-copy-to-main-grad ....................: (2.72, 2.75)
    optimizer-unscale-and-check-inf ................: (2.24, 2.24)
    optimizer ......................................: (5.18, 5.19)
 iteration       24/     100 | consumed samples:           96 | elapsed time per iteration (ms): 180.5 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 512.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.16, 168.20)
    forward-compute ................................: (68.06, 68.08)
    backward-compute ...............................: (99.75, 99.77)
    batch-generator ................................: (0.71, 0.78)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.84)
    optimizer-copy-to-main-grad ....................: (2.72, 2.75)
    optimizer-unscale-and-check-inf ................: (2.24, 2.25)
    optimizer ......................................: (5.18, 5.19)
 iteration       25/     100 | consumed samples:          100 | elapsed time per iteration (ms): 181.5 | learning rate: 1.500E-04 | global batch size:     4 | loss scale: 256.0 | number of skipped iterations:   1 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (169.00, 169.04)
    forward-compute ................................: (68.95, 68.99)
    backward-compute ...............................: (99.68, 99.72)
    batch-generator ................................: (0.69, 0.76)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.84)
    optimizer-copy-to-main-grad ....................: (2.72, 2.75)
    optimizer-unscale-and-check-inf ................: (2.24, 2.25)
    optimizer ......................................: (5.17, 5.18)
 iteration       26/     100 | consumed samples:          104 | elapsed time per iteration (ms): 197.4 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 1.121570E+01 | loss scale: 256.0 | grad norm: 6485.830 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.02, 168.06)
    forward-compute ................................: (68.17, 68.18)
    backward-compute ...............................: (99.51, 99.52)
    batch-generator ................................: (0.70, 0.78)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.24, 2.24)
    optimizer-clip-main-grad .......................: (3.64, 3.66)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (9.07, 9.19)
    optimizer-copy-main-to-model-params ............: (2.93, 2.97)
    optimizer ......................................: (21.22, 21.26)
 iteration       27/     100 | consumed samples:          108 | elapsed time per iteration (ms): 217.4 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 9.888872E+00 | loss scale: 256.0 | grad norm: 40.217 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (190.43, 190.47)
    forward-compute ................................: (90.22, 90.23)
    backward-compute ...............................: (99.84, 99.86)
    batch-generator ................................: (0.71, 0.80)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.73, 2.76)
    optimizer-unscale-and-check-inf ................: (2.26, 2.27)
    optimizer-clip-main-grad .......................: (3.57, 3.60)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.65, 6.69)
    optimizer-copy-main-to-model-params ............: (2.92, 2.96)
    optimizer ......................................: (18.66, 18.70)
 iteration       28/     100 | consumed samples:          112 | elapsed time per iteration (ms): 195.2 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 1.059651E+01 | loss scale: 256.0 | grad norm: 42.185 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.14, 168.16)
    forward-compute ................................: (68.17, 68.19)
    backward-compute ...............................: (99.59, 99.61)
    batch-generator ................................: (0.76, 0.83)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.85, 0.85)
    optimizer-copy-to-main-grad ....................: (2.72, 2.75)
    optimizer-unscale-and-check-inf ................: (2.28, 2.29)
    optimizer-clip-main-grad .......................: (3.58, 3.60)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.64, 6.68)
    optimizer-copy-main-to-model-params ............: (2.94, 3.00)
    optimizer ......................................: (18.71, 18.77)
 iteration       29/     100 | consumed samples:          116 | elapsed time per iteration (ms): 195.0 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 1.004906E+01 | loss scale: 256.0 | grad norm: 5.138 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.10, 168.13)
    forward-compute ................................: (68.12, 68.12)
    backward-compute ...............................: (99.61, 99.63)
    batch-generator ................................: (0.74, 0.82)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.84)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.25, 2.26)
    optimizer-clip-main-grad .......................: (3.59, 3.60)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.63, 6.68)
    optimizer-copy-main-to-model-params ............: (2.92, 3.00)
    optimizer ......................................: (18.63, 18.71)
 iteration       30/     100 | consumed samples:          120 | elapsed time per iteration (ms): 194.8 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 9.475056E+00 | loss scale: 256.0 | grad norm: 5.951 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.01, 168.03)
    forward-compute ................................: (68.18, 68.19)
    backward-compute ...............................: (99.47, 99.48)
    batch-generator ................................: (0.76, 0.83)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.87)
    optimizer-copy-to-main-grad ....................: (2.72, 2.75)
    optimizer-unscale-and-check-inf ................: (2.25, 2.25)
    optimizer-clip-main-grad .......................: (3.58, 3.59)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.62, 6.67)
    optimizer-copy-main-to-model-params ............: (2.91, 2.96)
    optimizer ......................................: (18.60, 18.65)
 iteration       31/     100 | consumed samples:          124 | elapsed time per iteration (ms): 195.7 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 9.476606E+00 | loss scale: 256.0 | grad norm: 6.464 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.76, 168.79)
    forward-compute ................................: (68.11, 68.75)
    backward-compute ...............................: (99.67, 100.31)
    batch-generator ................................: (0.74, 0.81)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.84)
    optimizer-copy-to-main-grad ....................: (2.72, 2.75)
    optimizer-unscale-and-check-inf ................: (2.27, 2.28)
    optimizer-clip-main-grad .......................: (3.57, 3.60)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.64, 6.68)
    optimizer-copy-main-to-model-params ............: (2.91, 2.96)
    optimizer ......................................: (18.64, 18.69)
 iteration       32/     100 | consumed samples:          128 | elapsed time per iteration (ms): 194.7 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 8.871542E+00 | loss scale: 256.0 | grad norm: 3.477 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (167.96, 167.98)
    forward-compute ................................: (68.10, 68.12)
    backward-compute ...............................: (99.48, 99.51)
    batch-generator ................................: (0.73, 0.80)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.72, 2.75)
    optimizer-unscale-and-check-inf ................: (2.26, 2.27)
    optimizer-clip-main-grad .......................: (3.56, 3.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.64, 6.67)
    optimizer-copy-main-to-model-params ............: (2.91, 2.96)
    optimizer ......................................: (18.61, 18.65)
 iteration       33/     100 | consumed samples:          132 | elapsed time per iteration (ms): 195.2 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 8.540271E+00 | loss scale: 256.0 | grad norm: 3.888 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.21, 168.24)
    forward-compute ................................: (68.17, 68.18)
    backward-compute ...............................: (99.69, 99.70)
    batch-generator ................................: (0.74, 0.81)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.72, 2.75)
    optimizer-unscale-and-check-inf ................: (2.26, 2.26)
    optimizer-clip-main-grad .......................: (3.56, 3.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.63, 6.72)
    optimizer-copy-main-to-model-params ............: (2.91, 2.96)
    optimizer ......................................: (18.65, 18.70)
 iteration       34/     100 | consumed samples:          136 | elapsed time per iteration (ms): 194.5 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.907450E+00 | loss scale: 256.0 | grad norm: 2.725 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (167.72, 167.74)
    forward-compute ................................: (67.92, 67.93)
    backward-compute ...............................: (99.45, 99.47)
    batch-generator ................................: (0.71, 0.80)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.84)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.25, 2.26)
    optimizer-clip-main-grad .......................: (3.56, 3.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.61, 6.69)
    optimizer-copy-main-to-model-params ............: (2.91, 2.96)
    optimizer ......................................: (18.61, 18.66)
 iteration       35/     100 | consumed samples:          140 | elapsed time per iteration (ms): 195.3 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 8.179628E+00 | loss scale: 256.0 | grad norm: 442.338 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.45, 168.49)
    forward-compute ................................: (68.33, 68.33)
    backward-compute ...............................: (99.76, 99.78)
    batch-generator ................................: (0.74, 0.82)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.84)
    optimizer-copy-to-main-grad ....................: (2.72, 2.75)
    optimizer-unscale-and-check-inf ................: (2.35, 2.35)
    optimizer-clip-main-grad .......................: (3.56, 3.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.62, 6.67)
    optimizer-copy-main-to-model-params ............: (2.92, 2.96)
    optimizer ......................................: (18.68, 18.73)
 iteration       36/     100 | consumed samples:          144 | elapsed time per iteration (ms): 194.7 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 9.111931E+00 | loss scale: 256.0 | grad norm: 26.539 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (167.86, 167.89)
    forward-compute ................................: (68.07, 68.09)
    backward-compute ...............................: (99.44, 99.46)
    batch-generator ................................: (0.70, 0.77)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.84)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.26, 2.26)
    optimizer-clip-main-grad .......................: (3.55, 3.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.62, 6.69)
    optimizer-copy-main-to-model-params ............: (2.92, 2.96)
    optimizer ......................................: (18.62, 18.66)
 iteration       37/     100 | consumed samples:          148 | elapsed time per iteration (ms): 194.8 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 8.408722E+00 | loss scale: 256.0 | grad norm: 14.947 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (167.95, 167.99)
    forward-compute ................................: (68.02, 68.03)
    backward-compute ...............................: (99.58, 99.60)
    batch-generator ................................: (0.70, 0.77)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.72, 2.75)
    optimizer-unscale-and-check-inf ................: (2.26, 2.27)
    optimizer-clip-main-grad .......................: (3.60, 3.61)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.64, 6.70)
    optimizer-copy-main-to-model-params ............: (2.91, 2.95)
    optimizer ......................................: (18.66, 18.71)
 iteration       38/     100 | consumed samples:          152 | elapsed time per iteration (ms): 195.1 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 8.379624E+00 | loss scale: 256.0 | grad norm: 10.236 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.02, 168.03)
    forward-compute ................................: (68.23, 68.23)
    backward-compute ...............................: (99.44, 99.45)
    batch-generator ................................: (0.73, 0.82)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.25, 2.26)
    optimizer-clip-main-grad .......................: (3.57, 3.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.63, 6.69)
    optimizer-copy-main-to-model-params ............: (2.91, 2.96)
    optimizer ......................................: (18.63, 18.68)
 iteration       39/     100 | consumed samples:          156 | elapsed time per iteration (ms): 195.0 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.529054E+00 | loss scale: 256.0 | grad norm: 3.603 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.16, 168.19)
    forward-compute ................................: (68.12, 68.13)
    backward-compute ...............................: (99.68, 99.71)
    batch-generator ................................: (0.77, 0.85)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.84)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.26, 2.26)
    optimizer-clip-main-grad .......................: (3.57, 3.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.61, 6.68)
    optimizer-copy-main-to-model-params ............: (2.92, 2.96)
    optimizer ......................................: (18.61, 18.66)
 iteration       40/     100 | consumed samples:          160 | elapsed time per iteration (ms): 194.7 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.632241E+00 | loss scale: 256.0 | grad norm: 2.866 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (167.88, 167.90)
    forward-compute ................................: (68.03, 68.03)
    backward-compute ...............................: (99.48, 99.50)
    batch-generator ................................: (0.68, 0.75)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.72, 2.75)
    optimizer-unscale-and-check-inf ................: (2.26, 2.26)
    optimizer-clip-main-grad .......................: (3.56, 3.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.62, 6.67)
    optimizer-copy-main-to-model-params ............: (2.92, 2.96)
    optimizer ......................................: (18.61, 18.65)
 iteration       41/     100 | consumed samples:          164 | elapsed time per iteration (ms): 194.9 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.458350E+00 | loss scale: 256.0 | grad norm: 3.813 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.00, 168.03)
    forward-compute ................................: (68.00, 68.01)
    backward-compute ...............................: (99.64, 99.65)
    batch-generator ................................: (0.74, 0.81)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.88)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.26, 2.26)
    optimizer-clip-main-grad .......................: (3.57, 3.60)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.63, 6.69)
    optimizer-copy-main-to-model-params ............: (2.91, 2.96)
    optimizer ......................................: (18.63, 18.68)
 iteration       42/     100 | consumed samples:          168 | elapsed time per iteration (ms): 194.7 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.566182E+00 | loss scale: 256.0 | grad norm: 3.003 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (167.86, 167.88)
    forward-compute ................................: (67.98, 67.99)
    backward-compute ...............................: (99.52, 99.54)
    batch-generator ................................: (0.73, 0.80)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.85, 0.89)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.25, 2.26)
    optimizer-clip-main-grad .......................: (3.58, 3.59)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.63, 6.68)
    optimizer-copy-main-to-model-params ............: (2.91, 2.95)
    optimizer ......................................: (18.62, 18.66)
 iteration       43/     100 | consumed samples:          172 | elapsed time per iteration (ms): 195.0 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.499282E+00 | loss scale: 256.0 | grad norm: 2.536 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.09, 168.12)
    forward-compute ................................: (68.13, 68.15)
    backward-compute ...............................: (99.59, 99.61)
    batch-generator ................................: (0.73, 0.81)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.03)
    grads-all-reduce ...............................: (0.84, 0.84)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.26, 2.26)
    optimizer-clip-main-grad .......................: (3.55, 3.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.63, 6.70)
    optimizer-copy-main-to-model-params ............: (2.92, 2.96)
    optimizer ......................................: (18.64, 18.69)
 iteration       44/     100 | consumed samples:          176 | elapsed time per iteration (ms): 195.1 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.312203E+00 | loss scale: 256.0 | grad norm: 2.640 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.29, 168.32)
    forward-compute ................................: (68.52, 68.52)
    backward-compute ...............................: (99.43, 99.43)
    batch-generator ................................: (0.69, 0.77)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.26, 2.27)
    optimizer-clip-main-grad .......................: (3.55, 3.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.62, 6.68)
    optimizer-copy-main-to-model-params ............: (2.92, 2.96)
    optimizer ......................................: (18.63, 18.67)
 iteration       45/     100 | consumed samples:          180 | elapsed time per iteration (ms): 195.0 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.543821E+00 | loss scale: 256.0 | grad norm: 3.004 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.08, 168.12)
    forward-compute ................................: (68.12, 68.13)
    backward-compute ...............................: (99.61, 99.63)
    batch-generator ................................: (0.76, 0.84)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.26, 2.28)
    optimizer-clip-main-grad .......................: (3.56, 3.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.62, 6.68)
    optimizer-copy-main-to-model-params ............: (2.91, 2.95)
    optimizer ......................................: (18.61, 18.65)
 iteration       46/     100 | consumed samples:          184 | elapsed time per iteration (ms): 195.6 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.086869E+00 | loss scale: 256.0 | grad norm: 2.092 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.86, 168.88)
    forward-compute ................................: (69.03, 69.03)
    backward-compute ...............................: (99.47, 99.49)
    batch-generator ................................: (0.74, 0.82)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.87)
    optimizer-copy-to-main-grad ....................: (2.72, 2.75)
    optimizer-unscale-and-check-inf ................: (2.25, 2.27)
    optimizer-clip-main-grad .......................: (3.57, 3.57)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.60, 6.67)
    optimizer-copy-main-to-model-params ............: (2.93, 2.96)
    optimizer ......................................: (18.62, 18.65)
 iteration       47/     100 | consumed samples:          188 | elapsed time per iteration (ms): 196.0 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.081430E+00 | loss scale: 256.0 | grad norm: 2.996 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (169.22, 169.27)
    forward-compute ................................: (68.08, 68.09)
    backward-compute ...............................: (100.78, 100.80)
    batch-generator ................................: (0.77, 0.84)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.25, 2.26)
    optimizer-clip-main-grad .......................: (3.56, 3.57)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.62, 6.70)
    optimizer-copy-main-to-model-params ............: (2.91, 2.96)
    optimizer ......................................: (18.61, 18.66)
 iteration       48/     100 | consumed samples:          192 | elapsed time per iteration (ms): 195.6 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.470257E+00 | loss scale: 256.0 | grad norm: 4.061 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.79, 168.82)
    forward-compute ................................: (68.01, 68.01)
    backward-compute ...............................: (100.43, 100.44)
    batch-generator ................................: (0.74, 0.82)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.88)
    optimizer-copy-to-main-grad ....................: (2.72, 2.75)
    optimizer-unscale-and-check-inf ................: (2.26, 2.27)
    optimizer-clip-main-grad .......................: (3.56, 3.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.64, 6.66)
    optimizer-copy-main-to-model-params ............: (2.91, 2.96)
    optimizer ......................................: (18.60, 18.65)
 iteration       49/     100 | consumed samples:          196 | elapsed time per iteration (ms): 195.2 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.723101E+00 | loss scale: 256.0 | grad norm: 1.920 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.37, 168.41)
    forward-compute ................................: (68.10, 68.12)
    backward-compute ...............................: (99.90, 99.92)
    batch-generator ................................: (0.76, 0.83)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.26, 2.27)
    optimizer-clip-main-grad .......................: (3.56, 3.57)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.62, 6.70)
    optimizer-copy-main-to-model-params ............: (2.91, 2.96)
    optimizer ......................................: (18.63, 18.68)
 iteration       50/     100 | consumed samples:          200 | elapsed time per iteration (ms): 194.8 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.296442E+00 | loss scale: 256.0 | grad norm: 1.724 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (167.96, 167.99)
    forward-compute ................................: (68.02, 68.04)
    backward-compute ...............................: (99.58, 99.60)
    batch-generator ................................: (0.68, 0.75)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.87)
    optimizer-copy-to-main-grad ....................: (2.71, 2.75)
    optimizer-unscale-and-check-inf ................: (2.25, 2.26)
    optimizer-clip-main-grad .......................: (3.57, 3.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.62, 6.69)
    optimizer-copy-main-to-model-params ............: (2.91, 2.95)
    optimizer ......................................: (18.61, 18.65)
 iteration       51/     100 | consumed samples:          204 | elapsed time per iteration (ms): 194.9 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.041797E+00 | loss scale: 256.0 | grad norm: 1.535 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.09, 168.11)
    forward-compute ................................: (68.07, 68.08)
    backward-compute ...............................: (99.67, 99.68)
    batch-generator ................................: (0.73, 0.81)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.26, 2.26)
    optimizer-clip-main-grad .......................: (3.56, 3.57)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.63, 6.66)
    optimizer-copy-main-to-model-params ............: (2.91, 2.96)
    optimizer ......................................: (18.58, 18.63)
 iteration       52/     100 | consumed samples:          208 | elapsed time per iteration (ms): 198.5 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.669041E+00 | loss scale: 256.0 | grad norm: 2.043 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (171.67, 171.71)
    forward-compute ................................: (68.08, 68.10)
    backward-compute ...............................: (103.24, 103.25)
    batch-generator ................................: (0.69, 0.77)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.72, 2.75)
    optimizer-unscale-and-check-inf ................: (2.26, 2.26)
    optimizer-clip-main-grad .......................: (3.56, 3.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.62, 6.67)
    optimizer-copy-main-to-model-params ............: (2.92, 2.96)
    optimizer ......................................: (18.60, 18.65)
 iteration       53/     100 | consumed samples:          212 | elapsed time per iteration (ms): 194.8 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 8.721564E+00 | loss scale: 256.0 | grad norm: 6.368 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (167.90, 167.96)
    forward-compute ................................: (68.01, 68.01)
    backward-compute ...............................: (99.55, 99.56)
    batch-generator ................................: (0.69, 0.77)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.88)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.25, 2.25)
    optimizer-clip-main-grad .......................: (3.56, 3.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.62, 6.67)
    optimizer-copy-main-to-model-params ............: (2.91, 2.97)
    optimizer ......................................: (18.59, 18.65)
 iteration       54/     100 | consumed samples:          216 | elapsed time per iteration (ms): 194.7 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.392185E+00 | loss scale: 256.0 | grad norm: 1.768 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (167.92, 167.94)
    forward-compute ................................: (68.05, 68.08)
    backward-compute ...............................: (99.50, 99.52)
    batch-generator ................................: (0.72, 0.80)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.72, 2.75)
    optimizer-unscale-and-check-inf ................: (2.26, 2.27)
    optimizer-clip-main-grad .......................: (3.57, 3.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.64, 6.68)
    optimizer-copy-main-to-model-params ............: (2.91, 2.95)
    optimizer ......................................: (18.61, 18.66)
 iteration       55/     100 | consumed samples:          220 | elapsed time per iteration (ms): 195.3 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.131256E+00 | loss scale: 256.0 | grad norm: 2.226 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.26, 168.30)
    forward-compute ................................: (68.01, 68.02)
    backward-compute ...............................: (99.90, 99.91)
    batch-generator ................................: (0.76, 0.83)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.84)
    optimizer-copy-to-main-grad ....................: (2.72, 2.75)
    optimizer-unscale-and-check-inf ................: (2.26, 2.27)
    optimizer-clip-main-grad .......................: (3.63, 3.66)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.62, 6.68)
    optimizer-copy-main-to-model-params ............: (2.92, 2.96)
    optimizer ......................................: (18.72, 18.76)
 iteration       56/     100 | consumed samples:          224 | elapsed time per iteration (ms): 195.2 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.293798E+00 | loss scale: 256.0 | grad norm: 1.514 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.53, 168.54)
    forward-compute ................................: (67.99, 68.00)
    backward-compute ...............................: (100.16, 100.18)
    batch-generator ................................: (0.74, 0.80)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.72, 2.75)
    optimizer-unscale-and-check-inf ................: (2.26, 2.27)
    optimizer-clip-main-grad .......................: (3.56, 3.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.63, 6.68)
    optimizer-copy-main-to-model-params ............: (2.91, 2.96)
    optimizer ......................................: (18.60, 18.65)
 iteration       57/     100 | consumed samples:          228 | elapsed time per iteration (ms): 195.6 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.228020E+00 | loss scale: 256.0 | grad norm: 2.227 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.72, 168.76)
    forward-compute ................................: (68.53, 68.54)
    backward-compute ...............................: (99.84, 99.85)
    batch-generator ................................: (0.74, 0.81)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.87)
    optimizer-copy-to-main-grad ....................: (2.72, 2.75)
    optimizer-unscale-and-check-inf ................: (2.26, 2.26)
    optimizer-clip-main-grad .......................: (3.57, 3.59)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.62, 6.69)
    optimizer-copy-main-to-model-params ............: (2.91, 2.96)
    optimizer ......................................: (18.62, 18.67)
 iteration       58/     100 | consumed samples:          232 | elapsed time per iteration (ms): 195.0 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.257778E+00 | loss scale: 256.0 | grad norm: 1.715 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (167.89, 167.92)
    forward-compute ................................: (68.02, 68.03)
    backward-compute ...............................: (99.52, 99.53)
    batch-generator ................................: (0.72, 0.80)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.84)
    optimizer-copy-to-main-grad ....................: (2.71, 2.76)
    optimizer-unscale-and-check-inf ................: (2.25, 2.26)
    optimizer-clip-main-grad .......................: (3.57, 3.57)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.63, 6.66)
    optimizer-copy-main-to-model-params ............: (2.92, 2.97)
    optimizer ......................................: (18.60, 18.65)
 iteration       59/     100 | consumed samples:          236 | elapsed time per iteration (ms): 200.3 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.217219E+00 | loss scale: 256.0 | grad norm: 1.812 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (173.48, 173.51)
    forward-compute ................................: (68.32, 68.32)
    backward-compute ...............................: (104.82, 104.83)
    batch-generator ................................: (0.74, 0.81)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.72, 2.75)
    optimizer-unscale-and-check-inf ................: (2.25, 2.26)
    optimizer-clip-main-grad .......................: (3.56, 3.57)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.62, 6.68)
    optimizer-copy-main-to-model-params ............: (2.92, 2.96)
    optimizer ......................................: (18.61, 18.66)
 iteration       60/     100 | consumed samples:          240 | elapsed time per iteration (ms): 195.0 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.596202E+00 | loss scale: 256.0 | grad norm: 1.950 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.06, 168.10)
    forward-compute ................................: (68.19, 68.19)
    backward-compute ...............................: (99.52, 99.53)
    batch-generator ................................: (0.71, 0.78)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.26, 2.26)
    optimizer-clip-main-grad .......................: (3.56, 3.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.62, 6.66)
    optimizer-copy-main-to-model-params ............: (2.92, 2.96)
    optimizer ......................................: (18.60, 18.64)
 iteration       61/     100 | consumed samples:          244 | elapsed time per iteration (ms): 194.9 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.228663E+00 | loss scale: 256.0 | grad norm: 1.639 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (167.99, 168.06)
    forward-compute ................................: (68.07, 68.08)
    backward-compute ...............................: (99.56, 99.58)
    batch-generator ................................: (0.69, 0.77)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.26, 2.26)
    optimizer-clip-main-grad .......................: (3.57, 3.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.61, 6.68)
    optimizer-copy-main-to-model-params ............: (2.91, 2.95)
    optimizer ......................................: (18.61, 18.66)
 iteration       62/     100 | consumed samples:          248 | elapsed time per iteration (ms): 194.8 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.146932E+00 | loss scale: 256.0 | grad norm: 1.887 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (167.94, 167.98)
    forward-compute ................................: (68.02, 68.02)
    backward-compute ...............................: (99.59, 99.60)
    batch-generator ................................: (0.72, 0.81)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.84)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.27, 2.27)
    optimizer-clip-main-grad .......................: (3.57, 3.59)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.62, 6.68)
    optimizer-copy-main-to-model-params ............: (2.90, 2.96)
    optimizer ......................................: (18.61, 18.67)
 iteration       63/     100 | consumed samples:          252 | elapsed time per iteration (ms): 194.9 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 6.879694E+00 | loss scale: 256.0 | grad norm: 1.968 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.03, 168.05)
    forward-compute ................................: (68.06, 68.06)
    backward-compute ...............................: (99.62, 99.64)
    batch-generator ................................: (0.71, 0.79)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.25, 2.26)
    optimizer-clip-main-grad .......................: (3.57, 3.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.63, 6.71)
    optimizer-copy-main-to-model-params ............: (2.91, 2.96)
    optimizer ......................................: (18.64, 18.69)
 iteration       64/     100 | consumed samples:          256 | elapsed time per iteration (ms): 194.7 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 6.952396E+00 | loss scale: 256.0 | grad norm: 1.575 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (167.88, 167.91)
    forward-compute ................................: (68.10, 68.10)
    backward-compute ...............................: (99.45, 99.45)
    batch-generator ................................: (0.69, 0.76)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.72, 2.75)
    optimizer-unscale-and-check-inf ................: (2.25, 2.26)
    optimizer-clip-main-grad .......................: (3.57, 3.57)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.61, 6.67)
    optimizer-copy-main-to-model-params ............: (2.92, 2.96)
    optimizer ......................................: (18.62, 18.66)
 iteration       65/     100 | consumed samples:          260 | elapsed time per iteration (ms): 194.8 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.595973E+00 | loss scale: 256.0 | grad norm: 1.369 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.07, 168.08)
    forward-compute ................................: (68.15, 68.16)
    backward-compute ...............................: (99.55, 99.58)
    batch-generator ................................: (0.76, 0.84)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.84)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.26, 2.27)
    optimizer-clip-main-grad .......................: (3.56, 3.57)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.61, 6.69)
    optimizer-copy-main-to-model-params ............: (2.91, 2.96)
    optimizer ......................................: (18.61, 18.66)
 iteration       66/     100 | consumed samples:          264 | elapsed time per iteration (ms): 194.8 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.211170E+00 | loss scale: 256.0 | grad norm: 1.894 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (167.99, 168.03)
    forward-compute ................................: (68.14, 68.16)
    backward-compute ...............................: (99.49, 99.51)
    batch-generator ................................: (0.75, 0.82)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.84)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.26, 2.27)
    optimizer-clip-main-grad .......................: (3.56, 3.59)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.65, 6.66)
    optimizer-copy-main-to-model-params ............: (2.91, 2.95)
    optimizer ......................................: (18.62, 18.66)
 iteration       67/     100 | consumed samples:          268 | elapsed time per iteration (ms): 195.7 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.419283E+00 | loss scale: 256.0 | grad norm: 2.035 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.75, 168.77)
    forward-compute ................................: (68.72, 68.73)
    backward-compute ...............................: (99.68, 99.69)
    batch-generator ................................: (0.74, 0.82)
    layernorm-grads-all-reduce .....................: (0.01, 0.02)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.72, 2.75)
    optimizer-unscale-and-check-inf ................: (2.25, 2.26)
    optimizer-clip-main-grad .......................: (3.57, 3.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.61, 6.70)
    optimizer-copy-main-to-model-params ............: (2.90, 2.96)
    optimizer ......................................: (18.61, 18.68)
 iteration       68/     100 | consumed samples:          272 | elapsed time per iteration (ms): 194.9 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 6.847935E+00 | loss scale: 256.0 | grad norm: 1.654 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (167.92, 167.95)
    forward-compute ................................: (68.09, 68.09)
    backward-compute ...............................: (99.48, 99.49)
    batch-generator ................................: (0.74, 0.82)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.26, 2.27)
    optimizer-clip-main-grad .......................: (3.56, 3.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.64, 6.67)
    optimizer-copy-main-to-model-params ............: (2.92, 2.96)
    optimizer ......................................: (18.62, 18.66)
 iteration       69/     100 | consumed samples:          276 | elapsed time per iteration (ms): 195.2 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 6.932998E+00 | loss scale: 256.0 | grad norm: 1.863 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.23, 168.26)
    forward-compute ................................: (68.18, 68.18)
    backward-compute ...............................: (99.70, 99.72)
    batch-generator ................................: (0.72, 0.79)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.87)
    optimizer-copy-to-main-grad ....................: (2.72, 2.75)
    optimizer-unscale-and-check-inf ................: (2.26, 2.26)
    optimizer-clip-main-grad .......................: (3.58, 3.59)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.63, 6.70)
    optimizer-copy-main-to-model-params ............: (2.91, 2.95)
    optimizer ......................................: (18.64, 18.68)
 iteration       70/     100 | consumed samples:          280 | elapsed time per iteration (ms): 195.7 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.102699E+00 | loss scale: 256.0 | grad norm: 1.948 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.89, 168.92)
    forward-compute ................................: (68.14, 68.15)
    backward-compute ...............................: (100.38, 100.39)
    batch-generator ................................: (0.78, 0.86)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.26, 2.26)
    optimizer-clip-main-grad .......................: (3.57, 3.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.64, 6.66)
    optimizer-copy-main-to-model-params ............: (2.91, 2.96)
    optimizer ......................................: (18.60, 18.65)
 iteration       71/     100 | consumed samples:          284 | elapsed time per iteration (ms): 195.3 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 6.743003E+00 | loss scale: 256.0 | grad norm: 1.373 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.48, 168.50)
    forward-compute ................................: (68.52, 68.53)
    backward-compute ...............................: (99.61, 99.63)
    batch-generator ................................: (0.75, 0.82)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.84)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.26, 2.26)
    optimizer-clip-main-grad .......................: (3.57, 3.57)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.63, 6.70)
    optimizer-copy-main-to-model-params ............: (2.91, 2.96)
    optimizer ......................................: (18.62, 18.67)
 iteration       72/     100 | consumed samples:          288 | elapsed time per iteration (ms): 194.9 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.207825E+00 | loss scale: 256.0 | grad norm: 4.242 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (167.98, 168.01)
    forward-compute ................................: (68.19, 68.19)
    backward-compute ...............................: (99.44, 99.46)
    batch-generator ................................: (0.76, 0.83)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.83, 0.85)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.27, 2.27)
    optimizer-clip-main-grad .......................: (3.57, 3.59)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.61, 6.70)
    optimizer-copy-main-to-model-params ............: (2.90, 2.96)
    optimizer ......................................: (18.64, 18.70)
 iteration       73/     100 | consumed samples:          292 | elapsed time per iteration (ms): 195.0 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 6.979714E+00 | loss scale: 256.0 | grad norm: 1.854 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.21, 168.23)
    forward-compute ................................: (68.08, 68.09)
    backward-compute ...............................: (99.76, 99.77)
    batch-generator ................................: (0.76, 0.83)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.84)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.26, 2.27)
    optimizer-clip-main-grad .......................: (3.56, 3.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.62, 6.68)
    optimizer-copy-main-to-model-params ............: (2.91, 2.97)
    optimizer ......................................: (18.63, 18.70)
 iteration       74/     100 | consumed samples:          296 | elapsed time per iteration (ms): 194.9 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.394892E+00 | loss scale: 256.0 | grad norm: 2.025 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (167.98, 168.01)
    forward-compute ................................: (68.08, 68.08)
    backward-compute ...............................: (99.55, 99.57)
    batch-generator ................................: (0.74, 0.81)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.88)
    optimizer-copy-to-main-grad ....................: (2.72, 2.75)
    optimizer-unscale-and-check-inf ................: (2.27, 2.27)
    optimizer-clip-main-grad .......................: (3.58, 3.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.62, 6.70)
    optimizer-copy-main-to-model-params ............: (2.91, 2.96)
    optimizer ......................................: (18.64, 18.69)
 iteration       75/     100 | consumed samples:          300 | elapsed time per iteration (ms): 195.0 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 6.873253E+00 | loss scale: 256.0 | grad norm: 1.947 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.16, 168.19)
    forward-compute ................................: (68.11, 68.13)
    backward-compute ...............................: (99.67, 99.69)
    batch-generator ................................: (0.78, 0.87)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.84)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.27, 2.27)
    optimizer-clip-main-grad .......................: (3.57, 3.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.63, 6.68)
    optimizer-copy-main-to-model-params ............: (2.91, 2.96)
    optimizer ......................................: (18.62, 18.68)
 iteration       76/     100 | consumed samples:          304 | elapsed time per iteration (ms): 195.1 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.345742E+00 | loss scale: 256.0 | grad norm: 2.254 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.33, 168.37)
    forward-compute ................................: (68.45, 68.45)
    backward-compute ...............................: (99.52, 99.54)
    batch-generator ................................: (0.76, 0.83)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.84)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.25, 2.26)
    optimizer-clip-main-grad .......................: (3.56, 3.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.65, 6.71)
    optimizer-copy-main-to-model-params ............: (2.91, 2.96)
    optimizer ......................................: (18.63, 18.69)
 iteration       77/     100 | consumed samples:          308 | elapsed time per iteration (ms): 195.2 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.181099E+00 | loss scale: 256.0 | grad norm: 1.676 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.21, 168.25)
    forward-compute ................................: (68.22, 68.23)
    backward-compute ...............................: (99.64, 99.65)
    batch-generator ................................: (0.73, 0.81)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.26, 2.27)
    optimizer-clip-main-grad .......................: (3.55, 3.57)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.63, 6.70)
    optimizer-copy-main-to-model-params ............: (2.94, 2.97)
    optimizer ......................................: (18.65, 18.68)
 iteration       78/     100 | consumed samples:          312 | elapsed time per iteration (ms): 195.0 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 8.573567E+00 | loss scale: 256.0 | grad norm: 2.737 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.18, 168.22)
    forward-compute ................................: (68.33, 68.34)
    backward-compute ...............................: (99.50, 99.51)
    batch-generator ................................: (0.70, 0.78)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.25, 2.26)
    optimizer-clip-main-grad .......................: (3.57, 3.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.64, 6.68)
    optimizer-copy-main-to-model-params ............: (2.91, 2.95)
    optimizer ......................................: (18.60, 18.65)
 iteration       79/     100 | consumed samples:          316 | elapsed time per iteration (ms): 194.9 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 6.899529E+00 | loss scale: 256.0 | grad norm: 1.889 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.05, 168.09)
    forward-compute ................................: (67.99, 68.00)
    backward-compute ...............................: (99.72, 99.73)
    batch-generator ................................: (0.71, 0.79)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.84)
    optimizer-copy-to-main-grad ....................: (2.72, 2.75)
    optimizer-unscale-and-check-inf ................: (2.25, 2.25)
    optimizer-clip-main-grad .......................: (3.57, 3.59)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.61, 6.67)
    optimizer-copy-main-to-model-params ............: (2.91, 2.96)
    optimizer ......................................: (18.60, 18.65)
 iteration       80/     100 | consumed samples:          320 | elapsed time per iteration (ms): 195.3 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 6.939613E+00 | loss scale: 256.0 | grad norm: 1.979 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.40, 168.42)
    forward-compute ................................: (68.39, 68.40)
    backward-compute ...............................: (99.64, 99.66)
    batch-generator ................................: (0.75, 0.83)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.87)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.26, 2.27)
    optimizer-clip-main-grad .......................: (3.56, 3.57)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.64, 6.72)
    optimizer-copy-main-to-model-params ............: (2.91, 2.96)
    optimizer ......................................: (18.65, 18.70)
 iteration       81/     100 | consumed samples:          324 | elapsed time per iteration (ms): 195.0 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.843187E+00 | loss scale: 256.0 | grad norm: 1.654 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.13, 168.16)
    forward-compute ................................: (68.19, 68.20)
    backward-compute ...............................: (99.59, 99.60)
    batch-generator ................................: (0.79, 0.85)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.84)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.26, 2.27)
    optimizer-clip-main-grad .......................: (3.56, 3.57)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.61, 6.68)
    optimizer-copy-main-to-model-params ............: (2.91, 2.96)
    optimizer ......................................: (18.61, 18.67)
 iteration       82/     100 | consumed samples:          328 | elapsed time per iteration (ms): 195.6 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.126770E+00 | loss scale: 256.0 | grad norm: 1.760 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.80, 168.84)
    forward-compute ................................: (68.89, 68.96)
    backward-compute ...............................: (99.49, 99.58)
    batch-generator ................................: (0.69, 0.77)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.84)
    optimizer-copy-to-main-grad ....................: (2.71, 2.75)
    optimizer-unscale-and-check-inf ................: (2.25, 2.26)
    optimizer-clip-main-grad .......................: (3.56, 3.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.62, 6.70)
    optimizer-copy-main-to-model-params ............: (2.92, 2.97)
    optimizer ......................................: (18.62, 18.67)
 iteration       83/     100 | consumed samples:          332 | elapsed time per iteration (ms): 194.9 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.345885E+00 | loss scale: 256.0 | grad norm: 1.754 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.06, 168.08)
    forward-compute ................................: (68.02, 68.03)
    backward-compute ...............................: (99.68, 99.70)
    batch-generator ................................: (0.70, 0.77)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.84)
    optimizer-copy-to-main-grad ....................: (2.72, 2.75)
    optimizer-unscale-and-check-inf ................: (2.26, 2.27)
    optimizer-clip-main-grad .......................: (3.56, 3.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.63, 6.68)
    optimizer-copy-main-to-model-params ............: (2.93, 2.97)
    optimizer ......................................: (18.63, 18.67)
 iteration       84/     100 | consumed samples:          336 | elapsed time per iteration (ms): 194.8 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 6.828680E+00 | loss scale: 256.0 | grad norm: 1.420 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.02, 168.07)
    forward-compute ................................: (68.08, 68.09)
    backward-compute ...............................: (99.55, 99.59)
    batch-generator ................................: (0.70, 0.79)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.72, 2.75)
    optimizer-unscale-and-check-inf ................: (2.26, 2.27)
    optimizer-clip-main-grad .......................: (3.58, 3.59)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.61, 6.68)
    optimizer-copy-main-to-model-params ............: (2.91, 2.96)
    optimizer ......................................: (18.61, 18.66)
 iteration       85/     100 | consumed samples:          340 | elapsed time per iteration (ms): 195.2 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 8.023499E+00 | loss scale: 256.0 | grad norm: 2.383 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.54, 168.57)
    forward-compute ................................: (68.54, 68.55)
    backward-compute ...............................: (99.65, 99.67)
    batch-generator ................................: (0.68, 0.77)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.83, 0.84)
    optimizer-copy-to-main-grad ....................: (2.72, 2.75)
    optimizer-unscale-and-check-inf ................: (2.25, 2.26)
    optimizer-clip-main-grad .......................: (3.55, 3.57)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.63, 6.69)
    optimizer-copy-main-to-model-params ............: (2.91, 2.96)
    optimizer ......................................: (18.59, 18.64)
 iteration       86/     100 | consumed samples:          344 | elapsed time per iteration (ms): 195.0 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.193636E+00 | loss scale: 256.0 | grad norm: 1.696 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.23, 168.26)
    forward-compute ................................: (68.23, 68.23)
    backward-compute ...............................: (99.65, 99.67)
    batch-generator ................................: (0.76, 0.83)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.27, 2.27)
    optimizer-clip-main-grad .......................: (3.57, 3.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.64, 6.70)
    optimizer-copy-main-to-model-params ............: (2.91, 2.96)
    optimizer ......................................: (18.65, 18.70)
 iteration       87/     100 | consumed samples:          348 | elapsed time per iteration (ms): 195.4 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 6.770253E+00 | loss scale: 256.0 | grad norm: 2.097 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.50, 168.53)
    forward-compute ................................: (68.39, 68.40)
    backward-compute ...............................: (99.76, 99.77)
    batch-generator ................................: (0.69, 0.77)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.84)
    optimizer-copy-to-main-grad ....................: (2.75, 2.77)
    optimizer-unscale-and-check-inf ................: (2.27, 2.27)
    optimizer-clip-main-grad .......................: (3.56, 3.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.64, 6.69)
    optimizer-copy-main-to-model-params ............: (2.93, 2.96)
    optimizer ......................................: (18.66, 18.69)
 iteration       88/     100 | consumed samples:          352 | elapsed time per iteration (ms): 195.6 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.265463E+00 | loss scale: 256.0 | grad norm: 1.635 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.81, 168.86)
    forward-compute ................................: (68.08, 68.89)
    backward-compute ...............................: (99.58, 100.38)
    batch-generator ................................: (0.73, 0.80)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.25, 2.26)
    optimizer-clip-main-grad .......................: (3.57, 3.57)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.61, 6.66)
    optimizer-copy-main-to-model-params ............: (2.92, 2.96)
    optimizer ......................................: (18.59, 18.63)
 iteration       89/     100 | consumed samples:          356 | elapsed time per iteration (ms): 195.1 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 6.747106E+00 | loss scale: 256.0 | grad norm: 1.380 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.26, 168.29)
    forward-compute ................................: (68.28, 68.28)
    backward-compute ...............................: (99.63, 99.64)
    batch-generator ................................: (0.77, 0.85)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.26, 2.26)
    optimizer-clip-main-grad .......................: (3.56, 3.56)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.63, 6.71)
    optimizer-copy-main-to-model-params ............: (2.91, 2.96)
    optimizer ......................................: (18.61, 18.67)
 iteration       90/     100 | consumed samples:          360 | elapsed time per iteration (ms): 194.9 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 6.812861E+00 | loss scale: 256.0 | grad norm: 1.524 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.02, 168.07)
    forward-compute ................................: (68.09, 68.10)
    backward-compute ...............................: (99.58, 99.59)
    batch-generator ................................: (0.74, 0.83)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.86)
    optimizer-copy-to-main-grad ....................: (2.73, 2.76)
    optimizer-unscale-and-check-inf ................: (2.26, 2.26)
    optimizer-clip-main-grad .......................: (3.57, 3.59)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.63, 6.71)
    optimizer-copy-main-to-model-params ............: (2.93, 2.96)
    optimizer ......................................: (18.66, 18.69)
 iteration       91/     100 | consumed samples:          364 | elapsed time per iteration (ms): 195.0 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.427866E+00 | loss scale: 256.0 | grad norm: 2.653 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.22, 168.24)
    forward-compute ................................: (68.21, 68.23)
    backward-compute ...............................: (99.66, 99.68)
    batch-generator ................................: (0.77, 0.83)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.84)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.27, 2.27)
    optimizer-clip-main-grad .......................: (3.56, 3.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.63, 6.67)
    optimizer-copy-main-to-model-params ............: (2.91, 2.95)
    optimizer ......................................: (18.61, 18.66)
 iteration       92/     100 | consumed samples:          368 | elapsed time per iteration (ms): 195.2 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 6.683096E+00 | loss scale: 256.0 | grad norm: 1.573 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.29, 168.33)
    forward-compute ................................: (68.40, 68.41)
    backward-compute ...............................: (99.53, 99.55)
    batch-generator ................................: (0.73, 0.81)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.84)
    optimizer-copy-to-main-grad ....................: (2.72, 2.75)
    optimizer-unscale-and-check-inf ................: (2.26, 2.27)
    optimizer-clip-main-grad .......................: (3.57, 3.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.62, 6.72)
    optimizer-copy-main-to-model-params ............: (2.91, 2.96)
    optimizer ......................................: (18.67, 18.72)
 iteration       93/     100 | consumed samples:          372 | elapsed time per iteration (ms): 195.6 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.434524E+00 | loss scale: 256.0 | grad norm: 2.360 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.69, 168.72)
    forward-compute ................................: (68.09, 68.10)
    backward-compute ...............................: (100.25, 100.26)
    batch-generator ................................: (0.70, 0.77)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.88)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.26, 2.26)
    optimizer-clip-main-grad .......................: (3.56, 3.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.63, 6.69)
    optimizer-copy-main-to-model-params ............: (2.90, 2.95)
    optimizer ......................................: (18.61, 18.66)
 iteration       94/     100 | consumed samples:          376 | elapsed time per iteration (ms): 195.0 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 6.611683E+00 | loss scale: 256.0 | grad norm: 1.904 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.18, 168.21)
    forward-compute ................................: (68.19, 68.20)
    backward-compute ...............................: (99.63, 99.64)
    batch-generator ................................: (0.74, 0.82)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.72, 2.75)
    optimizer-unscale-and-check-inf ................: (2.26, 2.26)
    optimizer-clip-main-grad .......................: (3.57, 3.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.62, 6.66)
    optimizer-copy-main-to-model-params ............: (2.92, 2.97)
    optimizer ......................................: (18.60, 18.65)
 iteration       95/     100 | consumed samples:          380 | elapsed time per iteration (ms): 195.0 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.817726E+00 | loss scale: 256.0 | grad norm: 1.945 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.23, 168.24)
    forward-compute ................................: (68.21, 68.22)
    backward-compute ...............................: (99.65, 99.69)
    batch-generator ................................: (0.74, 0.81)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.26, 2.26)
    optimizer-clip-main-grad .......................: (3.57, 3.57)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.64, 6.68)
    optimizer-copy-main-to-model-params ............: (2.91, 2.96)
    optimizer ......................................: (18.60, 18.65)
 iteration       96/     100 | consumed samples:          384 | elapsed time per iteration (ms): 194.7 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 6.790770E+00 | loss scale: 256.0 | grad norm: 1.522 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (167.92, 167.97)
    forward-compute ................................: (68.09, 68.10)
    backward-compute ...............................: (99.48, 99.50)
    batch-generator ................................: (0.75, 0.84)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.26, 2.27)
    optimizer-clip-main-grad .......................: (3.56, 3.57)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.62, 6.70)
    optimizer-copy-main-to-model-params ............: (2.91, 2.97)
    optimizer ......................................: (18.62, 18.68)
 iteration       97/     100 | consumed samples:          388 | elapsed time per iteration (ms): 194.9 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 6.897821E+00 | loss scale: 256.0 | grad norm: 1.925 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.00, 168.03)
    forward-compute ................................: (68.04, 68.05)
    backward-compute ...............................: (99.61, 99.63)
    batch-generator ................................: (0.70, 0.78)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.25, 2.26)
    optimizer-clip-main-grad .......................: (3.56, 3.57)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.62, 6.68)
    optimizer-copy-main-to-model-params ............: (2.91, 2.96)
    optimizer ......................................: (18.59, 18.64)
 iteration       98/     100 | consumed samples:          392 | elapsed time per iteration (ms): 194.9 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.304220E+00 | loss scale: 256.0 | grad norm: 2.462 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (168.06, 168.08)
    forward-compute ................................: (68.13, 68.13)
    backward-compute ...............................: (99.59, 99.60)
    batch-generator ................................: (0.70, 0.79)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.87)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.25, 2.26)
    optimizer-clip-main-grad .......................: (3.57, 3.59)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.62, 6.67)
    optimizer-copy-main-to-model-params ............: (2.91, 2.96)
    optimizer ......................................: (18.61, 18.65)
 iteration       99/     100 | consumed samples:          396 | elapsed time per iteration (ms): 195.9 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 7.076025E+00 | loss scale: 256.0 | grad norm: 1.958 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (169.03, 169.05)
    forward-compute ................................: (69.01, 69.02)
    backward-compute ...............................: (99.66, 99.68)
    batch-generator ................................: (0.74, 0.83)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.84)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.25, 2.26)
    optimizer-clip-main-grad .......................: (3.57, 3.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.63, 6.69)
    optimizer-copy-main-to-model-params ............: (2.91, 2.96)
    optimizer ......................................: (18.61, 18.67)
 iteration      100/     100 | consumed samples:          400 | elapsed time per iteration (ms): 195.9 | learning rate: 1.500E-04 | global batch size:     4 | lm loss: 6.966874E+00 | loss scale: 256.0 | grad norm: 1.534 | number of skipped iterations:   0 | number of nan iterations:   0 |
(min, max) time across ranks (ms):
    forward-backward ...............................: (169.04, 169.09)
    forward-compute ................................: (69.13, 69.14)
    backward-compute ...............................: (99.56, 99.57)
    batch-generator ................................: (0.72, 0.80)
    layernorm-grads-all-reduce .....................: (0.01, 0.01)
    embedding-grads-all-reduce .....................: (0.02, 0.02)
    grads-all-reduce ...............................: (0.84, 0.85)
    optimizer-copy-to-main-grad ....................: (2.72, 2.76)
    optimizer-unscale-and-check-inf ................: (2.25, 2.25)
    optimizer-clip-main-grad .......................: (3.57, 3.58)
    optimizer-count-zeros ..........................: (0.01, 0.01)
    optimizer-inner-step ...........................: (6.62, 6.66)
    optimizer-copy-main-to-model-params ............: (2.91, 2.96)
    optimizer ......................................: (18.58, 18.63)
[after training is done] datetime: 2023-09-24 15:18:37 
Evaluating on 4 samples
Evaluating iter 1/1
-----------------------------------------------------------------------------------------------------------------
 validation loss at iteration 100 on validation set | lm loss value: 6.823257E+00 | lm loss PPL: 9.189736E+02 | 
-----------------------------------------------------------------------------------------------------------------
Evaluating on 4 samples
Evaluating iter 1/1
-----------------------------------------------------------------------------------------------------------
 validation loss at iteration 100 on test set | lm loss value: 6.761868E+00 | lm loss PPL: 8.642555E+02 | 
-----------------------------------------------------------------------------------------------------------
